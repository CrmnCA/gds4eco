[
  {
    "objectID": "environPy.html#coding-language",
    "href": "environPy.html#coding-language",
    "title": "Environment",
    "section": "Coding language",
    "text": "Coding language\nThis course is primarily designed to introduce Geographic Data Science using Python as the core programming language. All course materials, assignments, and exercises are built with Python in mind, ensuring consistency and clarity throughout the learning process. Python was selected for its versatility, extensive libraries, and widespread use in the Geographic Data Science field, making it an excellent choice for both beginners and advanced users. If you are curious about conducting similar geospatial analyses in R, you can access additional resources here. However, for this course, all work should be completed in Python and following the environment setup that we introduce below."
  },
  {
    "objectID": "environPy.html#reproducing-code-in-this-course",
    "href": "environPy.html#reproducing-code-in-this-course",
    "title": "Environment",
    "section": "Reproducing code in this course",
    "text": "Reproducing code in this course\nTo run the analysis and reproduce the code in Python, you will need to set up your Python environment according to the following instructions. Please follow the instructions according to your operating system.\n\n\n\n\n\n\nNote\n\n\n\nEven if you have used Python before and have set up your own environment, we very much recommend following the set up described below to ensure you can run the code smoothly.\n\n\nFollow these instructions and test your installation prior to the first session of the course. Setting up the Python environment is necessary for:\n\nExecuting the Jupyter notebooks of the sessions of the course.\nPreparing your own Jupyter notebooks.\n\nTo learn more about Jupyter notebooks, please visit this site."
  },
  {
    "objectID": "environPy.html#set-up-python",
    "href": "environPy.html#set-up-python",
    "title": "Environment",
    "section": "Set up Python",
    "text": "Set up Python\n\nInstallation of Miniconda\n\nInstall Miniconda on your personal laptop: Follow the instructions here.\nDuring the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it gds4eco. You can save it wherever you want, but remember to be tidy with your folder structure!\nDownload the data to run and render the Jupyter notebooks. To learn how to download folders from github see the next section ?sec-download.\nUnzip the folders and store the nested folders into a subfolder named data within the folder gds4eco.\nCreate another subfolder named labs within gds4eco.\n\nThe folder structure should look like:\ngds4eco/\n├── data/\n└── labs/\n\n\nSet up the Python Environment\n\nMS WindowsMac\n\n\n\nDownload the gds4eco.yml from GitHub by cliciking Download raw file, top right at this page MODIFY!\nSave it in the folder gds4eco created before.\nType in the search bar and find the Anaconda Prompt (miniconda 3) in your personal computer. Launch it. The terminal should appear.\n\n\n\nIn the Anaconda Terminal write: conda env create -n gds4eco --file M:\\gds4eco\\gds4eco.yml and press Enter; if the file is located elsewhere you’ll need to use the corresponding file path.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nIn the Anaconda Terminal write conda activate gds4eco and press Enter. This activates your working environment.\n\n\n\n\nConfiguration of Jupyter Notebooks:\n\nIn the Anaconda Terminal, write jupyter server --generate-config and press enter. This, at least in Windows, should create a file to: C:\\Users\\username\\.jupyter\\jupyter_server_config.py.\nOpen the file with a text editor (e.g. Notepad++), do a ctrl-f search for: c.ServerApp.root_dir, uncomment it by removing the # and change it to c.ServerApp.notebook_dir = 'M:\\\\your\\\\new\\\\path, for example the directory where you created the gds4eco folder.\nSave the file and close it.\n\n\n\n\n\n\nDownload the gds4eco.yml from GitHub by clicking Download raw file, top right at this page.\nSave it in the folder envs363_563 created before.\nType in the search bar and open the Terminal.\nIn the Terminal write conda env create -n gds4eco --file gds4eco.yml and press Enter. This will need to be modified according to where you placed the gds4eco folder. For example, Elisabetta has named her folder gds4eco and it’s in her Dropbox in Users/PIETROST/Library/CloudStorage/Dropbox/envs363_563/envs363_563.yml. If you created the gds4eco folder on your desktop, the path would be Desktop/gds4eco.\n\n\n\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nYou should then see this\n\n\n\n\n\n\n\nStart a Lab Session\n\nMS WindowsMac\n\n\n\nDownload the Jupyter Notebook of the session in your folder. Choose one Jupyter notebook and click Dowload raw file as shown below.\n\n\n\nSave the file in the labs folder within your geo4eco folder on your machine.\nType in the search bar, find and open the Anaconda Prompt (miniconda 3).\nIn the Anaconda Terminal write and run conda activate geo4eco.\nIn the Anaconda Terminal write and run jupyter notebook. This should open Jupyter Notebook in your default browser.\n\n\n\nNavigate to your course folder in and double click on the notebook downloaded in step 1.\nYou can now work on your copy of the notebook.\n\n\n\n\nDownload the Jupyter Notebook of the session in your folder. Choose one jupyter notebook and click Dowload raw file as shown below\n\n\n\nSave the file in the labs folder within your envs363 folder on your machine.\nType in the search bar, find and open the Terminal.\nIn the Terminal write and run conda activate envs363.\nIn the Terminal write and run jupyter notebook.\n\n\n\nThis should open Jupyter Notebook in your default browser. You should see something like this:\n\n\n\nNavigate to your folder. You can now work on your copy of the notebook."
  },
  {
    "objectID": "environPy.html#py-basics",
    "href": "environPy.html#py-basics",
    "title": "Environment",
    "section": "Py Basics",
    "text": "Py Basics\nPlease refer to the tutorials from learnpython.org for an introduction to coding in Python. We particularly recommend the tutorials listed under the “Learn the Basics” section."
  },
  {
    "objectID": "environPy.html#resources",
    "href": "environPy.html#resources",
    "title": "Environment",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python.\nPython for Geographic Data Analysis"
  },
  {
    "objectID": "spatial-data.html#data-tables",
    "href": "spatial-data.html#data-tables",
    "title": "1  Spatial data",
    "section": "",
    "text": "1.1.1 Points\nAssuming you have the file locally on the path ./data/:\n\npts = geopandas.read_file(\"./data/madrid_abb.gpkg\")\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes, points are provided as separate columns in an otherwise non-spatial table. For example imagine we have an object cols with a column named X for longitude and Y for latitude. Then, we can convert those into proper geometries by running pts = geopandas.GeoSeries( geopandas.points_from_xy(cols[\"X\"], cols[\"Y\"]).\n\n\nLet’s explore the points dataset that we loaded above.\n\npts.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 18399 entries, 0 to 18398\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   price            18399 non-null  object  \n 1   price_usd        18399 non-null  float64 \n 2   log1p_price_usd  18399 non-null  float64 \n 3   accommodates     18399 non-null  int64   \n 4   bathrooms        18399 non-null  object  \n 5   bedrooms         18399 non-null  float64 \n 6   beds             18399 non-null  float64 \n 7   neighbourhood    18399 non-null  object  \n 8   room_type        18399 non-null  object  \n 9   property_type    18399 non-null  object  \n 10  WiFi             18399 non-null  object  \n 11  Coffee           18399 non-null  object  \n 12  Gym              18399 non-null  object  \n 13  Parking          18399 non-null  object  \n 14  km_to_retiro     18399 non-null  float64 \n 15  geometry         18399 non-null  geometry\ndtypes: float64(5), geometry(1), int64(1), object(9)\nmemory usage: 2.2+ MB\n\n\n\npts.head()\n\n\n\n\n\n\n\n\nprice\nprice_usd\nlog1p_price_usd\naccommodates\nbathrooms\nbedrooms\nbeds\nneighbourhood\nroom_type\nproperty_type\nWiFi\nCoffee\nGym\nParking\nkm_to_retiro\ngeometry\n\n\n\n\n0\n$60.00\n60.0\n4.110874\n2\n1 shared bath\n1.0\n1.0\nHispanoamérica\nPrivate room\nPrivate room in apartment\n1\n0\n0\n0\n5.116664\nPOINT (-3.67688 40.45724)\n\n\n1\n$31.00\n31.0\n3.465736\n1\n1 bath\n1.0\n1.0\nCármenes\nPrivate room\nPrivate room in apartment\n1\n1\n0\n1\n5.563869\nPOINT (-3.74084 40.40341)\n\n\n2\n$60.00\n60.0\n4.110874\n6\n2 baths\n3.0\n5.0\nLegazpi\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n3.048442\nPOINT (-3.69304 40.38695)\n\n\n3\n$115.00\n115.0\n4.753590\n4\n1.5 baths\n2.0\n3.0\nJusticia\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n2.075484\nPOINT (-3.69764 40.41995)\n\n\n4\n$26.00\n26.0\n3.295837\n1\n1 private bath\n1.0\n1.0\nLegazpi\nPrivate room\nPrivate room in house\n1\n0\n0\n0\n2.648058\nPOINT (-3.69011 40.38985)\n\n\n\n\n\n\n\n\n\n1.1.2 Lines\nAssuming you have the file locally on the path ./data/:\n\nlines = geopandas.read_file(\"./data/arturo_streets.gpkg\")\n\n\nlines.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 66499 entries, 0 to 66498\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   OGC_FID             66499 non-null  object  \n 1   dm_id               66499 non-null  object  \n 2   dist_barri          66483 non-null  object  \n 3   average_quality     66499 non-null  float64 \n 4   population_density  66499 non-null  float64 \n 5   X                   66499 non-null  float64 \n 6   Y                   66499 non-null  float64 \n 7   value               5465 non-null   float64 \n 8   geometry            66499 non-null  geometry\ndtypes: float64(5), geometry(1), object(3)\nmemory usage: 4.6+ MB\n\n\n\nlines.loc[0, \"geometry\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Print descriptive statistics for population_density and average_quality.\n\n\n\n\n1.1.3 Polygons\nAssuming you have the file locally on the path ./data/:\n\npolys = geopandas.read_file(\"./data/neighbourhoods.geojson\")\n\n\npolys.head()\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n0\nPalacio\nCentro\nMULTIPOLYGON (((-3.70584 40.42030, -3.70625 40...\n\n\n1\nEmbajadores\nCentro\nMULTIPOLYGON (((-3.70384 40.41432, -3.70277 40...\n\n\n2\nCortes\nCentro\nMULTIPOLYGON (((-3.69796 40.41929, -3.69645 40...\n\n\n3\nJusticia\nCentro\nMULTIPOLYGON (((-3.69546 40.41898, -3.69645 40...\n\n\n4\nUniversidad\nCentro\nMULTIPOLYGON (((-3.70107 40.42134, -3.70155 40...\n\n\n\n\n\n\n\n\npolys.query(\"neighbourhood_group == 'Retiro'\")\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n13\nPacífico\nRetiro\nMULTIPOLYGON (((-3.67015 40.40654, -3.67017 40...\n\n\n14\nAdelfas\nRetiro\nMULTIPOLYGON (((-3.67283 40.39468, -3.67343 40...\n\n\n15\nEstrella\nRetiro\nMULTIPOLYGON (((-3.66506 40.40647, -3.66512 40...\n\n\n16\nIbiza\nRetiro\nMULTIPOLYGON (((-3.66916 40.41796, -3.66927 40...\n\n\n17\nJerónimos\nRetiro\nMULTIPOLYGON (((-3.67874 40.40751, -3.67992 40...\n\n\n18\nNiño Jesús\nRetiro\nMULTIPOLYGON (((-3.66994 40.40850, -3.67012 40...\n\n\n\n\n\n\n\n\npolys.neighbourhood_group.unique()\n\narray(['Centro', 'Arganzuela', 'Retiro', 'Salamanca', 'Chamartín',\n       'Moratalaz', 'Tetuán', 'Chamberí', 'Fuencarral - El Pardo',\n       'Moncloa - Aravaca', 'Puente de Vallecas', 'Latina', 'Carabanchel',\n       'Usera', 'Ciudad Lineal', 'Hortaleza', 'Villaverde',\n       'Villa de Vallecas', 'Vicálvaro', 'San Blas - Canillejas',\n       'Barajas'], dtype=object)",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#surfaces",
    "href": "spatial-data.html#surfaces",
    "title": "1  Spatial data",
    "section": "1.3 Surfaces",
    "text": "1.3 Surfaces\nAssuming you have the file locally on the path ./data/:\n\nsat = rioxarray.open_rasterio(\"./data/madrid_scene_s2_10_tc.tif\")\n\n\nsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 3681, x: 3129)&gt;\n[34553547 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 3681x: 3129...[34553547 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0,\n              424825.0, 424835.0, 424845.0, 424855.0,\n              ...\n              455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0,\n              456015.0, 456025.0, 456035.0, 456045.0],\n             dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Float64Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n              4499305.0, 4499295.0, 4499285.0, 4499275.0,\n              ...\n              4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n              4462595.0, 4462585.0, 4462575.0, 4462565.0],\n             dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(band=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3681, x: 3129)&gt;\n[11517849 values with dtype=uint8]\nCoordinates:\n    band         int64 1\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 3681x: 3129...[11517849 values with dtype=uint8]Coordinates: (4)band()int641array(1)x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (2)xPandasIndexPandasIndex(Float64Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0,\n              424825.0, 424835.0, 424845.0, 424855.0,\n              ...\n              455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0,\n              456015.0, 456025.0, 456035.0, 456045.0],\n             dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Float64Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n              4499305.0, 4499295.0, 4499285.0, 4499275.0,\n              ...\n              4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n              4462595.0, 4462585.0, 4462575.0, 4462565.0],\n             dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(\n    x=slice(430000, 440000),  # x is ascending\n    y=slice(4480000, 4470000) # y is descending\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 1000, x: 1000)&gt;\n[3000000 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.3e+05 4.3e+05 4.3e+05 ... 4.4e+05 4.4e+05 4.4e+05\n  * y            (y) float64 4.48e+06 4.48e+06 4.48e+06 ... 4.47e+06 4.47e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 1000x: 1000...[3000000 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.3e+05 4.3e+05 ... 4.4e+05 4.4e+05array([430005., 430015., 430025., ..., 439975., 439985., 439995.])y(y)float644.48e+06 4.48e+06 ... 4.47e+06array([4479995., 4479985., 4479975., ..., 4470025., 4470015., 4470005.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([430005.0, 430015.0, 430025.0, 430035.0, 430045.0, 430055.0,\n              430065.0, 430075.0, 430085.0, 430095.0,\n              ...\n              439905.0, 439915.0, 439925.0, 439935.0, 439945.0, 439955.0,\n              439965.0, 439975.0, 439985.0, 439995.0],\n             dtype='float64', name='x', length=1000))yPandasIndexPandasIndex(Float64Index([4479995.0, 4479985.0, 4479975.0, 4479965.0, 4479955.0, 4479945.0,\n              4479935.0, 4479925.0, 4479915.0, 4479905.0,\n              ...\n              4470095.0, 4470085.0, 4470075.0, 4470065.0, 4470055.0, 4470045.0,\n              4470035.0, 4470025.0, 4470015.0, 4470005.0],\n             dtype='float64', name='y', length=1000))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Subset sat to band 2 and the section within [444444, 455555] of Easting and [4470000, 4480000] of Northing.\n\nHow many pixels does it contain?\nWhat if you used bands 1 and 3 instead?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#visualisation",
    "href": "spatial-data.html#visualisation",
    "title": "1  Spatial data",
    "section": "1.4 Visualisation",
    "text": "1.4 Visualisation\nYou will need version 0.10.0 or greater of geopandas to use explore.\n\npolys.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfig, ax = plt.subplots()\npolys.plot(ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nlines.plot(linewidth=0.1, color=\"black\", ax=ax)\n#contextily.add_basemap(ax, crs=lines.crs)\nplt.show()\n\n\n\n\n\n\n\n\nSee more basemap options here.\n\nfig, ax = plt.subplots()\npts.plot(color=\"red\", figsize=(12, 12), markersize=0.1, ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs = pts.crs,\n    source = contextily.providers.CartoDB.DarkMatter\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nsat.plot.imshow(figsize=(12, 12))\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsat.plot.imshow(ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs=sat.rio.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=11,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Make three plots of sat, plotting one single band in each.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#spatial-operations",
    "href": "spatial-data.html#spatial-operations",
    "title": "1  Spatial data",
    "section": "1.5 Spatial operations",
    "text": "1.5 Spatial operations\n\n1.5.1 (Re-)Projections\n\npts.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.crs\n\nCRS.from_epsg(32630)\n\n\n\npts.to_crs(sat.rio.crs).crs\n\n&lt;Projected CRS: EPSG:32630&gt;\nName: WGS 84 / UTM zone 30N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.reproject(pts.crs).rio.crs\n\nCRS.from_epsg(4326)\n\n\n\n# All into Web Mercator (EPSG:3857)\nfig, ax = plt.subplots(1, figsize=(12, 12))\n\n## Satellite image\nsat.rio.reproject(\n    \"EPSG:3857\"\n).plot.imshow(\n    ax=ax\n)\n\n## Neighbourhoods\npolys.to_crs(epsg=3857).plot(\n    linewidth=1, \n    edgecolor=\"xkcd:lime\", \n    facecolor=\"none\",\n    ax=ax\n)\n\n## Labels\ncontextily.add_basemap( # No need to reproject\n    ax,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.5.2 Centroids\nNote the warning that geometric operations with non-projected CRS object result in biases.\n\npolys.centroid\n\n/var/folders/79/65l52xsj7vq_4_t_l6k5bl2c0000gn/T/ipykernel_30771/2101097851.py:1: UserWarning:\n\nGeometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n\n\n\n0      POINT (-3.71398 40.41543)\n1      POINT (-3.70237 40.40925)\n2      POINT (-3.69674 40.41485)\n3      POINT (-3.69657 40.42367)\n4      POINT (-3.70698 40.42568)\n                 ...            \n123    POINT (-3.59135 40.45656)\n124    POINT (-3.59723 40.48441)\n125    POINT (-3.55847 40.47613)\n126    POINT (-3.57889 40.47471)\n127    POINT (-3.60718 40.46415)\nLength: 128, dtype: geometry\n\n\nIt is therefore important to re-project these geometries to a projected crs such as we did with with pts before.\n\npolys = polys.to_crs(sat.rio.crs)\n\nNow, we can compute centroids without warnings:\n\npolys.centroid\n\n0      POINT (439425.451 4474112.019)\n1      POINT (440404.977 4473418.085)\n2      POINT (440887.707 4474036.547)\n3      POINT (440909.920 4475014.820)\n4      POINT (440028.666 4475245.024)\n                    ...              \n123    POINT (449860.280 4478601.086)\n124    POINT (449382.527 4481695.863)\n125    POINT (452661.832 4480754.248)\n126    POINT (450929.735 4480608.573)\n127    POINT (448523.423 4479452.348)\nLength: 128, dtype: geometry\n\n\n\nfig, ax = plt.subplots()\npolys.plot(color=\"purple\", ax=ax)\npolys.centroid.plot(\n    ax=ax, color=\"lime\", markersize=1\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.5.3 Spatial joins\nMore information about spatial joins in geopandas is available on its documentation page.\nLet’s ensure that the geometries we are looking to join are in the same projection.\n\nlines = lines.to_crs(polys.crs)\n\n\nsj = geopandas.sjoin(\n    lines,\n    polys\n)\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nInt64Index: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\nfig, ax = plt.subplots()\n\n# Subset of lines\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).plot(color=\"xkcd:bright turquoise\", ax=ax)\n\n# Subset of line centroids\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).centroid.plot(\n    color=\"xkcd:bright violet\", markersize=7, ax=ax\n)\n\n# Local basemap\ncontextily.add_basemap(\n    ax,\n    crs=sj.crs,\n    source=\"./data/madrid_scene_s2_10_tc.tif\",\n    alpha=0.5\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nInt64Index: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\n\n1.5.4 Areas\nTo compute areas of polygons, use a projected crs (we already transformed polys to the same projection as sat, which is a projected crs).\n\nareas = polys.area * 1e-6 # Km2\nareas.head()\n\n0    1.471037\n1    1.033253\n2    0.592049\n3    0.742031\n4    0.947616\ndtype: float64\n\n\n\n\n1.5.5 Distances\nWe can give geopandas.tools.geocode() a string or a set of strings corresponding to addresses. It will geocode it and return a GeoDataFrame of the resulting point geometries\n\ncemfi = geopandas.tools.geocode(\n    \"Calle Casado del Alisal, 5, Madrid\"\n).to_crs(sat.rio.crs)\n\n\ncemfi\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (441477.245 4473939.537)\n5, Calle Casado del Alisal, 28014, Calle Casad...\n\n\n\n\n\n\n\nWe can compute the distance between the point for cemfi and the centroids of all the polygons in polys ensuring they both are in the same crs:\n\npolys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry\n)\n\n/var/folders/79/65l52xsj7vq_4_t_l6k5bl2c0000gn/T/ipykernel_30771/176561454.py:3: UserWarning:\n\nThe indices of the two GeoSeries are different.\n\n\n\n0      1491.338749\n1              NaN\n2              NaN\n3              NaN\n4              NaN\n          ...     \n123            NaN\n124            NaN\n125            NaN\n126            NaN\n127            NaN\nLength: 128, dtype: float64\n\n\n\nd2cemfi = polys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry[0] # NO index\n)\nd2cemfi.head()\n\n0    1491.338749\n1     565.418135\n2     278.121017\n3     650.926572\n4    1196.771601\ndtype: float64\n\n\nMake a map, colouring the polygons according the the distance of their centroid to cemfi:\n\nfig, ax = plt.subplots()\n\npolys.assign(\n    dist=d2cemfi/1000\n).plot(\"dist\", legend=True, ax=ax)\n\ncemfi.to_crs(\n    polys.crs\n).plot(\n    marker=\"*\", \n    markersize=15, \n    color=\"r\", \n    label=\"CEMFI\", \n    ax=ax\n)\n\nax.legend()\nax.set_title(\n    \"Distance to CEMFI\"\n)\n\nplt.show()",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#next-steps",
    "href": "spatial-data.html#next-steps",
    "title": "1  Spatial data",
    "section": "1.6 Next steps",
    "text": "1.6 Next steps\nIf you are interested in following up on some of the topics explored in this block, the following pointers might be useful:\n\nAlthough we have seen here geopandas only, all non-geographic operations on geo-tables are really thanks to pandas, the workhorse for tabular data in Python. Their official documentation is an excellent first stop. If you prefer a book, (McKinney 2013) is a great one.\nFor more detail on geographic operations on geo-tables, the Geopandas official documentation is a great place to continue the journey.\nSurfaces, as covered here, are really an example of multi-dimensional labelled arrays. The library we use, xarray represents the cutting edge for working with these data structures in Python, and their documentation is a great place to wrap your head around how data of this type can be manipulated. For geographic extensions (CRS handling, reprojections, etc.), we have used rioxarray under the hood, and its documentation is also well worth checking.\n\n\n\n\n\nMcKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. 1st ed. Paperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "geovisualisation.html#importing-modules",
    "href": "geovisualisation.html#importing-modules",
    "title": "Geovisualisation",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "geovisualisation.html#datasets",
    "href": "geovisualisation.html#datasets",
    "title": "Geovisualisation",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "geovisualisation.html#inspecting-spatial-data",
    "href": "geovisualisation.html#inspecting-spatial-data",
    "title": "Geovisualisation",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "geovisualisation.html#styling-plots",
    "href": "geovisualisation.html#styling-plots",
    "title": "Geovisualisation",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "geovisualisation.html#coordinate-reference-systems",
    "href": "geovisualisation.html#coordinate-reference-systems",
    "title": "Geovisualisation",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "geovisualisation.html#manipulating-spatial-tables",
    "href": "geovisualisation.html#manipulating-spatial-tables",
    "title": "Geovisualisation",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "geovisualisation.html#joins",
    "href": "geovisualisation.html#joins",
    "title": "Geovisualisation",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "geovisualisation.html#join-districts-with-educational-level-data",
    "href": "geovisualisation.html#join-districts-with-educational-level-data",
    "title": "Geovisualisation",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "geovisualisation.html#saving-maps-to-figures",
    "href": "geovisualisation.html#saving-maps-to-figures",
    "title": "Geovisualisation",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "geovisualisation.html#adding-baselayers",
    "href": "geovisualisation.html#adding-baselayers",
    "title": "Geovisualisation",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "geovisualisation.html#interactive-maps",
    "href": "geovisualisation.html#interactive-maps",
    "title": "Geovisualisation",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "spatial-feature-i.html#importing-modules",
    "href": "spatial-feature-i.html#importing-modules",
    "title": "Spatial feature engineering (I)",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "spatial-feature-i.html#datasets",
    "href": "spatial-feature-i.html#datasets",
    "title": "Spatial feature engineering (I)",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "spatial-feature-i.html#inspecting-spatial-data",
    "href": "spatial-feature-i.html#inspecting-spatial-data",
    "title": "Spatial feature engineering (I)",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "spatial-feature-i.html#styling-plots",
    "href": "spatial-feature-i.html#styling-plots",
    "title": "Spatial feature engineering (I)",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "spatial-feature-i.html#coordinate-reference-systems",
    "href": "spatial-feature-i.html#coordinate-reference-systems",
    "title": "Spatial feature engineering (I)",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-i.html#manipulating-spatial-tables",
    "href": "spatial-feature-i.html#manipulating-spatial-tables",
    "title": "Spatial feature engineering (I)",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-i.html#joins",
    "href": "spatial-feature-i.html#joins",
    "title": "Spatial feature engineering (I)",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "spatial-feature-i.html#join-districts-with-educational-level-data",
    "href": "spatial-feature-i.html#join-districts-with-educational-level-data",
    "title": "Spatial feature engineering (I)",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "spatial-feature-i.html#saving-maps-to-figures",
    "href": "spatial-feature-i.html#saving-maps-to-figures",
    "title": "Spatial feature engineering (I)",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "spatial-feature-i.html#adding-baselayers",
    "href": "spatial-feature-i.html#adding-baselayers",
    "title": "Spatial feature engineering (I)",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-i.html#interactive-maps",
    "href": "spatial-feature-i.html#interactive-maps",
    "title": "Spatial feature engineering (I)",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "spatial-feature-ii.html#importing-modules",
    "href": "spatial-feature-ii.html#importing-modules",
    "title": "Spatial feature engineering (II)",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "spatial-feature-ii.html#datasets",
    "href": "spatial-feature-ii.html#datasets",
    "title": "Spatial feature engineering (II)",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "spatial-feature-ii.html#inspecting-spatial-data",
    "href": "spatial-feature-ii.html#inspecting-spatial-data",
    "title": "Spatial feature engineering (II)",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "spatial-feature-ii.html#styling-plots",
    "href": "spatial-feature-ii.html#styling-plots",
    "title": "Spatial feature engineering (II)",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "spatial-feature-ii.html#coordinate-reference-systems",
    "href": "spatial-feature-ii.html#coordinate-reference-systems",
    "title": "Spatial feature engineering (II)",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-ii.html#manipulating-spatial-tables",
    "href": "spatial-feature-ii.html#manipulating-spatial-tables",
    "title": "Spatial feature engineering (II)",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-ii.html#joins",
    "href": "spatial-feature-ii.html#joins",
    "title": "Spatial feature engineering (II)",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "spatial-feature-ii.html#join-districts-with-educational-level-data",
    "href": "spatial-feature-ii.html#join-districts-with-educational-level-data",
    "title": "Spatial feature engineering (II)",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "spatial-feature-ii.html#saving-maps-to-figures",
    "href": "spatial-feature-ii.html#saving-maps-to-figures",
    "title": "Spatial feature engineering (II)",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "spatial-feature-ii.html#adding-baselayers",
    "href": "spatial-feature-ii.html#adding-baselayers",
    "title": "Spatial feature engineering (II)",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "spatial-feature-ii.html#interactive-maps",
    "href": "spatial-feature-ii.html#interactive-maps",
    "title": "Spatial feature engineering (II)",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "openstreetmap.html#importing-modules",
    "href": "openstreetmap.html#importing-modules",
    "title": "OpenStreetMap",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "openstreetmap.html#datasets",
    "href": "openstreetmap.html#datasets",
    "title": "OpenStreetMap",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "openstreetmap.html#inspecting-spatial-data",
    "href": "openstreetmap.html#inspecting-spatial-data",
    "title": "OpenStreetMap",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "openstreetmap.html#styling-plots",
    "href": "openstreetmap.html#styling-plots",
    "title": "OpenStreetMap",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "openstreetmap.html#coordinate-reference-systems",
    "href": "openstreetmap.html#coordinate-reference-systems",
    "title": "OpenStreetMap",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "openstreetmap.html#manipulating-spatial-tables",
    "href": "openstreetmap.html#manipulating-spatial-tables",
    "title": "OpenStreetMap",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "openstreetmap.html#joins",
    "href": "openstreetmap.html#joins",
    "title": "OpenStreetMap",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "openstreetmap.html#join-districts-with-educational-level-data",
    "href": "openstreetmap.html#join-districts-with-educational-level-data",
    "title": "OpenStreetMap",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "openstreetmap.html#saving-maps-to-figures",
    "href": "openstreetmap.html#saving-maps-to-figures",
    "title": "OpenStreetMap",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "openstreetmap.html#adding-baselayers",
    "href": "openstreetmap.html#adding-baselayers",
    "title": "OpenStreetMap",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "openstreetmap.html#interactive-maps",
    "href": "openstreetmap.html#interactive-maps",
    "title": "OpenStreetMap",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "transport-costs.html#importing-modules",
    "href": "transport-costs.html#importing-modules",
    "title": "Transport costs",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "transport-costs.html#datasets",
    "href": "transport-costs.html#datasets",
    "title": "Transport costs",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "transport-costs.html#inspecting-spatial-data",
    "href": "transport-costs.html#inspecting-spatial-data",
    "title": "Transport costs",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "transport-costs.html#styling-plots",
    "href": "transport-costs.html#styling-plots",
    "title": "Transport costs",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "transport-costs.html#coordinate-reference-systems",
    "href": "transport-costs.html#coordinate-reference-systems",
    "title": "Transport costs",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "transport-costs.html#manipulating-spatial-tables",
    "href": "transport-costs.html#manipulating-spatial-tables",
    "title": "Transport costs",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "transport-costs.html#joins",
    "href": "transport-costs.html#joins",
    "title": "Transport costs",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "transport-costs.html#join-districts-with-educational-level-data",
    "href": "transport-costs.html#join-districts-with-educational-level-data",
    "title": "Transport costs",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "transport-costs.html#saving-maps-to-figures",
    "href": "transport-costs.html#saving-maps-to-figures",
    "title": "Transport costs",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "transport-costs.html#adding-baselayers",
    "href": "transport-costs.html#adding-baselayers",
    "title": "Transport costs",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "transport-costs.html#interactive-maps",
    "href": "transport-costs.html#interactive-maps",
    "title": "Transport costs",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "web-mapping.html#importing-modules",
    "href": "web-mapping.html#importing-modules",
    "title": "Web mapping with CARTO",
    "section": "Importing modules",
    "text": "Importing modules\n\n# Import the pandas library, which is useful for data manipulation and analysis.\nimport pandas as pd\n# Import the geopandas library, which extends pandas to support spatial data operations.\nimport geopandas as gpd\n# Import the Point class from the shapely.geometry module, used for handling geometric points.\nfrom shapely.geometry import Point\n# Import the osmnx library, which simplifies the process of downloading and analyzing street networks and other geospatial data from OpenStreetMap.\nimport osmnx as ox\n# Import the contextily library, which is used for adding basemaps (like raster tiles) to geospatial plots.\nimport contextily as cx\n# Import the pyplot module from matplotlib, a library for creating static, animated, and interactive visualizations in Python.\nimport matplotlib.pyplot as plt\n# Import the CRS class from pyproj, which provides tools for defining and transforming coordinate reference systems.\nfrom pyproj import CRS\n# Operating systems\nimport os\n# Interactive maps\nimport folium\n# Import the display function\nfrom IPython.display import display"
  },
  {
    "objectID": "web-mapping.html#datasets",
    "href": "web-mapping.html#datasets",
    "title": "Web mapping with CARTO",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\n# Create the DataFrame\ndata = {\n    'name': [\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\"],\n    'lon': [-0.1459604, -0.1272057, -0.1319481, -0.173734],\n    'lat': [51.5045975, 51.5007325, 51.5301701, 51.4938451]\n}\npoi_df = pd.DataFrame(data)\n\n# Convert DataFrame to GeoDataFrame\ngeometry = [Point(xy) for xy in zip(poi_df['lon'], poi_df['lat'])]\npoi_gdf = gpd.GeoDataFrame(poi_df, geometry=geometry)\n\n# Set the coordinate reference system (CRS)\npoi_gdf.set_crs(epsg=4326, inplace=True)\n\nprint(poi_gdf)\n\n                         name       lon        lat                   geometry\n0          The British Museum -0.145960  51.504598  POINT (-0.14596 51.50460)\n1                     Big Ben -0.127206  51.500732  POINT (-0.12721 51.50073)\n2                King's Cross -0.131948  51.530170  POINT (-0.13195 51.53017)\n3  The Natural History Museum -0.173734  51.493845  POINT (-0.17373 51.49385)\n\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use gpd.read_file, we then plot it to make sure we are seeing it ‘correctly’.\n\n# Read the shapefile for districts\ndistricts = gpd.read_file(\"data/London/Polygons/districts.shp\")\n\n# Create a simple plot\ndistricts.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\n# Read the shapefile for A roads\na_roads = gpd.read_file(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` file, this would be the function:\n# a_roads = gpd.read_file(\"data/London/Lines/a_roads.geojson\")\n\n# Create a simple plot of the roads\na_roads.plot()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\nNote that we use the method features_from_place, which queries for points in a particular place (London in this case) and creates a GeoDataFrame of OSM features.\n\n# Create an OSM query for \"Greater London, U.K.\"\nquery = \"London, United Kingdom\"\nrestaurants = ox.features_from_place(query, tags={\"amenity\": [\"restaurant\", \"bar\", \"pub\"]})\n# Create a simple plot of the roads\nrestaurants.plot()\n# Display the plot\nplt.show()\n\n\n\n\nAnd to inspect the data queried:\n\nrestaurants.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 12197 entries, ('node', 451152) to ('relation', 17299869)\nColumns: 572 entries, addr:city to ways\ndtypes: geometry(1), object(571)\nmemory usage: 53.6+ MB\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “restaurant in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cells above requires internet connectivity. For more about querying from osm see here.\nImportant: Be careful, if you query too much data, your environment is likely to get stuck."
  },
  {
    "objectID": "web-mapping.html#inspecting-spatial-data",
    "href": "web-mapping.html#inspecting-spatial-data",
    "title": "Web mapping with CARTO",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\n# Read the first 5 rows of the data\nprint(districts.head()) \n\n  DIST_CODE             DIST_NAME  \\\n0      00AA        City of London   \n1      00AB  Barking and Dagenham   \n2      00AC                Barnet   \n3      00AD                Bexley   \n4      00AE                 Brent   \n\n                                            geometry  \n0  POLYGON ((531028.507 181611.160, 531036.062 18...  \n1  POLYGON ((550817.007 184195.999, 550814.000 18...  \n2  POLYGON ((526830.313 187535.453, 526830.302 18...  \n3  POLYGON ((552373.534 174606.900, 552372.893 17...  \n4  POLYGON ((524661.688 184631.047, 524665.261 18...  \n\n\nWe can inspect the object in different ways :\n\n# Read the first row\nprint(districts.iloc[0])\n\n# Read the first column\nprint(districts.iloc[:, 0])\n\n# Read the first row, first column\nprint(districts.iloc[0, 0])\n\n# Read the column \"DIST_NAME\"\nprint(districts['DIST_NAME'])\n\nDIST_CODE                                                 00AA\nDIST_NAME                                       City of London\ngeometry     POLYGON ((531028.5069610038 181611.159611177, ...\nName: 0, dtype: object\n0     00AA\n1     00AB\n2     00AC\n3     00AD\n4     00AE\n5     00AF\n6     00AG\n7     00AH\n8     00AJ\n9     00AK\n10    00AL\n11    00AM\n12    00AN\n13    00AP\n14    00AQ\n15    00AR\n16    00AS\n17    00AT\n18    00AU\n19    00AW\n20    00AX\n21    00AY\n22    00AZ\n23    00BA\n24    00BB\n25    00BC\n26    00BD\n27    00BE\n28    00BF\n29    00BG\n30    00BH\n31    00BJ\n32    00BK\nName: DIST_CODE, dtype: object\n00AA\n0             City of London\n1       Barking and Dagenham\n2                     Barnet\n3                     Bexley\n4                      Brent\n5                    Bromley\n6                     Camden\n7                    Croydon\n8                     Ealing\n9                    Enfield\n10                 Greenwich\n11                   Hackney\n12    Hammersmith and Fulham\n13                  Haringey\n14                    Harrow\n15                  Havering\n16                Hillingdon\n17                  Hounslow\n18                 Islington\n19    Kensington and Chelsea\n20      Kingston upon Thames\n21                   Lambeth\n22                  Lewisham\n23                    Merton\n24                    Newham\n25                 Redbridge\n26      Richmond upon Thames\n27                 Southwark\n28                    Sutton\n29             Tower Hamlets\n30            Waltham Forest\n31                Wandsworth\n32               Westminster\nName: DIST_NAME, dtype: object\n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\n# Filter rows where 'DIST_NAME' is 'City of London'\nfiltered_districts = districts[districts['DIST_NAME'] == 'City of London']\n\nprint(filtered_districts)\n\n  DIST_CODE       DIST_NAME                                           geometry\n0      00AA  City of London  POLYGON ((531028.507 181611.160, 531036.062 18...\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# Plot London in grey\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, color='lightgrey')\n\n# Add city of London (Hackney) in turquoise to the map\nhackney = districts[districts['DIST_NAME'] == 'Hackney']\nhackney.plot(ax=ax, color='turquoise')\n\nplt.show()\n\n\n\n\nSome guidance on colours in Python can be found here."
  },
  {
    "objectID": "web-mapping.html#styling-plots",
    "href": "web-mapping.html#styling-plots",
    "title": "Web mapping with CARTO",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown')  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads in brown\n\nplt.show()\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n\n# Remove the axis\nax.axis('off')\n\nplt.show()\n\n\n\n\nLet us stop for a second a study each of the previous lines:\nWe have first created a figure named fig with one axis named ax by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook). Note how the method is returning two elements and we can assign each of them to objects with different name (fig and ax) by simply listing them at the front of the line, separated by commas.\nSecond, we plot the geographies as before, but this time we tell the function that we want it to draw the polygons on the axis we are passing, ax. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, ax.\nOn the third line, we effectively remove the box with coordinates.\nFinally, we draw the entire plot by calling plt.show().\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n\n# Plot districts with no fill (transparent fill)\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')  # No fill, only border\n# Plot roads with transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)  # Roads with 50% transparency\n# Remove the axis\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display\nplt.show()\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\n# Convert CRS from WGS 84 (EPSG:4326) to British National Grid (EPSG:27700)\npoi_gdf_bng = poi_gdf.to_crs(epsg=27700)\n\n# Plotting the geometries\nfig, ax = plt.subplots()\n# Plot districts with no fill, black borders\ndistricts.plot(ax=ax, edgecolor='black', facecolor='none')\n# Plot roads with brown color and 50% transparency\na_roads.plot(ax=ax, color='brown', alpha=0.5)\n# Plot restaurants with blue color and adjusted size\npoi_gdf_bng.plot(ax=ax, edgecolor='blue', facecolor='blue', markersize=100)# Adjust size accordingly\n# Remove the axis for a clean look\nax.axis('off')\n# Add figure title\nfig.suptitle(\"Main roads in London\")\n# Display the plot\nplt.show()\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\n\niterrows() is a pandas function that iterates over DataFrame rows as (index, Series) pairs. Here, idx is the index of the row, and row is a pandas series containing the data for that particular district.\ncentroid = row['geometry'].centroid gets the centroid of each district’s geometry. row['geometry'] refers to the geometry of the district, which could be a polygon or a multipolygon. .centroid computes the geometric center (centroid) of this polygon.\nax.text() is a method from Matplotlib, used to place text at specific coordinates on the plot. centroid.x and centroid.y provide the x and y coordinates of the centroid, which determine where the text will be placed. row['DIST_NAME'] is the name of the district that will be displayed as the label. fontsize=6 sets the size of the text to 8 points. ha='center' ensures that the text is horizontally aligned to the center of the specified coordinates.\n\n\n# Plot the districts with a gray fill\nfig, ax = plt.subplots()\ndistricts.plot(ax=ax, edgecolor=\"black\", facecolor='none')\n\n# Add text labels at the centroids of the districts\nfor idx, row in districts.iterrows():\n    centroid = row['geometry'].centroid\n    ax.text(centroid.x, centroid.y, row['DIST_NAME'], fontsize=6, ha='center')\n\n# Remove axis\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "web-mapping.html#coordinate-reference-systems",
    "href": "web-mapping.html#coordinate-reference-systems",
    "title": "Web mapping with CARTO",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in Python\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\n# Retrieve the CRS from the GeoDataFrame\ncrs = districts.crs\n# Print the CRS information\nprint(crs)\n\nEPSG:27700\n\n\nWe can also retrieve some additional information about the used CRS. For example, try to run:\n\n# Check if the CRS is geographic or not\nis_geographic = CRS(crs).is_geographic\n# Find out the CRS units\nunits_gdal = CRS(crs).axis_info[0].unit_name if CRS(crs).axis_info else None\n# Extract the SRID\nsrid = CRS(crs).to_epsg()\n# Extract the proj4string representation\nproj4string = CRS(crs).to_proj4()\n\n# Print results\nprint(f\"Is Geographic: {is_geographic}\")\nprint(f\"Units (GDAL): {units_gdal}\")\nprint(f\"SRID: {srid}\")\nprint(f\"Proj4 String: {proj4string}\")\n\nIs Geographic: False\nUnits (GDAL): metre\nSRID: 27700\nProj4 String: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid EPSG:27700), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the .to_crs function can be used:\n\n# Transform the CRS to EPSG:4326\ndistricts_4326 = districts.to_crs(epsg=4326)\n\n# Optionally, print the new CRS to verify\nprint(districts_4326.crs)\n\nEPSG:4326\n\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a GeoDataFrame. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a GeoDataFrame using the coordinates stored in columns 17 and 18, and then we set the GeoDataFrame CRS to the British National Grid (EPSG:27700).\n\n# Import housesales data from CSV\nhousesales = pd.read_csv(\"data/London/Tables/housesales.csv\")\n\n# Filter housesales to include only those with price less than 500000\nhousesales_f = housesales[housesales['price'] &lt; 500000]\n\n# Assume columns 17 and 18 are 'longitude' and 'latitude' respectively\nhousesales_gdf = gpd.GeoDataFrame(\n    housesales_f, geometry=gpd.points_from_xy(housesales_f.greastings, housesales_f.grnorthing), crs=\"EPSG:27700\"\n)\n\nprint(housesales_gdf.head())\n\n   propid   price  lnprice  advance  age  bathroom  bedroom buyage  centheat  \\\n0       1  105000       12    85000   18         1        2     25         1   \n1       2   84000       11    79800   31         1        1     47         0   \n2       3   89000       11    84550   16         1        1     30         0   \n3       4  136000       12   106000   31         2        4     32         2   \n4       5  103550       12    88000   31         1        2     38         1   \n\n   chelec  ...  psemi  pterrace  popdens  prkdouble  prknone  prksingle  \\\n0       0  ...      0         0        3          0        0          0   \n1       0  ...      0         0        0          0        0          0   \n2       0  ...      0         1       23          0        1          0   \n3       1  ...      0         1       23          0        1          0   \n4       0  ...      0         0       39          0        0          1   \n\n   prkspace  tenfree  tenlease                       geometry  \n0         1        0         1  POINT (504998.000 188930.000)  \n1         1        0         1  POINT (505026.000 177041.000)  \n2         0        1         0  POINT (505049.000 189030.000)  \n3         0        1         0  POINT (505087.000 189238.000)  \n4         0        0         1  POINT (505132.000 183300.000)  \n\n[5 rows x 34 columns]\n\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts_4326.plot(ax=ax, color='none', edgecolor='black')\n# Plot the points of interest\npoi_gdf.plot(ax=ax, color='blue', markersize=50)\n# Set the coordinate limits\nax.set_xlim(-0.180723, -0.014212)\nax.set_ylim(51.476668, 51.532337)\n# Remove axis labels and ticks\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "web-mapping.html#manipulating-spatial-tables",
    "href": "web-mapping.html#manipulating-spatial-tables",
    "title": "Web mapping with CARTO",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a GeoDataFrame contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial DataFrame (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, GeoDataFrame also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeoDataFrames come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the GeoDataFrame you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts_areas = districts.area\ndistricts_areas.head()\n\nareas_in_sqkm = districts_areas / 1000000 #convert into squared kilometres\nareas_in_sqkm.head()\n\n0     3.151465\n1    37.778900\n2    86.736434\n3    64.263476\n4    43.235288\ndtype: float64\n\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\nstreet_length = a_roads.length\nstreet_length.head()\n\n0       7.695310\n1     374.714434\n2     417.493662\n3      45.221697\n4    1748.445461\ndtype: float64\n\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncents = districts.centroid\ncents.head()\n\n0    POINT (532464.075 181219.688)\n1    POINT (548021.148 184939.599)\n2    POINT (524027.452 192316.258)\n3    POINT (548927.608 175720.862)\n4    POINT (520176.906 185829.122)\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='lightgrey', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='red', markersize=10, edgecolor='none')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()\n\n\n\n\n\n\nTo create a buffer using geopandas, simply call the buffer method, passing in the radious. For example, to draw a 1000m. buffer around every centroid of every district:\n\n# buffer\nbuf = cents.buffer(1000)\nbuf.head()\n\n0    POLYGON ((533464.075 181219.688, 533459.260 18...\n1    POLYGON ((549021.148 184939.599, 549016.333 18...\n2    POLYGON ((525027.452 192316.258, 525022.637 19...\n3    POLYGON ((549927.608 175720.862, 549922.793 17...\n4    POLYGON ((521176.906 185829.122, 521172.091 18...\ndtype: geometry\n\n\n\n# Create a plot\nfig, ax = plt.subplots(figsize=(10, 10))\n# Plot the districts\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the centroids\ncents.plot(ax=ax, color='black', markersize=20, edgecolor='none')\n# Plot the centroid buffers\nbuf.plot(ax=ax, color='none', alpha=0.5, edgecolor='red')\n# Set minimal theme by removing axes and grid\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "web-mapping.html#joins",
    "href": "web-mapping.html#joins",
    "title": "Web mapping with CARTO",
    "section": "Joins",
    "text": "Joins"
  },
  {
    "objectID": "web-mapping.html#join-districts-with-educational-level-data",
    "href": "web-mapping.html#join-districts-with-educational-level-data",
    "title": "Web mapping with CARTO",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# Import qualifications data from CSV\nqualifications2001_df = pd.read_csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# Take a quick look at the table by reading the first 5 lines\nqualifications2001_df.head()\n\n\n\n\n\n\n\n\nZone_Code\nZone_Name\nPopulation1674\nNoquals\nLevel1\nLevel2\nLevel3\nLevel4\n\n\n\n\n0\n00AA\nCity of London\n6067\n607\n359\n634\n665\n3647\n\n\n1\n00AB\nBarking and Dagenham\n113579\n44873\n21654\n20564\n6626\n11615\n\n\n2\n00AC\nBarnet\n228123\n44806\n25558\n41118\n24695\n80907\n\n\n3\n00AD\nBexley\n156172\n44887\n32110\n35312\n10759\n20704\n\n\n4\n00AE\nBrent\n198712\n48915\n23913\n33280\n21121\n60432\n\n\n\n\n\n\n\nCheck the data\n\n# Join check that the code you are joining the data on is the same.\n# First check the GeoDataFrame\ndistricts.head()\n\n# Then the DataFrame\nqualifications2001_df.head()\n\n## rename(columns={'Zone-Code': 'Dist_code'}) specifies that the column name Zone-Code should be renamed to Dist_code\nqualifications2001_df.rename(columns={'Zone_Code': 'DIST_CODE'}, inplace=True)\n\nMerge the qualification dataset to the districts GeoDataFrame:\n\nMerge the qualifications2001_df DataFrame with the districts GeoDataFrame.\nThe merge is done on the DIST_CODE column, which must be present in both DataFrames.\nBy default, this performs an inner join, but you can specify different types of joins (e.g., left, right) with the ‘how’ parameter.\ngpd.GeoDataFrame(...) converts the resulting merged DataFrame into a GeoDataFrame using the ‘geopandas’ library.\ndistrict_qual is the new GeoDataFrame that contains the combined data from ‘qualifications2001_df’ and ‘districts’.\ndistricts.plot() plots the GeoDataFrame and column='level 4' specifies the column to plot.\nlegend=True adds a legend to the plot and cmap='OrRd' applies a color map.\n\n\ndistrict_qual = districts.merge(qualifications2001_df, on='DIST_CODE')\n# Prove it worked\ndistrict_qual.plot(column=\"Level4\", cmap=\"OrRd\")\n# Show the plot\nplt.show()\n\n\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\n# Assuming 'Level4' and 'Pop' columns exist in the merged GeoDataFrame district_qual\ndistrict_qual['Level4p'] = district_qual['Level4'] / district_qual['Population1674']"
  },
  {
    "objectID": "web-mapping.html#saving-maps-to-figures",
    "href": "web-mapping.html#saving-maps-to-figures",
    "title": "Web mapping with CARTO",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\n# Create directory \"maps\" if it doesn't exist\nos.makedirs(\"maps2\", exist_ok=True)\n\nLet’s create a simple map with the variable we just created and save to external file:\n\n# Create a figure and axes for the plot\nfig, ax = plt.subplots()\n# Plot the districts' geometry with no fill color and black edges\ndistricts.plot(ax=ax, color='none', edgecolor='black')\n# Plot the district qualifications with a color map\ndistrict_qual.plot(ax=ax, column=\"Level4\", cmap=\"OrRd\", legend=True)\n# Save the plot to a PDF file\nplt.savefig(\"maps/london_test2.pdf\")\n# Save the plot as a JPG file\nplt.savefig(\"maps/london_test2.jpg\", format='jpg', dpi=300)\n# Close the plot\nplt.close()\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/fontTools/misc/py23.py:11: DeprecationWarning:\n\nThe py23 module has been deprecated and will be removed in a future release. Please update your code."
  },
  {
    "objectID": "web-mapping.html#adding-baselayers",
    "href": "web-mapping.html#adding-baselayers",
    "title": "Web mapping with CARTO",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nMany single datasets lack context when displayed on their own. A common approach to alleviate this is to use web tiles, which are a way of quickly obtaining geographical context to present spatial data. In Python, we can use contextily to pull down tiles and display them along with our own geographic data.\nWe can begin by creating a map in the same way we would do normally, and then use the add_basemap command to add a basemap:\n\nax = restaurants.plot(color=\"black\")\ncx.add_basemap(ax, crs=restaurants.crs, source=cx.providers.CartoDB.Voyager);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote that we need to be explicit when adding the basemap to state the coordinate reference system (crs) our data is expressed in, contextily will not be able to pick it up otherwise. Conversely, we could change our data’s CRS into Pseudo-Mercator, the native reference system for most web tiles:\n\nrestaurants_wm = restaurants.to_crs(epsg=3857)\nax = restaurants_wm.plot(color=\"black\")\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik);\n\n# Show the plot\nplt.show()\n\n\n\n\nNote how the coordinates are different but, if we set it right, either approach aligns tiles and data nicely.\nNow, contextily offers a lot of options in terms of the sources and providers you can use to create your basemaps. For example, we can use satellite imagery instead. A lot more about basemap options with contextly here.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\ndistricts.plot(alpha=0.5, ax=ax)\ncx.add_basemap(\n    ax, \n    crs=districts.crs,\n    source=cx.providers.Esri.WorldImagery\n)\nax.set_axis_off()\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "web-mapping.html#interactive-maps",
    "href": "web-mapping.html#interactive-maps",
    "title": "Web mapping with CARTO",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nWe wil use the folium library, which is a Python wrapper for Leaflet. Here’s how you can do it:\n\n# Define the locations and popups\nlocations = {\n    \"The British Museum\": (51.5045975, -0.1459604),\n    \"Big Ben\": (51.5007325, -0.1272057),\n    \"King's Cross\": (51.5301701, -0.1319481),\n    \"The Natural History Museum\": (51.4938451, -0.173734)\n}\n\n# Create a base map\nm = folium.Map(location=[51.505, -0.127], zoom_start=14, tiles='CartoDB positron')\n\n# Add markers\nfor popup, (lat, lng) in locations.items():\n    folium.Marker(location=[lat, lng], popup=popup).add_to(m)\n\n# Display the map\ndisplay(m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "McKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython. 1st ed.\nPaperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024.\n“A Course in Geographic\nData Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming.\nGeographic Data Science with PySAL and the PyData Stack. CRC\npress.",
    "crumbs": [
      "Epilogue",
      "References"
    ]
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download data folders from GitHub",
    "section": "",
    "text": "Go to https://download-directory.github.io\n\n\n\nGo to the folder you need for your Lab. For example copy: https://github.com/pietrostefani/gds/tree/main/data/London\n\n\n\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aims\nThe course has three primary aims:",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "",
    "text": "Equip students with essential skills in Geographic Data Science (GDS), improving their statistical and numerical understanding and familiarity with fundamental programming concepts and modern computational tools for GDS;\nOffer a thorough overview of key methodologies used by Geographic Data Scientists, along with insights on how and when to apply them;\nEmphasise practical applications of these techniques within real-world geographical and applied settings.",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this course, students will be able to:\n\nIllustrate advanced techniques in GIS/GDS and use them programmatically for importing, manipulating, and analysing data in various formats.\nExplain the rationale and mechanics behind key methodological approaches in GDS, both from analytical and visual perspectives.\nAssess the suitability of specific techniques, their capabilities, and how they can address relevant questions.\nImplement various spatial analysis methods and interpret the outcomes, transforming raw data into meaningful insights.\nIndependently handle new datasets using GIS/GDS tools in a programmatic manner.\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations.",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "spatial-data.html",
    "href": "spatial-data.html",
    "title": "1  Spatial data",
    "section": "",
    "text": "1.1 Packages and modules\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "geovisualisation.html",
    "href": "geovisualisation.html",
    "title": "2  Geovisualisation",
    "section": "",
    "text": "2.1 Packages and modules\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport seaborn as sns\nfrom pysal.viz import mapclassify as mc\nfrom legendgram import legendgram\nimport matplotlib.pyplot as plt\nimport palettable.matplotlib as palmpl\nfrom splot.mapping import vba_choropleth",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Geovisualisation</span>"
    ]
  },
  {
    "objectID": "geovisualisation.html#data",
    "href": "geovisualisation.html#data",
    "title": "2  Geovisualisation",
    "section": "2.2 Data",
    "text": "2.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ./data/:\n\ndb = geopandas.read_file(\"./data/cambodia_regional.gpkg\")\n\nQuick visualisation:\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    edgecolor=\"red\",\n    facecolor=\"none\",\n    linewidth=2,\n    alpha=0.25,\n    figsize=(9, 9),\n    ax=ax\n)\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.Esri.NatGeoWorldMap\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndb.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 198 entries, 0 to 197\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_name   198 non-null    object  \n 1   adm2_altnm  122 non-null    object  \n 2   motor_mean  198 non-null    float64 \n 3   walk_mean   198 non-null    float64 \n 4   no2_mean    198 non-null    float64 \n 5   geometry    198 non-null    geometry\ndtypes: float64(3), geometry(1), object(2)\nmemory usage: 9.4+ KB\n\n\nWe will use the average measurement of nitrogen dioxide (no2_mean) by region throughout the block.\nTo make visualisation a bit easier below, we create an additional column with values rescaled:\n\ndb[\"no2_viz\"] = db[\"no2_mean\"] * 1e5\n\nThis way, numbers are larger and will fit more easily on legends:\n\ndb[[\"no2_mean\", \"no2_viz\"]].describe()\n\n\n\n\n\n\n\n\nno2_mean\nno2_viz\n\n\n\n\ncount\n198.000000\n198.000000\n\n\nmean\n0.000032\n3.236567\n\n\nstd\n0.000017\n1.743538\n\n\nmin\n0.000014\n1.377641\n\n\n25%\n0.000024\n2.427438\n\n\n50%\n0.000029\n2.922031\n\n\n75%\n0.000034\n3.390426\n\n\nmax\n0.000123\n12.323324",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Geovisualisation</span>"
    ]
  },
  {
    "objectID": "geovisualisation.html#choropleths",
    "href": "geovisualisation.html#choropleths",
    "title": "2  Geovisualisation",
    "section": "2.3 Choropleths",
    "text": "2.3 Choropleths\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    \"no2_viz\", \n    legend=True,\n    figsize=(12, 9),\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n2.3.1 A classiffication problem\n\ndb[\"no2_viz\"].unique().shape\n\n(198,)\n\n\n\nsns.displot(\n    db, x=\"no2_viz\", kde=True, aspect=2\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.2 How to assign colors?\n\n\n\n\n\n\nImportant\n\n\n\nTo build an intuition behind each classification algorithm more easily, we create a helper method (plot_classi) that generates a visualisation of a given classification.\n\n\n\ndef plot_classi(classi, col, db):\n    \"\"\"\n    Illustrate a classiffication\n    ...\n    \n    Arguments\n    ---------\n    classi : mapclassify.classifiers\n             Classification object\n    col    : str\n             Column name used for `classi`\n    db     : geopandas.GeoDataFrame\n             Geo-table with data for\n             the classification    \n    \"\"\"\n    f, ax = plt.subplots(figsize=(12, 6))\n    ax.set_title(classi.name)\n    # KDE\n    sns.kdeplot(\n        db[col], fill=True, ax=ax\n    )\n    for i in range(0, len(classi.bins)-1):\n        ax.axvline(classi.bins[i], color=\"red\")\n    # Map\n    aux = f.add_axes([.6, .45, .32, .4])\n    db.assign(lbls=classi.yb).plot(\n        \"lbls\", cmap=\"viridis\", ax=aux\n    )\n    aux.set_axis_off()\n    plt.show()\n    return None\n\n\nEqual intervals\n\n\nclassi = mc.EqualInterval(db[\"no2_viz\"], k=7)\nclassi\n\nEqualInterval\n\n   Interval      Count\n----------------------\n[ 1.38,  2.94] |   103\n( 2.94,  4.50] |    80\n( 4.50,  6.07] |     6\n( 6.07,  7.63] |     1\n( 7.63,  9.20] |     3\n( 9.20, 10.76] |     0\n(10.76, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\n\n\n\n\nQuantiles\n\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\nclassi\n\nQuantiles\n\n   Interval      Count\n----------------------\n[ 1.38,  2.24] |    29\n( 2.24,  2.50] |    28\n( 2.50,  2.76] |    28\n( 2.76,  3.02] |    28\n( 3.02,  3.35] |    28\n( 3.35,  3.76] |    28\n( 3.76, 12.32] |    29\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\n\n\n\n\nFisher-Jenks\n\n\nclassi = mc.FisherJenks(db[\"no2_viz\"], k=7)\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\n\n\n\n\nNow let’s dig into the internals of classi:\n\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nclassi.k\n\n7\n\n\n\nclassi.bins\n\narray([ 2.05617382,  2.6925931 ,  3.30281182,  4.19124954,  5.63804861,\n        9.19190206, 12.32332434])\n\n\n\nclassi.yb\n\narray([2, 3, 3, 1, 1, 2, 1, 1, 1, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 1, 2, 0,\n       0, 4, 2, 1, 3, 1, 0, 0, 0, 1, 2, 2, 6, 5, 4, 2, 1, 3, 2, 3, 2, 1,\n       2, 3, 2, 3, 1, 1, 3, 1, 2, 3, 3, 1, 3, 3, 1, 0, 1, 1, 3, 2, 0, 0,\n       2, 1, 0, 0, 0, 2, 0, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 3, 1, 1, 1, 1,\n       2, 1, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1, 2, 3, 3, 2, 0, 3,\n       1, 0, 1, 2, 1, 1, 2, 1, 2, 6, 5, 6, 2, 2, 3, 6, 3, 4, 3, 4, 2, 3,\n       0, 2, 5, 6, 4, 5, 2, 2, 2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2,\n       1, 1, 3, 4, 2, 1, 3, 1, 2, 3, 4, 0, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2,\n       2, 2, 0, 0, 1, 2, 3, 3, 3, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1])\n\n\n\n\n2.3.3 How many colors?\nThe code used to generate the next figure uses more advanced features than planned for this course.\nIf you want to inspect it, look at the code cell below.\n\nvals = [3, 5, 7, 9, 12, 15]\nalgos = [\"equal_interval\", \"quantiles\", \"fisherjenks\"]\nf, axs = plt.subplots(\n    len(algos), len(vals), figsize=(3*len(vals), 3*len(algos))\n)\nfor i in range(len(algos)):\n    for j in range(len(vals)):\n        db.plot(\n            \"no2_viz\", scheme=algos[i], k=vals[j], ax=axs[i, j]\n        )\n        axs[i, j].set_axis_off()\n        if i==0:\n            axs[i, j].set_title(f\"k={vals[j]}\")\n        if j==0:\n            axs[i, j].text(\n                -0.1, \n                0.5, \n                algos[i], \n                horizontalalignment='center',\n                verticalalignment='center', \n                transform=axs[i, j].transAxes,\n                rotation=90\n            )\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.4 Using the right color\nFor a “safe” choice, make sure to visit ColorBrewer\n\n Categories, non-ordered\n Graduated, sequential\n Graduated, divergent\n\n\n\n2.3.5 Choropleths on Geo-Tables\n\n2.3.5.1 Streamlined\nHow can we create classifications from data on geo-tables? Two ways:\n\nDirectly within plot (only for some algorithms)\n\n\nfig, ax = plt.subplots()\ndb.plot(\n    \"no2_viz\", scheme=\"quantiles\", k=7, legend=True, ax=ax\n)\nplt.show()\n\n\n\n\n\n\n\n\nSee this tutorial for more details on fine tuning choropleths manually.\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Create an equal interval map with five bins for no2_viz .\n\n\n\n\n2.3.5.2 Manual approach\nThis is valid for any algorithm and provides much more flexibility at the cost of effort.\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\nfig, ax = plt.subplots()\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.5.3 Value by alpha mapping\n\ndb['area_inv'] = 1 / db.to_crs(epsg=5726).area\n\n\nfig, ax = plt.subplots()\ndb.plot('area_inv', scheme='quantiles', ax=ax)\nax.set_title('area_inv')\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set up figure and axis\nfig, ax = plt.subplots(1, figsize=(12, 9))\n# VBA choropleth\nvba_choropleth(\n    'no2_viz',          # Column for color \n    'area_inv',         # Column for transparency (alpha)\n    db,                 # Geo-table\n    rgb_mapclassify={   # Options for color classification\n        'classifier': 'quantiles', 'k':5\n    },\n    alpha_mapclassify={ # Options for alpha classification\n        'classifier': 'quantiles', 'k':5\n    },\n    legend=True,        # Add legend\n    ax=ax               # Axis\n)\n# Add boundary lines\ndb.plot(color='none', linewidth=0.05, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\nSee here for more examples of value-by-alpha (VBA) mapping.\n\n\n2.3.5.4 Legendgrams\nLegendgrams are a way to more closely connect the statistical characteristics of your data to the map display.\n\n\n\n\n\n\nWarning\n\n\n\nLegendgram are in an experimental development stage, so the code is a bit more involved and less stable. Use at your own risk!\n\n\nHere is an example:\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nlegendgram(\n    fig,                   # Figure object\n    ax,                  # Axis object of the map\n    db[\"no2_viz\"],       # Values for the histogram\n    classi.bins,         # Bin boundaries\n    pal=palmpl.Viridis_7,# color palette (as palettable object)\n    legend_size=(.5,.2), # legend size in fractions of the axis\n    loc = 'lower right', # matplotlib-style legend locations\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Give Task I and II from the GDS course a go.\n\n\n\n\n\n2.3.6 Choropleths on surfaces\nAssuming you have the file locally on the path ./data/:\n\ngrid = xarray.open_rasterio(\n    \"./data/cambodia_s5_no2.tif\"\n).sel(band=1)\n\n/var/folders/79/65l52xsj7vq_4_t_l6k5bl2c0000gn/T/ipykernel_30848/3548825724.py:1: DeprecationWarning:\n\nopen_rasterio is Deprecated in favor of rioxarray. For information about transitioning, see: https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html\n\n\n\n\ngrid = rioxarray.open_rasterio(\"./data/cambodia_s5_no2.tif\").sel(band=1)\n\n\nImplicit continuous equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", robust=True, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscrete equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", levels=7, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCombining with mapclassify\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.Quantiles(\n    grid_nona.to_series().dropna(), k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.FisherJenksSampled(\n    grid_nona.to_series().dropna().values, k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.StdMean(\n    grid_nona.to_series().dropna().values\n)\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.BoxPlot(\n    grid_nona.to_series().dropna().values\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Read the satellite image for Madrid used in the Chapter 1 and create three choropleths, one for each band, using the colormapsReds, Greens, Blues.\nPlay with different classification algorithms.\n\nDo the results change notably?\nIf so, why do you think that is?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Geovisualisation</span>"
    ]
  },
  {
    "objectID": "geovisualisation.html#next-steps",
    "href": "geovisualisation.html#next-steps",
    "title": "2  Geovisualisation",
    "section": "2.4 Next steps",
    "text": "2.4 Next steps\nIf you are interested in statistical maps based on classification, here are two recommendations to check out next:\n\nOn the technical side, the documentation for mapclassify (including its tutorials) provides more detail and illustrates more classification algorithms than those reviewed in this block.\nOn a more conceptual note, Cynthia Brewer’s “Designing better maps” (Brewer 2015) is an excellent blueprint for good map making.\n\n\n\n\n\nArribas-Bel, Dani. 2019. “A Course on Geographic Data Science.” The Journal of Open Source Education 2 (14). https://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS Users. ESRI press.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Geovisualisation</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#data",
    "href": "spatial-data.html#data",
    "title": "1  Spatial data",
    "section": "1.2 Data",
    "text": "1.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\n\n1.2.1 Points\nAssuming you have the file locally on the path ./data/:\n\npts = geopandas.read_file(\"./data/madrid_abb.gpkg\")\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes, points are provided as separate columns in an otherwise non-spatial table. For example imagine we have an object cols with a column named X for longitude and Y for latitude. Then, we can convert those into proper geometries by running pts = geopandas.GeoSeries( geopandas.points_from_xy(cols[\"X\"], cols[\"Y\"]).\n\n\nLet’s explore the points dataset that we loaded above.\n\npts.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 18399 entries, 0 to 18398\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   price            18399 non-null  object  \n 1   price_usd        18399 non-null  float64 \n 2   log1p_price_usd  18399 non-null  float64 \n 3   accommodates     18399 non-null  int64   \n 4   bathrooms        18399 non-null  object  \n 5   bedrooms         18399 non-null  float64 \n 6   beds             18399 non-null  float64 \n 7   neighbourhood    18399 non-null  object  \n 8   room_type        18399 non-null  object  \n 9   property_type    18399 non-null  object  \n 10  WiFi             18399 non-null  object  \n 11  Coffee           18399 non-null  object  \n 12  Gym              18399 non-null  object  \n 13  Parking          18399 non-null  object  \n 14  km_to_retiro     18399 non-null  float64 \n 15  geometry         18399 non-null  geometry\ndtypes: float64(5), geometry(1), int64(1), object(9)\nmemory usage: 2.2+ MB\n\n\n\npts.head()\n\n\n\n\n\n\n\n\nprice\nprice_usd\nlog1p_price_usd\naccommodates\nbathrooms\nbedrooms\nbeds\nneighbourhood\nroom_type\nproperty_type\nWiFi\nCoffee\nGym\nParking\nkm_to_retiro\ngeometry\n\n\n\n\n0\n$60.00\n60.0\n4.110874\n2\n1 shared bath\n1.0\n1.0\nHispanoamérica\nPrivate room\nPrivate room in apartment\n1\n0\n0\n0\n5.116664\nPOINT (-3.67688 40.45724)\n\n\n1\n$31.00\n31.0\n3.465736\n1\n1 bath\n1.0\n1.0\nCármenes\nPrivate room\nPrivate room in apartment\n1\n1\n0\n1\n5.563869\nPOINT (-3.74084 40.40341)\n\n\n2\n$60.00\n60.0\n4.110874\n6\n2 baths\n3.0\n5.0\nLegazpi\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n3.048442\nPOINT (-3.69304 40.38695)\n\n\n3\n$115.00\n115.0\n4.753590\n4\n1.5 baths\n2.0\n3.0\nJusticia\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n2.075484\nPOINT (-3.69764 40.41995)\n\n\n4\n$26.00\n26.0\n3.295837\n1\n1 private bath\n1.0\n1.0\nLegazpi\nPrivate room\nPrivate room in house\n1\n0\n0\n0\n2.648058\nPOINT (-3.69011 40.38985)\n\n\n\n\n\n\n\n\n\n1.2.2 Lines\nAssuming you have the file locally on the path ./data/:\n\nlines = geopandas.read_file(\"./data/arturo_streets.gpkg\")\n\n\nlines.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 66499 entries, 0 to 66498\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   OGC_FID             66499 non-null  object  \n 1   dm_id               66499 non-null  object  \n 2   dist_barri          66483 non-null  object  \n 3   average_quality     66499 non-null  float64 \n 4   population_density  66499 non-null  float64 \n 5   X                   66499 non-null  float64 \n 6   Y                   66499 non-null  float64 \n 7   value               5465 non-null   float64 \n 8   geometry            66499 non-null  geometry\ndtypes: float64(5), geometry(1), object(3)\nmemory usage: 4.6+ MB\n\n\n\nlines.loc[0, \"geometry\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Print descriptive statistics for population_density and average_quality.\n\n\n\n\n1.2.3 Polygons\nAssuming you have the file locally on the path ./data/:\n\npolys = geopandas.read_file(\"./data/neighbourhoods.geojson\")\n\n\npolys.head()\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n0\nPalacio\nCentro\nMULTIPOLYGON (((-3.70584 40.42030, -3.70625 40...\n\n\n1\nEmbajadores\nCentro\nMULTIPOLYGON (((-3.70384 40.41432, -3.70277 40...\n\n\n2\nCortes\nCentro\nMULTIPOLYGON (((-3.69796 40.41929, -3.69645 40...\n\n\n3\nJusticia\nCentro\nMULTIPOLYGON (((-3.69546 40.41898, -3.69645 40...\n\n\n4\nUniversidad\nCentro\nMULTIPOLYGON (((-3.70107 40.42134, -3.70155 40...\n\n\n\n\n\n\n\n\npolys.query(\"neighbourhood_group == 'Retiro'\")\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n13\nPacífico\nRetiro\nMULTIPOLYGON (((-3.67015 40.40654, -3.67017 40...\n\n\n14\nAdelfas\nRetiro\nMULTIPOLYGON (((-3.67283 40.39468, -3.67343 40...\n\n\n15\nEstrella\nRetiro\nMULTIPOLYGON (((-3.66506 40.40647, -3.66512 40...\n\n\n16\nIbiza\nRetiro\nMULTIPOLYGON (((-3.66916 40.41796, -3.66927 40...\n\n\n17\nJerónimos\nRetiro\nMULTIPOLYGON (((-3.67874 40.40751, -3.67992 40...\n\n\n18\nNiño Jesús\nRetiro\nMULTIPOLYGON (((-3.66994 40.40850, -3.67012 40...\n\n\n\n\n\n\n\n\npolys.neighbourhood_group.unique()\n\narray(['Centro', 'Arganzuela', 'Retiro', 'Salamanca', 'Chamartín',\n       'Moratalaz', 'Tetuán', 'Chamberí', 'Fuencarral - El Pardo',\n       'Moncloa - Aravaca', 'Puente de Vallecas', 'Latina', 'Carabanchel',\n       'Usera', 'Ciudad Lineal', 'Hortaleza', 'Villaverde',\n       'Villa de Vallecas', 'Vicálvaro', 'San Blas - Canillejas',\n       'Barajas'], dtype=object)",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html",
    "href": "spatial-feature-i.html",
    "title": "3  Spatial feature engineering (I)",
    "section": "",
    "text": "3.1 Packages and modules\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport numpy as np\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#data",
    "href": "spatial-feature-i.html#data",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.2 Data",
    "text": "3.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ./data/:\n\nregions = geopandas.read_file(\"./data/cambodia_regional.gpkg\")\ncities = geopandas.read_file(\"./data/cambodian_cities.geojson\")\npollution = rioxarray.open_rasterio(\n    \"./data/cambodia_s5_no2.tif\"\n).sel(band=1)\nfriction = rioxarray.open_rasterio(\n    \"./data/cambodia_2020_motorized_friction_surface.tif\"\n).sel(band=1)\n\nCheck both geo-tables and the surface are in the same CRS:\n\n(\n    regions.crs.to_epsg() ==\n    cities.crs.to_epsg() ==\n    pollution.rio.crs.to_epsg()\n)\n\nTrue",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#next-steps",
    "href": "spatial-feature-i.html#next-steps",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.11 Next steps",
    "text": "3.11 Next steps\nIf you are interested in learning more about spatial feature engineering through map matching, the following pointers might be useful to delve deeper into specific types of “data transfer”:\n\nThe datashader library is a great option to transfer geo-tables into surfaces, providing tooling to perform these operations in a highly efficient and performant way.\nWhen aggregating surfaces into geo-tables, the library rasterstats contains most if not all of the machinery you will need.\nFor transfers from polygon to polygon geographies, tobler is your friend. Its official documentation contains examples for different use cases.\n\n\n\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html",
    "href": "spatial-feature-ii.html",
    "title": "4  Spatial feature engineering (II)",
    "section": "",
    "text": "4.1 Packages and modules\nimport pandas, geopandas\nimport numpy as np\nimport contextily\nimport tobler\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#data",
    "href": "spatial-feature-ii.html#data",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.2 Data",
    "text": "4.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ./data/:\n\npts = geopandas.read_file(\"./data/madrid_abb.gpkg\")\n\nWe will be working with a modified version of pts:\n\nSince we will require distance calculations, we will switch to the Spanish official projection\nTo make calculations in the illustration near-instantaneous, we will work with a smaller (random) sample of Airbnb properties (10% of the total)\n\n\ndb = pts.sample(\n    frac=0.1, random_state=123\n).to_crs(epsg=25830)\n\nAs you can see in the description, the new CRS is expressed in metres:\n\ndb.crs\n\n&lt;Projected CRS: EPSG:25830&gt;\nName: ETRS89 / UTM zone 30N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Europe between 6°W and 0°W: Faroe Islands offshore; Ireland - offshore; Jan Mayen - offshore; Norway including Svalbard - offshore; Spain - onshore and offshore.\n- bounds: (-6.0, 35.26, 0.01, 80.49)\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#distance-buffers",
    "href": "spatial-feature-ii.html#distance-buffers",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.3 Distance buffers",
    "text": "4.3 Distance buffers\nHow many Airbnb’s are within 500m of each Airbnb?\n\nfrom pysal.lib import weights\n\nUsing DistanceBand, we can build a spatial weights matrix that assigns 1 to each observation within 500m, and 0 otherwise.\n\nw500m = weights.DistanceBand.from_dataframe(\n    db, threshold=500, binary=True\n)\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/libpysal/weights/weights.py:224: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 86 disconnected components.\n There are 47 islands with ids: 6878, 16772, 15006, 1336, 3168, 15193, 1043, 5257, 4943, 12849, 10609, 11309, 10854, 10123, 3388, 9380, 10288, 13071, 3523, 15316, 3856, 205, 7720, 10454, 18307, 3611, 12405, 10716, 14813, 15467, 1878, 16597, 14329, 7933, 16215, 13525, 13722, 11932, 14456, 8848, 15197, 8277, 9922, 13072, 13852, 5922, 17151.\n\n\n\nThe number of neighbors can be accessed through the cardinalities attribute:\n\nn_neis = pandas.Series(w500m.cardinalities)\nn_neis.head()\n\n11297    213\n2659       5\n16242     21\n15565      9\n14707    159\ndtype: int64\n\n\n\nfig, ax = plt.subplots()\n\ndb.assign(\n    n_neis=n_neis\n).plot(\"n_neis\", markersize=0.1, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate the number of AirBnb properties within 250m of each other property. What is the average?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#distance-rings",
    "href": "spatial-feature-ii.html#distance-rings",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.4 Distance rings",
    "text": "4.4 Distance rings\nHow many Airbnb’s are between 500m and 1km of each Airbnb?\n\nw1km = weights.DistanceBand.from_dataframe(\n    db, threshold=1000, binary=True\n)\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/libpysal/weights/weights.py:224: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 20 disconnected components.\n There are 5 islands with ids: 4943, 12849, 15467, 13525, 11932.\n\n\n\nNow, we could do simply a subtraction:\n\nn_ring_neis = pandas.Series(w1km.cardinalities) - n_neis\n\nOr, if we need to know which is which, we can use set operations on weights:\n\nw_ring = weights.w_difference(w1km, w500m, constrained=False)\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/libpysal/weights/weights.py:224: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 34 disconnected components.\n There are 23 islands with ids: 3744, 4143, 4857, 4943, 6986, 8345, 8399, 9062, 10592, 10865, 11574, 11613, 11785, 11840, 11932, 12015, 12635, 12714, 12849, 13091, 13317, 13525, 15467.\n\n\n\nAnd we can confirm they’re both the same:\n\n(pandas.Series(w_ring.cardinalities) - n_ring_neis).sum()\n\n0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you create a plot with the following two lines?\n\nOne depicting the average number of properties within a range of 50m, 100m, 250m, 500m, 750m\nAnother one with the increase of average neighbors for the same distances above",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#cluster-membership-points",
    "href": "spatial-feature-ii.html#cluster-membership-points",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.5 Cluster membership (points)",
    "text": "4.5 Cluster membership (points)\nWe can use the spatial configuration of observations to classify them as part of clusters or not, which can then be encoded, for example, as dummy variables in a model.\nThese magic numbers need to be pre-set and you can play with both min_pct (or min_pts directly) and eps to see how they affect the results (spoiler: a lot!).\n\nfrom sklearn.cluster import DBSCAN\n\nmin_pct = 2\nmin_pts = len(db) * min_pct // 100\neps = 500\n\nWe will illustrate it with a minimum number of points of min_pct % of the sample and a maximum radious of eps metres.\n\nmodel = DBSCAN(min_samples=min_pts, eps=eps)\nmodel.fit(\n    db.assign(\n        x=db.geometry.x\n    ).assign(\n        y=db.geometry.y\n    )[['x', 'y']]\n);\n\nWe will attach the labels to db for easy access:\n\ndb[\"labels\"] = model.labels_\n\nWe can define boundaries to turn point clusters into polygons if that fits our needs better:\nThe code in the next cell is a bit more advanced than expected for this course, but is used here as an illustration.\n\nfrom pysal.lib import cg\n\nboundaries = []\ncl_ids = [i for i in db[\"labels\"].unique() if i!=-1]\nfor cl_id in cl_ids:\n    sub = db.query(f\"labels == {cl_id}\")\n    cluster_boundaries = cg.alpha_shape_auto(\n        np.array(\n            [sub.geometry.x, sub.geometry.y]\n        ).T,\n    )\n    boundaries.append(cluster_boundaries)\nboundaries = geopandas.GeoSeries(\n    boundaries, index=cl_ids, crs=db.crs\n)\n\nAnd we can see what the clusters look like:\n\nfig, ax = plt.subplots()\n\ndb.to_crs(\n    epsg=3857\n).plot(\n    markersize=0.1, color=\"lime\"\n)\nboundaries.to_crs(\n    epsg=3857\n).plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\n\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.CartoDB.DarkMatterNoLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How does the map above change if you require 5% of points instead of 2% for a candidate cluster to be considered so?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#cluster-membership-polygons",
    "href": "spatial-feature-ii.html#cluster-membership-polygons",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.6 Cluster membership (polygons)",
    "text": "4.6 Cluster membership (polygons)\nWe can take a similar approach as above if we have polygon geographies instead of points. Rather than using DBSCAN, here we can rely on local indicators of spatial association (LISAs) to pick up spatial concentrations of high or low values.\nFor the illustration, we will aggregate the location of Airbnb properties to a regular hexagonal grid, similar to how we generated it when transferring from polygons to polygons. First we create a polygon covering the extent of points:\n\none = geopandas.GeoSeries(\n    [cg.alpha_shape_auto(\n        np.array(\n            [db.geometry.x, db.geometry.y]\n        ).T,\n    )],\n    crs=db.crs\n)\n\nThen we can tessellate:\n\nabb_hex = tobler.util.h3fy(\n    one, resolution=8\n)\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAnd obtain a count of points in each polygon:\n\ncounts = geopandas.sjoin(\n    db, abb_hex\n).groupby(\n    \"index_right\"\n).size()\n\nabb_hex[\"count\"] = counts\nabb_hex[\"count\"] = abb_hex[\"count\"].fillna(0)\n\nfig, ax = plt.subplots()\n\nabb_hex.plot(\"count\", scheme=\"fisherjenks\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo identify spatial clusters, we rely on esda:\n\nfrom pysal.explore import esda\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/spaghetti/network.py:40: FutureWarning:\n\nThe next major release of pysal/spaghetti (2.0.0) will drop support for all ``libpysal.cg`` geometries. This change is a first step in refactoring ``spaghetti`` that is expected to result in dramatically reduced runtimes for network instantiation and operations. Users currently requiring network and point pattern input as ``libpysal.cg`` geometries should prepare for this simply by converting to ``shapely`` geometries.\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\n\nAnd compute the LISA statistics:\n\nw = weights.Queen.from_dataframe(abb_hex)\nlisa = esda.Moran_Local(abb_hex[\"count\"], w)\n\n/var/folders/79/65l52xsj7vq_4_t_l6k5bl2c0000gn/T/ipykernel_31016/2473509840.py:1: FutureWarning:\n\n`use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n\n\n\nFor a visual inspection of the clusters, splot:\n\nfrom pysal.viz import splot\nfrom splot.esda import lisa_cluster\n\n\nlisa_cluster(lisa, abb_hex, p=0.01)\nplt.show()\n\n\n\n\n\n\n\n\nAnd, if we want to extract the labels for each polygon, we can do so from the lisa object:\n\nlisa.q * (lisa.p_sim &lt; 0.01)\n\narray([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 0, 0, 0, 3,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n       3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0,\n       0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0,\n       0, 3, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3,\n       0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n       0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n       0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0,\n       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0])",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-ii.html#next-steps",
    "href": "spatial-feature-ii.html#next-steps",
    "title": "4  Spatial feature engineering (II)",
    "section": "4.7 Next steps",
    "text": "4.7 Next steps\nIf you want a bit more background into some of the techniques reviewed in this block, the following might be of interest:\n\nThis block of the GDS Course {cite}darribas_gds_course will introduce you to more techniques like the LISAs seen above to explore the spatial dimension of the statistical properties of your data. If you want a more detailed read, this Chapter of the GDS Book {cite}reyABwolf will do just that.\nThis block of the GDS Course {cite}darribas_gds_course will introduce you to more techniques like the LISAs seen above to explore the spatial dimension of the statistical properties of your data. If you want a more detailed read, this Chapter of the GDS Book {cite}reyABwolf will do just that.\nThis block of the GDS Course {cite}darribas_gds_course will introduce you to more techniques for exploring point patterns. If you want a more comprehensive read, this Chapter of the GDS Book {cite}reyABwolf will do just that.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (II)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#polygons-to-points",
    "href": "spatial-feature-i.html#polygons-to-points",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.3 Polygons to points",
    "text": "3.3 Polygons to points\nIn which region is a city?\n\nsj = geopandas.sjoin(cities, regions)\n\n\n#   City name | Region name\nsj[[\"UC_NM_MN\", \"adm2_name\"]]\n\n\n\n\n\n\n\n\nUC_NM_MN\nadm2_name\n\n\n\n\n0\nSampov Lun\nSampov Lun\n\n\n1\nKhum Pech Chenda\nPhnum Proek\n\n\n2\nPoipet\nPaoy Paet\n\n\n3\nSisophon\nSerei Saophoan\n\n\n4\nBattambang\nBattambang\n\n\n5\nSiem Reap\nSiem Reap\n\n\n6\nSihanoukville\nPreah Sihanouk\n\n\n7\nN/A\nTrapeang Prasat\n\n\n8\nKampong Chhnang\nKampong Chhnang\n\n\n9\nPhnom Penh\nTuol Kouk\n\n\n10\nKampong Cham\nKampong Cham\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties and neighbourhoods datasets, can you determine the neighbourhood group of the first ten properties?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#points-to-polygons",
    "href": "spatial-feature-i.html#points-to-polygons",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.4 Points to polygons",
    "text": "3.4 Points to polygons\nIf we were after the number of cities per region, it is a similar approach, with a (groupby) twist at the end.\n\nWe set_index to align both tables\nWe assign to create a new column\n\nIf you want no missing values, you can fillna(0) since you know missing data are zeros.\n\nregions.set_index(\n    \"adm2_name\"\n).assign(\n    city_count=sj.groupby(\"adm2_name\").size()\n).info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 198 entries, Mongkol Borei to Administrative unit not available\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_altnm  122 non-null    object  \n 1   motor_mean  198 non-null    float64 \n 2   walk_mean   198 non-null    float64 \n 3   no2_mean    198 non-null    float64 \n 4   geometry    198 non-null    geometry\n 5   city_count  11 non-null     float64 \ndtypes: float64(4), geometry(1), object(1)\nmemory usage: 18.9+ KB\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties, can you compute how many properties each neighbourhood group has?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#surface-to-points",
    "href": "spatial-feature-i.html#surface-to-points",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.5 Surface to points",
    "text": "3.5 Surface to points\nConsider attaching to each city in cities the pollution level, as expressed in pollution.\nThe code for generating the next figure is a bit more advanced as it fiddles with text, but if you want to explore it you can look at the code cell below.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    ax=ax, add_colorbar=False\n)\n\nfor i, row in cities.iterrows():\n    plt.text(\n        row.geometry.x,\n        row.geometry.y,\n        row[\"UC_NM_MN\"],\n        fontdict={\"color\": \"white\"},\n    )\n    \ncities.plot(ax=ax, color=\"r\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom rasterstats import point_query\n\ncity_pollution = point_query(\n    cities,\n    pollution.values,\n    affine=pollution.rio.transform(),\n    nodata=pollution.rio.nodata\n)\ncity_pollution\n\n[3.9397064813333136e-05,\n 3.4949825609644426e-05,\n 3.825255125820345e-05,\n 4.103826573585785e-05,\n 3.067677208474005e-05,\n 5.108273256655399e-05,\n 2.2592785882580366e-05,\n 4.050414400882722e-05,\n 2.4383652926989897e-05,\n 0.0001285838935209779,\n 3.258245740282522e-05]\n\n\nAnd we can map these on the city locations:\n\nfig, ax = plt.subplots()\n\ncities.assign(\n    pollution=city_pollution\n).plot(\n    \"pollution\", \n    cmap=\"YlOrRd\",\n    legend=True,\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax,\n    crs=cities.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you calculate the pollution level at the centroid of each Cambodian region in the regional aggregates dataset? how does it compare to their average value?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#surface-to-polygons",
    "href": "spatial-feature-i.html#surface-to-polygons",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.6 Surface to polygons",
    "text": "3.6 Surface to polygons\nInstead of transferring to points, we want to aggregate all the information in a surface that falls within a polygon.\nFor this case, we will use the motorised friction surface. The question we are asking thus is: what is the average degree of friction of each region? Or, in other words: what regions are harder to get through with motorised transport?\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nfriction.plot.imshow(\n    add_colorbar=False, ax=ax\n)\nregions.plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\ncontextily.add_basemap(\n    ax, \n    crs=regions.crs,\n    source=contextily.providers.CartoDB.DarkMatterOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n\n\n\nAgain, we can rely on rasterstats. The output is returned from zonal_stats as a list of dicts. To make it more manageable, we convert it into a pandas.DataFrame.\n\nfrom rasterstats import zonal_stats\n\nregional_friction = pandas.DataFrame(\n    zonal_stats(\n        regions,\n        friction.values,\n        affine=friction.rio.transform(),\n        nodata=friction.rio.nodata\n    ),\n    index=regions.index\n)\nregional_friction.head()\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.001200\n0.037000\n0.006494\n979\n\n\n1\n0.001200\n0.060000\n0.007094\n1317\n\n\n2\n0.001200\n0.024112\n0.006878\n324\n\n\n3\n0.001333\n0.060000\n0.009543\n758\n\n\n4\n0.001200\n0.060132\n0.008619\n55\n\n\n\n\n\n\n\nThis can then also be mapped onto the polygon geography:\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nregions.to_crs(\n    epsg=3857\n).join(\n    regional_friction\n).plot(\n    \"mean\", scheme=\"quantiles\", ax=ax\n)\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above to obtain the average friction for each region using the walking surface (cambodia_2020_walking_friction_surface.tif).",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#surface-to-surface",
    "href": "spatial-feature-i.html#surface-to-surface",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.7 Surface to surface",
    "text": "3.7 Surface to surface\nIf we want to align the pollution surface with that of friction, we need to resample them to make them “fit on the same frame”.\n\npollution.shape\n\n(138, 152)\n\n\n\nfriction.shape\n\n(574, 636)\n\n\nThis involves either moving one surface to the frame of the other one, or both into an entirely new one. For the sake of the illustration, we will do the latter and select a frame that is 300 by 400 pixels. Note this involves stretching (upsampling) pollution, while compressing (downsampling) friction.\n\n# Define dimensions\ndimX, dimY = 300, 400\nminx, miny, maxx, maxy = pollution.rio.bounds()\n# Create XY indices\nys = np.linspace(miny, maxy, dimY)\nxs = np.linspace(minx, maxx, dimX)\n# Set up placeholder array\ncanvas = xarray.DataArray(\n    np.zeros((dimY, dimX)),\n    coords=[ys, xs],\n    dims=[\"y\", \"x\"]\n).rio.write_crs(4326) # Add CRS\n\n\ncvs_pollution = pollution.rio.reproject_match(canvas)\ncvs_friction = friction.rio.reproject_match(canvas)\n\n\ncvs_pollution.shape\n\n(400, 300)\n\n\n\ncvs_pollution.shape == cvs_friction.shape\n\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Trasfer the pollution surface to the frame of friction, and viceversa,\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe following methods involve modelling and are thus more sophisticated. Take these as a conceptual introduction with an empirical illustration, but keep in mind there are extense literatures on each of them and these cover some of the simplest cases.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#points-to-points",
    "href": "spatial-feature-i.html#points-to-points",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.8 Points to points",
    "text": "3.8 Points to points\nSee this section of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) for more details on the technique.\nFor this exampe, we will assume that, instead of a surface with pollution values, we only have available a sample of points and we would like to obtain estimates for other locations.\nFor that we will first generate 100 random points within the extent of pollution which we will take as the location of our measurement stations.\nThe code in this cell contains bits that are a bit more advanced, do not despair if not everything makes sense!\n\nnp.random.seed(123456)\n\nbb = pollution.rio.bounds()\nstation_xs = np.random.uniform(bb[0], bb[2], 100)\nstation_ys = np.random.uniform(bb[1], bb[3], 100)\nstations = geopandas.GeoSeries(\n    geopandas.points_from_xy(station_xs, station_ys),\n    crs=\"EPSG:4326\"\n)\n\nOur station values come from the pollution surface, but we assume we do not have access to the latter, and we would like to obtain estimates for the location of the cities:\n\nfig, ax = plt.subplots(1, figsize=(6, 6))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    add_colorbar=False, cmap=\"Blues\", ax=ax\n)\n\nstations.plot(ax=ax, color=\"red\", label=\"Stations\")\ncities.plot(ax=ax, color=\"lime\", label=\"Cities\")\n\nax.set_title(\"Pollution sampling\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nWe will need the location and the pollution measurements for every station as separate arrays. Before we do that, since we will be calculating distances, we convert our coordinates to a system expressed in metres.\n\nstations_mt = stations.to_crs(epsg=5726)\nstation_xys = np.array(\n    [stations_mt.geometry.x, stations_mt.geometry.y]\n).T\n\nWe also need to extract the pollution measurements for each station location:\n\nstation_measurements = np.array(\n    point_query(\n        stations,\n        pollution.values,\n        affine=pollution.rio.transform(),\n        nodata=pollution.rio.nodata\n    )\n)\n\nAnd finally, we will also need the locations of each city expressed in the same coordinate system:\n\ncities_mt = cities.to_crs(epsg=5726)\ncity_xys = np.array(\n    [cities_mt.geometry.x, cities_mt.geometry.y]\n).T\n\nFor this illustration, we will use a \\(k\\)-nearest neighbors regression that estimates the value for each target point (cities in our case) as the average weighted by distance of its \\(k\\) nearest neigbours. In this illustration we will use \\(k=10\\).\nNote how sklearn relies only on array data structures, hence why we first had to express all the required information in that format.\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor(\n    n_neighbors=10, weights=\"distance\"\n).fit(station_xys, station_measurements)\n\nOnce we have trained the model, we can use it to obtain predictions for each city location:\n\npredictions = model.predict(city_xys)\n\nThese can be compared with the originally observed values:\n\np2p_comparison = pandas.DataFrame(\n    {\n        \"Observed\": city_pollution,\n        \"Predicted\": predictions\n    },\n    index=cities[\"UC_NM_MN\"]\n)\n\n\nfig, ax = plt.subplots(1)\np2p_comparison[\"Observed\"].plot.kde(ax=ax)\np2p_comparison[\"Predicted\"].plot.kde(ax=ax)\nax.set_axis_off()\nplt.legend(frameon=False, fontsize=20)\nplt.show()\n\n\n\n\n\n\n\n\n\np2p_comparison\n\n\n\n\n\n\n\n\nObserved\nPredicted\n\n\nUC_NM_MN\n\n\n\n\n\n\nSampov Lun\n0.000039\n0.000027\n\n\nKhum Pech Chenda\n0.000035\n0.000025\n\n\nPoipet\n0.000038\n0.000030\n\n\nSisophon\n0.000041\n0.000030\n\n\nBattambang\n0.000031\n0.000027\n\n\nSiem Reap\n0.000051\n0.000027\n\n\nSihanoukville\n0.000023\n0.000019\n\n\nN/A\n0.000041\n0.000028\n\n\nKampong Chhnang\n0.000024\n0.000032\n\n\nPhnom Penh\n0.000129\n0.000042\n\n\nKampong Cham\n0.000033\n0.000033\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above with \\(k=15\\) and \\(k=5\\). Do results change? Why do you think that is?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#points-to-surface",
    "href": "spatial-feature-i.html#points-to-surface",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.9 Points to surface",
    "text": "3.9 Points to surface\nImagine we do not have a surface like pollution but we need it. In this context, if you have measurements from some locations, such as in stations, we can use the approach reviewed above to generate a surface. The trick to do this is to realise that we can generate a uniform grid of target locations that we can then express as a surface.\nWe will set as our target locations those of the pixels in the target surface we have seen above:\n\ncanvas_mt = canvas.rio.reproject(5726)\n\n\nxy_pairs = canvas_mt.to_series().index\nxys = np.array(\n    [\n        xy_pairs.get_level_values(\"x\"),\n        xy_pairs.get_level_values(\"y\")\n    ]\n).T\n\nTo obtain pollution estimates at each location, we can predict with model:\n\npredictions_grid = model.predict(xys)\n\nAnd with these at hand, we can convert them into a surface:\n\npredictions_series = pandas.DataFrame(\n    {\"predictions_grid\": predictions_grid}\n).join(\n    pandas.DataFrame(xys, columns=[\"x\", \"y\"])\n).set_index([\"y\", \"x\"])\n\npredictions_surface = xarray.DataArray().from_series(\n    predictions_series[\"predictions_grid\"]\n).rio.write_crs(canvas_mt.rio.crs)\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 6))\n\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot(ax=axs[0])\naxs[0].set_title(\"Observed\")\n\npredictions_surface.where(\n    predictions_surface&gt;0\n).rio.reproject_match(\n    cvs_pollution\n).plot(ax=axs[1])\naxs[1].set_title(\"Predicted\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(9, 4))\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, label=\"Observed\"\n)\npredictions_surface.rio.reproject_match(\n    cvs_pollution\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, color=\"g\", label=\"predicted\"\n)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nRoom for improvement but, remember this was a rough first pass!\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Train a model with pollution measurements from each city location and generate a surface from it. How does the output compare to the one above? Why do you think that is?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "spatial-feature-i.html#polygons-to-polygons",
    "href": "spatial-feature-i.html#polygons-to-polygons",
    "title": "3  Spatial feature engineering (I)",
    "section": "3.10 Polygons to polygons",
    "text": "3.10 Polygons to polygons\nIn this final example, we transfer data from a polygon geography to another polygon geography. Effectively, we re-apportion values from one set of areas to another based on the extent of shared area.\nOur illustration will cover how to move pollution estimates from regions into a uniform hexagonal grid we will first create.\n\nimport tobler\n\nhex_grid = tobler.util.h3fy(\n    regions, resolution=5\n)\n\nNot that pollution is expressed as an intesive (rate) variable. We need to recognise this when specifying the interpolation model:\n\npollution_hex = tobler.area_weighted.area_interpolate(\n    regions.assign(geometry=regions.buffer(0)).to_crs(epsg=5726),\n    hex_grid.to_crs(epsg=5726), \n    intensive_variables=[\"no2_mean\"]\n)\n\nAnd the results look like:\n\nf, axs = plt.subplots(1, 3, figsize=(12, 4))\n\nregions.plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[0]\n)\naxs[0].set_axis_off()\n\nhex_grid.plot(\n    facecolor=\"none\", edgecolor=\"red\", ax=axs[1]\n)\naxs[1].set_axis_off()\n\npollution_hex.to_crs(epsg=4326).plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[2]\n)\naxs[2].set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analytis using resolution = 4. How is the result different? Why?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (I)</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html",
    "href": "openstreetmap.html",
    "title": "5  OpenStreetMap",
    "section": "",
    "text": "5.1 Packages and modules\nimport geopandas\nimport contextily\nimport matplotlib.pyplot as plt\nfrom IPython.display import GeoJSON",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#data",
    "href": "openstreetmap.html#data",
    "title": "5  OpenStreetMap",
    "section": "5.2 Data",
    "text": "5.2 Data\nSince some of the query options we will discuss involve pre-defined extents, we will read the Madrid neighbourhoods dataset first.\nAssuming you have the file locally on the path ./data/:\n\nneis = geopandas.read_file(\"./data/neighbourhoods.geojson\")\n\nTo make some of the examples below computationally easier on OpenStreetMap servers, we will single out the smallest neighborhood:\n\nareas = neis.to_crs(\n    epsg=32630\n).area\n\nsmallest = neis[areas == areas.min()]\nsmallest\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n98\nAtalaya\nCiudad Lineal\nMULTIPOLYGON (((-3.66195 40.46338, -3.66364 40...\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nsmallest.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#osmnx",
    "href": "openstreetmap.html#osmnx",
    "title": "5  OpenStreetMap",
    "section": "5.3 osmnx",
    "text": "5.3 osmnx\nLet’s import one more package, osmnx, designed to easily download, model, analyse, and visualise street networks and other geospatial features from OpenStreetMap.\n\nimport osmnx as ox\n\nHere is a trick (courtesy of Martin Fleischmann to pin all your queries to OpenStreetMap to a specific date, so results are always reproducible, even if the map changes in the meantime.\n\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of the methods covered here rely on the osmnx.features module. Check out its reference here.\n\n\nThere are two broad areas to keep in mind when querying data on OpenStreetMap through osmnx:\n\nThe interface to specify the extent of the search.\nThe nature of the entities being queried. Here, the interface relies entirely on OpenStreetMap’s tagging system. Given the distributed nature of the project, this is variable, but a good place to start is:\n\n\nhttps://wiki.openstreetmap.org/wiki/Tags\n\nGenerally, the interface we will follow involves the following:\nreceived_entities = ox.features_from_XXX(\n    &lt;extent&gt;, tags={&lt;key&gt;: True/&lt;value(s)&gt;}, ...\n)\nThe &lt;extent&gt; can take several forms. We can print out the available forms:\n\n[i for i in dir(ox) if \"features_from_\" in i]\n\n['features_from_address',\n 'features_from_bbox',\n 'features_from_place',\n 'features_from_point',\n 'features_from_polygon',\n 'features_from_xml']\n\n\nThe tags follow the official feature spec.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#buildings",
    "href": "openstreetmap.html#buildings",
    "title": "5  OpenStreetMap",
    "section": "5.4 Buildings",
    "text": "5.4 Buildings\n\nblgs = ox.features_from_polygon(\n    smallest.squeeze().geometry, tags={\"building\": True}\n)\n\n\nfig, ax = plt.subplots()\n\nblgs.plot(ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nblgs.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 115 entries, ('way', 442595762) to ('way', 577690922)\nData columns (total 27 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   name              2 non-null      object  \n 1   amenity           2 non-null      object  \n 2   geometry          115 non-null    geometry\n 3   nodes             115 non-null    object  \n 4   building          115 non-null    object  \n 5   addr:housenumber  21 non-null     object  \n 6   addr:postcode     3 non-null      object  \n 7   addr:street       9 non-null      object  \n 8   denomination      1 non-null      object  \n 9   phone             2 non-null      object  \n 10  religion          1 non-null      object  \n 11  source            1 non-null      object  \n 12  source:date       1 non-null      object  \n 13  url               1 non-null      object  \n 14  wheelchair        1 non-null      object  \n 15  building:levels   11 non-null     object  \n 16  addr:city         8 non-null      object  \n 17  addr:country      6 non-null      object  \n 18  wikidata          1 non-null      object  \n 19  website           1 non-null      object  \n 20  country           1 non-null      object  \n 21  diplomatic        1 non-null      object  \n 22  name:en           1 non-null      object  \n 23  name:fr           1 non-null      object  \n 24  name:ko           1 non-null      object  \n 25  office            1 non-null      object  \n 26  target            1 non-null      object  \ndtypes: geometry(1), object(26)\nmemory usage: 29.7+ KB\n\n\n\nblgs.head()\n\n\n\n\n\n\n\n\n\nname\namenity\ngeometry\nnodes\nbuilding\naddr:housenumber\naddr:postcode\naddr:street\ndenomination\nphone\n...\naddr:country\nwikidata\nwebsite\ncountry\ndiplomatic\nname:en\nname:fr\nname:ko\noffice\ntarget\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nway\n442595762\nNaN\nNaN\nPOLYGON ((-3.66377 40.46317, -3.66363 40.46322...\n[4402722774, 4402722775, 4402722776, 440272277...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595763\nNaN\nNaN\nPOLYGON ((-3.66394 40.46346, -3.66415 40.46339...\n[4402722778, 4402722779, 4402722780, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595764\nNaN\nNaN\nPOLYGON ((-3.66379 40.46321, -3.66401 40.46314...\n[4402722782, 4402722783, 4402722784, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595765\nNaN\nNaN\nPOLYGON ((-3.66351 40.46356, -3.66294 40.46371...\n[4402722786, 4402722787, 4402722788, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442596830\nNaN\nNaN\nPOLYGON ((-3.66293 40.46289, -3.66281 40.46294...\n[4402729658, 4402729659, 4402729660, 440272966...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 27 columns\n\n\n\nIf you want to visit the entity online, you can do so at:\n\nhttps://www.openstreetmap.org/&lt;unique_id&gt;\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Extract the building footprints for the Sol neighbourhood in neis.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#other-polygons",
    "href": "openstreetmap.html#other-polygons",
    "title": "5  OpenStreetMap",
    "section": "5.5 Other polygons",
    "text": "5.5 Other polygons\n\npark = ox.features_from_place(\n    \"Parque El Retiro, Madrid\", tags={\"leisure\": \"park\"}\n)\n\n\nfig, ax = plt.subplots()\n\npark.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#points-of-interest",
    "href": "openstreetmap.html#points-of-interest",
    "title": "5  OpenStreetMap",
    "section": "5.6 Points of interest",
    "text": "5.6 Points of interest\nBars around Atocha station:\n\nbars = ox.features_from_address(\n    \"Puerta de Atocha, Madrid\", tags={\"amenity\": \"bar\"}, dist=1500\n)\n\nWe can quickly explore with GeoJSON:\n\nbars.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd stores within Malasaña:\n\nshops = ox.features_from_address(\n    \"Malasaña, Madrid, Spain\", # Boundary to search within\n    tags={\n        \"shop\": True,\n        \"landuse\": [\"retail\", \"commercial\"],\n        \"building\": \"retail\"\n    },\n    dist=1000\n)\n\nshops.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe use features_from_place for delineated areas (“polygonal entities”):\n\ncs = ox.features_from_place(\n    \"Madrid, Spain\",\n    tags={\"amenity\": \"charging_station\"}\n)\n\ncs.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nSimilarly, we can work with location data. For example, searches around a given point:\n\nbakeries = ox.features_from_point(\n    (40.418881103417675, -3.6920446157455444),\n    tags={\"shop\": \"bakery\", \"craft\": \"bakery\"},\n    dist=500\n)\n\nbakeries.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n\nHow many music shops does OSM record within 750 metres of Puerta de Alcalá?\n\n- Are there more restaurants or clothing shops within the polygon that represents the Pacífico neighbourhood in neis table?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#streets",
    "href": "openstreetmap.html#streets",
    "title": "5  OpenStreetMap",
    "section": "5.7 Streets",
    "text": "5.7 Streets\nStreet data can be obtained as another type of entity, as above; or as a graph object.\n\n5.7.1 Geo-tables\n\ncentro = ox.features_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n    tags={\"highway\": True}\n)\n\nWe can get a quick peak into what is returned (in grey), compared to the region we used for the query:\n\nfig, ax = plt.subplots()\n\nneis.query(\n    \"neighbourhood == 'Sol'\"\n).plot(color=\"k\", ax=ax)\n\ncentro.plot(\n    ax=ax, \n    color=\"0.5\", \n    linewidth=0.2, \n    markersize=0.5\n)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis however will return all sorts of things:\n\ncentro.geometry\n\nelement_type  osmid    \nnode          21734214                             POINT (-3.70427 40.41662)\n              21734250                             POINT (-3.70802 40.41612)\n              21734252                             POINT (-3.70847 40.41677)\n              21968134                             POINT (-3.69945 40.41786)\n              21968197                             POINT (-3.70054 40.41645)\n                                                 ...                        \nway           907553665    LINESTRING (-3.70686 40.41380, -3.70719 40.41369)\n              909056211    LINESTRING (-3.70705 40.42021, -3.70680 40.42020)\nrelation      5662178      POLYGON ((-3.70948 40.41551, -3.70952 40.41563...\n              7424032      POLYGON ((-3.70263 40.41712, -3.70253 40.41714...\n              8765884      POLYGON ((-3.70636 40.41475, -3.70635 40.41481...\nName: geometry, Length: 609, dtype: geometry\n\n\n\n\n5.7.2 Spatial graphs\nThe graph_from_XXX() functions return clean, processed graph objects for the street network. Available options are:\n\n[i for i in dir(ox) if \"graph_from_\" in i]\n\n['graph_from_address',\n 'graph_from_bbox',\n 'graph_from_gdfs',\n 'graph_from_place',\n 'graph_from_point',\n 'graph_from_polygon',\n 'graph_from_xml']\n\n\nHere is an example:\n\ncentro_gr = ox.graph_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n)\n\nThis is indeed a graph object (as defined by the networkx package):\n\ncentro_gr\n\n&lt;networkx.classes.multidigraph.MultiDiGraph at 0x15e7214e0&gt;\n\n\nTo visualise it, there are several plotting options:\n\n[i for i in dir(ox) if \"plot_graph\" in i]\n\n['plot_graph', 'plot_graph_folium', 'plot_graph_route', 'plot_graph_routes']\n\n\nFor example:\n\nfig, ax = plt.subplots()\n\nox.plot_figure_ground(centro_gr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nox.graph_to_gdfs(centro_gr, nodes=False).explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How many bookshops are within a 50m radious of the Paseo de la Castellana?\n\n\nBonus tip: this one involves the following steps:\n\nExtracting the street segment for Paseo de la Castellana\nDrawing a 50m buffer around it\nQuerying OSM for bookshops ```",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "openstreetmap.html#next-steps",
    "href": "openstreetmap.html#next-steps",
    "title": "5  OpenStreetMap",
    "section": "5.8 Next steps",
    "text": "5.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nParts of the block are inspired and informed by Geoff Boeing’s excellent course on Urban Data Science\nMore in depth content about osmnx is available in the official examples collection\nBoeing (2020) {cite}boeing2020exploring illustrates how OpenStreetMap can be used to analyse urban form (Open Access)\n\n\n\n\n\nAnderson, Jennings, Dipto Sarkar, and Leysia Palen. 2019. “Corporate Editors in the Evolving Landscape of OpenStreetMap.” ISPRS International Journal of Geo-Information 8 (5): 232.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "transport-costs.html",
    "href": "transport-costs.html",
    "title": "6  Transport costs",
    "section": "",
    "text": "6.1 Packages and modules\nimport momepy\nimport geopandas\nimport contextily\nimport xarray, rioxarray\nimport osmnx as ox\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#data",
    "href": "transport-costs.html#data",
    "title": "6  Transport costs",
    "section": "6.2 Data",
    "text": "6.2 Data\nAssuming you have the file locally on the path ./data/:\n\nstreets = geopandas.read_file(\"./data/arturo_streets.gpkg\")\nabbs = geopandas.read_file(\"./data/madrid_abb.gpkg\")\nneis = geopandas.read_file(\"./data/neighbourhoods.geojson\")",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#pandana-graphs",
    "href": "transport-costs.html#pandana-graphs",
    "title": "6  Transport costs",
    "section": "6.3 pandana graphs",
    "text": "6.3 pandana graphs\n\nimport pandana\n\nBefore building the routing network, we convert to graph and back in momepy to “clean” the network and ensure it complies with requirements for routing.\n\nnodes, edges = momepy.nx_to_gdf( # Convert back to geo-table\n    momepy.gdf_to_nx(            # Convert to a clean NX graph\n        streets.explode(index_parts='True')        # We \"explode\" to avoid multi-part rows\n    )\n)\nnodes = nodes.set_index(\"nodeID\") # Reindex nodes on ID\n\nOnce we have nodes and edges “clean” from the graph representation, we can build a pandana.Network object we will use for routing:\n\nstreets_pdn = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"mm_len\"]]\n)\n\nstreets_pdn\n\nGenerating contraction hierarchies with 10 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\n&lt;pandana.network.Network at 0x16338d450&gt;",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#shortest-path-routing",
    "href": "transport-costs.html#shortest-path-routing",
    "title": "6  Transport costs",
    "section": "6.4 Shortest-path routing",
    "text": "6.4 Shortest-path routing\nHow do I go from A to B?\nFor example, from the first Airbnb in the geo-table…\n\nfirst = abbs.loc[[0], :].to_crs(streets.crs)\n\n…to Puerta del Sol.\n\nimport geopy\ngeopy.geocoders.options.default_user_agent = \"gds4eco\"\nsol = geopandas.tools.geocode(\n    \"Puerta del Sol, Madrid\", geopy.Nominatim\n).to_crs(streets.crs)\nsol\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (440284.049 4474264.421)\nPuerta del Sol, Barrio de los Austrias, Sol, C...\n\n\n\n\n\n\n\nFirst we snap locations to the network:\n\npt_nodes = streets_pdn.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\npt_nodes\n\n0     3071\n1    35729\nName: node_id, dtype: int64\n\n\nThen we can route the shortest path:\n\nroute_nodes = streets_pdn.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\nroute_nodes\n\narray([ 3071,  3476,  8268,  8266,  8267, 18695, 18693,  1432,  1430,\n         353,  8175,  8176, 18121, 17476, 16858, 14322, 16857, 17810,\n       44795, 41220, 41217, 41221, 41652, 18924, 18928, 48943, 18931,\n       21094, 21095, 23219, 15398, 15399, 15400, 47446, 47447, 23276,\n       47448, 23259, 23260, 23261, 27951, 27952, 27953, 48327, 11950,\n       11949, 11944, 19475, 19476, 27333, 30088, 43294, 11940, 11941,\n       11942, 48325, 37484, 48316, 15893, 15890, 15891, 29954, 25453,\n        7341, 34991, 23608, 28217, 21648, 21649, 21651, 39075, 25108,\n       25102, 25101, 25100, 48518, 47287, 34623, 31187, 29615, 48556,\n       22844, 48553, 48555, 40922, 40921, 40923, 48585, 46372, 46371,\n       46370, 45675, 45676, 38778, 38777, 19144, 20498, 20497, 20499,\n       47737, 42303, 42302, 35730, 35727, 35729])\n\n\nWith this information, we can build the route line manually.\nThe code to generate the route involves writing a function and is a bit more advanced than expected for this course. If this looks too complicated, do not despair.\n\nfrom shapely.geometry import LineString\n\ndef route_nodes_to_line(nodes, network):\n    pts = network.nodes_df.loc[nodes, :]\n    s = geopandas.GeoDataFrame(\n        {\"src_node\": [nodes[0]], \"tgt_node\": [nodes[1]]},\n        geometry=[LineString(pts.values)],\n        crs=streets.crs\n    )\n    return s\n\nWe can calculate the route:\n\nroute = route_nodes_to_line(route_nodes, streets_pdn)\n\nAnd we get it back as a geo-table (with one row):\n\nroute\n\n\n\n\n\n\n\n\nsrc_node\ntgt_node\ngeometry\n\n\n\n\n0\n3071\n3476\nLINESTRING (442606.507 4478714.516, 442597.100...\n\n\n\n\n\n\n\nPlease note this builds a simplified line for the route, not one that is based on the original geometries.\n\nfig, ax = plt.subplots()\n\nroute.plot(\n    figsize=(9, 9),\n    color=\"red\",\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=route.crs,\n    source=contextily.providers.CartoDB.Voyager,\n    zoom=14\n)\n\nplt.show()\n\n\n\n\n\n\n\n\nBut distance calculations are based on the original network). If we wanted to obtain the length of the route:\n\nroute_len = streets_pdn.shortest_path_length(\n    pt_nodes[0], pt_nodes[1]\n)\nround(route_len / 1000, 3) # Dist in Km\n\n5.458\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n- What is the network distance between CEMFI and Puerta del Sol?\n- BONUS I: how much longer is it than if you could fly in a straight line?\n- BONUS II: if one walks at a speed of 5 Km/h, how long does the walk take you?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#weighted-routing",
    "href": "transport-costs.html#weighted-routing",
    "title": "6  Transport costs",
    "section": "6.5 Weighted routing",
    "text": "6.5 Weighted routing\nHow do I go from A to B passing by the “best” buildings?\nThis is really an extension of standard routing that takes advantage of the flexibility of pandana.Network objects.\nNote that the route we defined above, does not pass by the “best” buildings.\n\nbb = route.total_bounds\n\nfig, ax = plt.subplots()\n\nstreets.cx[\n    bb[0]: bb[2], bb[1]:bb[3]\n].plot(\n    \"average_quality\", scheme=\"quantiles\", ax=ax\n)\n\nroute.plot(color=\"r\", linewidth=2.5, ax=ax)\n\nax.set_title(\"Mean Building Quality\")\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\nThe overall process to achieve this is the very similar; the main difference is, when we build the Network object, to replace distance (mm_len) with a measure that combines distance and building quality. Note that we want to maximise building quality, but the routing algorithms use a minimisation function. Hence, our composite index will need to reflect that.\nThe strategy is divided in the following steps:\n\nRe-scale distance between 0 and 1\nBuild a measure inverse to building quality in the \\([0, 1]\\) range\nGenerate a combined measure (wdist) by picking a weighting parameter\nBuild a new Network object that incorporates wdist instead of distance\nCompute route between the two points of interest\n\nFor 1., we can use the scaler in scikit-learn:\n\nfrom sklearn.preprocessing import minmax_scale\n\nThen generate and attach to edges a scaled version of mm_len:\n\nedges[\"scaled_dist\"] = minmax_scale(edges[\"mm_len\"])\n\nWe can compare distance with scaled distance. The correlation should be perfect, the scaling is only a change of scale or unit.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\"mm_len\", \"scaled_dist\", ax=ax)\nax.set_title(\"Distance Vs Scaled Distance\")\nplt.show()\n\n\n\n\n\n\n\n\nWe move on to 2., with a similar approach. We will use the negative of the building quality average (average_quality):\n\nedges[\"scaled_inv_bquality\"] = minmax_scale(\n    -edges[\"average_quality\"]\n)\n\nAnd again, we can plot the relation between building quality and the scaled quality.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\n    \"average_quality\", \"scaled_inv_bquality\", ax=ax\n)\nax.set_title(\"Quality Vs Inv. Scaled Quality\")\nplt.show()\n\n\n\n\n\n\n\n\nTaking 1. and 2. into 3. we can build wdist. For this example, we will give each dimension the same weight (0.5), but this is at discretion of the researcher.\n\nw = 0.5\nedges[\"wdist\"] = (\n    edges[\"scaled_dist\"] * w +\n    edges[\"scaled_inv_bquality\"] * (1-w)\n)\n\nNow we can recreate the Network object based on our new measure (4.) and provide routing. Since it is the same process as with distance, we will do it all in one go:\n\n# Build new graph object\nw_graph = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"wdist\"]]\n)\n# Snap locations to their nearest node\npt_nodes = w_graph.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\n# Generate route\nw_route_nodes = w_graph.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\n# Build LineString\nw_route = route_nodes_to_line(\n    w_route_nodes, w_graph\n)\n\nGenerating contraction hierarchies with 10 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\nNow we are ready to display it on a map:\n\nfig, ax = plt.subplots()\n# Building quality\nstreets.plot(\n    \"average_quality\", \n    scheme=\"quantiles\", \n    cmap=\"magma\",\n    linewidth=0.5,\n    figsize=(9, 9), \n    ax=ax\n)\n# Shortest route\nroute.plot(\n    color=\"xkcd:orange red\", linewidth=3, ax=ax, label=\"Shortest\"\n)\n# Weighted route\nw_route.plot(\n    color=\"xkcd:easter green\", linewidth=3, ax=ax, label=\"Weighted\"\n)\n# Styling\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n1. Explore the differences in the output of weighted routing if you change the weight between distance and the additional constrain.\n2. Recreate weighted routing using the linearity of street segments. How can you go from A to B avoiding long streets?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#proximity",
    "href": "transport-costs.html#proximity",
    "title": "6  Transport costs",
    "section": "6.6 Proximity",
    "text": "6.6 Proximity\nWhat is the nearest internet cafe for Airbnb’s without WiFi?\nFirst we identify Airbnb’s without WiFi:\n\nno_wifi = abbs.query(\n    \"WiFi == '0'\"\n).to_crs(streets.crs)\n\nThen pull WiFi spots in Madrid from OpenStreetMap:\n\nicafes = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"amenity\": \"internet_cafe\"}\n).to_crs(streets.crs).reset_index()\n\n\nfig, ax = plt.subplots()\n\nno_wifi.plot(\n    color=\"red\", \n    markersize=1,\n    alpha=0.5,\n    label=\"Airbnb no WiFi\",\n    figsize=(9, 9),\n    ax=ax\n)\n\nicafes.plot(\n    ax=ax, color=\"lime\", label=\"Internet cafes\"\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=no_wifi.crs,\n    source=contextily.providers.CartoDB.Voyager\n)\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe logic for this operation is the following:\n\nAdd the points of interest (POIs, the internet cafes) to the network object (streets_pdn)\nFind the nearest node to each POI\nFind the nearest node to each Airbnb without WiFi\nConnect each Airbnb to its nearest internet cafe\n\nWe can add the internet cafes to the network object (1.) with the set_pois method. Note we set maxitems=1 because we are only going to query for the nearest cafe. This will make computations much faster.\n\nstreets_pdn.set_pois(\n    category=\"Internet cafes\", # Our name for the layer in the `Network` object\n    maxitems=1,                # Use to count only nearest cafe\n    maxdist=100000,            # 100km so everything is included\n    x_col=icafes.geometry.x,   # X coords of cafes\n    y_col=icafes.geometry.y,   # Y coords of cafes\n)\n\nOnce the cafes are added to the network, we can find the nearest one to each node (2.). Note there are some nodes for which we can’t find a nearest cafe. These are related to disconnected parts of the network.\n\ncafe2nnode = streets_pdn.nearest_pois(\n    100000,              # Max distance to look for\n    \"Internet cafes\",    # POIs to look for\n    num_pois=1,          # No. of POIs to include\n    include_poi_ids=True # Store POI ID\n).join(# Then add the internet cafee IDs and name\n    icafes[['osmid', 'name']],\n    on=\"poi1\"\n).rename(# Rename the distance from node to cafe\n    columns={1: \"dist2icafe\"}\n)\ncafe2nnode.head()\n\n\n\n\n\n\n\n\ndist2icafe\npoi1\nosmid\nname\n\n\nnodeID\n\n\n\n\n\n\n\n\n0\n5101.421875\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n1\n5190.265137\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n2\n5252.475098\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n3\n5095.101074\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n4\n5676.117188\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n\n\n\n\n\nTo make things easier down the line, we can link cafe2nnode to the cafe IDs. And we can also link Airbnb’s to nodes (3.) following a similar approach as we have seen above:\n\nabbs_nnode = streets_pdn.get_node_ids(\n    no_wifi.geometry.x, no_wifi.geometry.y\n)\nabbs_nnode.head()\n\n26      8872\n50     10905\n62     41158\n63     34257\n221    32215\nName: node_id, dtype: int64\n\n\nFinally, we can bring together both to find out what is the nearest internet cafe for each Airbnb (4.).\n\nabb_icafe = no_wifi[\n    [\"geometry\"]     # Keep only geometries of ABBs w/o WiFi\n].assign(\n    nnode=abbs_nnode # Attach to thse ABBs the nearest node in the network\n).join(              # Join to each ABB the nearest cafe using node IDs\n    cafe2nnode, \n    on=\"nnode\"\n)\nabb_icafe.head()\n\n\n\n\n\n\n\n\ngeometry\nnnode\ndist2icafe\npoi1\nosmid\nname\n\n\n\n\n26\nPOINT (443128.256 4483599.841)\n8872\n4926.223145\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n50\nPOINT (441885.677 4475916.602)\n10905\n1876.392944\n19.0\n6.922981e+09\nLocutorio\n\n\n62\nPOINT (440439.640 4476480.771)\n41158\n1164.812988\n17.0\n5.573414e+09\nNaN\n\n\n63\nPOINT (438485.311 4471714.377)\n34257\n1466.537964\n5.0\n2.304485e+09\nNaN\n\n\n221\nPOINT (439941.104 4473117.914)\n32215\n354.268005\n15.0\n5.412145e+09\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate distances to nearest internet cafe for ABBs with WiFi. On average, which of the two groups (with and without WiFi) are closer to internet cafes?",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#accessibility",
    "href": "transport-costs.html#accessibility",
    "title": "6  Transport costs",
    "section": "6.7 Accessibility",
    "text": "6.7 Accessibility\nThis flips the previous question on its head and, instead of asking what is the nearest POI to a given point, along the network (irrespective of distance), it asks how many POIs can I access within a network-based distance radius?\n\nparks = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"leisure\": \"park\"}\n).to_crs(streets.crs)\n\n\nFor example, how many parks are within 500m(-euclidean) of an Airbnb?\n\nWe draw a radius of 500m around each AirBnb:\n\nbuffers = geopandas.GeoDataFrame(\n    geometry=abbs.to_crs(\n        streets.crs\n    ).buffer(\n        500\n    )\n)\n\nThen intersect it with the location of parks, and count by buffer (ie. Airbnb):\n\npark_count = geopandas.sjoin(\n    parks, buffers\n).groupby(\n    \"index_right\"\n).size()\n\n\nHow many parks are within 500m(-network) of an Airbnb?\n\nWe need to approach this as a calculation within the network. The logic of steps thus looks like:\n\nUse the aggregation module in pandana to count the number of parks within 500m of each node in the network\nExtract the counts for the nodes nearest to Airbnb properties\nAssign park counts to each Airbnb\n\nWe can set up the aggregate engine (1.). This involves three steps:\n\nObtain nearest node for each park\n\n\nparks_nnode = streets_pdn.get_node_ids(\n    parks.centroid.x, parks.centroid.y\n)\n\n\nInsert the parks’ nearest node through set so it can be “aggregated”\n\n\nstreets_pdn.set(\n    parks_nnode, name=\"Parks\"\n)\n\n\n“Aggregate” for a distance of 500m, effectively counting the number of parks within 500m of each node\n\n\nparks_by_node = streets_pdn.aggregate(\n    distance=500, type=\"count\", name=\"Parks\"\n)\nparks_by_node.head()\n\nnodeID\n0    5.0\n1    5.0\n2    6.0\n3    8.0\n4    1.0\ndtype: float64\n\n\nAt this point, we have the number of parks within 500m of every node in the network. To identify those that correspond to each Airbnb (3.), we first pull out the nearest nodes to each ABB:\n\nabbs_xys = abbs.to_crs(streets.crs).geometry\nabbs_nnode = streets_pdn.get_node_ids(\n    abbs_xys.x, abbs_xys.y\n)\n\nAnd use the list to assign the count of the nearest node to each Airbnb:\n\npark_count_network = abbs_nnode.map(\n    parks_by_node\n)\npark_count_network.head()\n\n0     4.0\n1     9.0\n2     5.0\n3     0.0\n4    12.0\nName: node_id, dtype: float64\n\n\n\nFor which areas do both differ most?\n\nWe can compare the two counts above to explore to what extent the street layout is constraining access to nearby parks.\n\npark_comp = geopandas.GeoDataFrame(\n    {\n        \"Euclidean\": park_count, \n        \"Network\": park_count_network\n    },\n    geometry=abbs.geometry,\n    crs=abbs.crs\n)\n\n\nfig, ax = plt.subplots()\npark_comp.plot.scatter(\"Euclidean\", \"Network\", ax=ax)\nax.axline([0, 0], [1, 1], color='red') #45-degree line\nplt.show()\n\n\n\n\n\n\n\n\nNote there are a few cases where there are more network counts than Euclidean. These are due to the slight inaccuracies introduced by calculating network distances from nodes rather than the locations themselves.\nGeographically:\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Euclidean count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[0]\n)\ncontextily.add_basemap(\n    axs[0], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[0].set_axis_off()\naxs[0].set_title(\"Euclidean Distances\")\n\n# Count difference\nwith_parks = park_comp.query(\n    \"(Network &gt; 0) & (Euclidean &gt; 0)\"\n)\ncount_diff = 100 * (\n    with_parks[\"Euclidean\"] - \n    with_parks[\"Network\"]\n) / with_parks[\"Euclidean\"]\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=count_diff\n).dropna().plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[1]\n)\ncontextily.add_basemap(\n    axs[1], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[1].set_axis_off()\naxs[1].set_title(\"Count Difference (%)\")\n\n# Network count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count_network\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[2]\n)\ncontextily.add_basemap(\n    axs[2], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[2].set_axis_off()\naxs[2].set_title(\"Network Distances\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate accessibility to other ABBs from each ABB through the network. How many ABBs can you access within 500m of each ABB?\nNote you will need to use the locations of ABBs both as the source and the target for routing in this case.",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "transport-costs.html#next-steps",
    "href": "transport-costs.html#next-steps",
    "title": "6  Transport costs",
    "section": "6.8 Next steps",
    "text": "6.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nThe pandana tutorial and documentation are excellent places to get a more detailed and comprehensive view into the functionality of the library",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html",
    "href": "quarto-notebooks/spatial-feature-ii.html",
    "title": "4  Spatial feature engineering (part II)",
    "section": "",
    "text": "4.1 Packages and modules\nimport pandas, geopandas\nimport numpy as np\nimport contextily\nimport tobler\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial feature engineering (part II)</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#data",
    "href": "quarto-notebooks/spatial-feature-ii.html#data",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.2 Data",
    "text": "4.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\npts = geopandas.read_file(\"../data/madrid_abb.gpkg\")\n\nWe will be working with a modified version of pts:\n\nSince we will require distance calculations, we will switch to the Spanish official projection\nTo make calculations in the illustration near-instantaneous, we will work with a smaller (random) sample of Airbnb properties (10% of the total)\n\n\ndb = pts.sample(\n    frac=0.1, random_state=123\n).to_crs(epsg=25830)\n\nAs you can see in the description, the new CRS is expressed in metres:\n\ndb.crs\n\n&lt;Projected CRS: EPSG:25830&gt;\nName: ETRS89 / UTM zone 30N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Europe between 6°W and 0°W: Faroe Islands offshore; Ireland - offshore; Jan Mayen - offshore; Norway including Svalbard - offshore; Spain - onshore and offshore.\n- bounds: (-6.0, 35.26, 0.01, 80.49)\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#distance-buffers",
    "href": "quarto-notebooks/spatial-feature-ii.html#distance-buffers",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.3 Distance buffers",
    "text": "4.3 Distance buffers\nHow many Airbnb’s are within 500m of each Airbnb?\n\nfrom pysal.lib import weights\n\nUsing DistanceBand, we can build a spatial weights matrix that assigns 1 to each observation within 500m, and 0 otherwise.\n\nw500m = weights.DistanceBand.from_dataframe(\n    db, threshold=500, binary=True\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/util.py:826: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 86 disconnected components.\n There are 47 islands with ids: 6878, 16772, 15006, 1336, 3168, 15193, 1043, 5257, 4943, 12849, 10609, 11309, 10854, 10123, 3388, 9380, 10288, 13071, 3523, 15316, 3856, 205, 7720, 10454, 18307, 3611, 12405, 10716, 14813, 15467, 1878, 16597, 14329, 7933, 16215, 13525, 13722, 11932, 14456, 8848, 15197, 8277, 9922, 13072, 13852, 5922, 17151.\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/distance.py:844: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 86 disconnected components.\n There are 47 islands with ids: 6878, 16772, 15006, 1336, 3168, 15193, 1043, 5257, 4943, 12849, 10609, 11309, 10854, 10123, 3388, 9380, 10288, 13071, 3523, 15316, 3856, 205, 7720, 10454, 18307, 3611, 12405, 10716, 14813, 15467, 1878, 16597, 14329, 7933, 16215, 13525, 13722, 11932, 14456, 8848, 15197, 8277, 9922, 13072, 13852, 5922, 17151.\n\n\n\nThe number of neighbors can be accessed through the cardinalities attribute:\n\nn_neis = pandas.Series(w500m.cardinalities)\nn_neis.head()\n\n11297    213\n2659       5\n16242     21\n15565      9\n14707    159\ndtype: int64\n\n\n\nfig, ax = plt.subplots()\n\ndb.assign(\n    n_neis=n_neis\n).plot(\"n_neis\", markersize=0.1, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate the number of AirBnb properties within 250m of each other property. What is the average?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#distance-rings",
    "href": "quarto-notebooks/spatial-feature-ii.html#distance-rings",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.4 Distance rings",
    "text": "4.4 Distance rings\nHow many Airbnb’s are between 500m and 1km of each Airbnb?\n\nw1km = weights.DistanceBand.from_dataframe(\n    db, threshold=1000, binary=True\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/util.py:826: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 20 disconnected components.\n There are 5 islands with ids: 4943, 12849, 15467, 13525, 11932.\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/distance.py:844: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 20 disconnected components.\n There are 5 islands with ids: 4943, 12849, 15467, 13525, 11932.\n\n\n\nNow, we could do simply a subtraction:\n\nn_ring_neis = pandas.Series(w1km.cardinalities) - n_neis\n\nOr, if we need to know which is which, we can use set operations on weights:\n\nw_ring = weights.w_difference(w1km, w500m, constrained=False)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/set_operations.py:242: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 34 disconnected components.\n There are 23 islands with ids: 3744, 4143, 4857, 4943, 6986, 8345, 8399, 9062, 10592, 10865, 11574, 11613, 11785, 11840, 11932, 12015, 12635, 12714, 12849, 13091, 13317, 13525, 15467.\n\n\n\nAnd we can confirm they’re both the same:\n\n(pandas.Series(w_ring.cardinalities) - n_ring_neis).sum()\n\n0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you create a plot with the following two lines?\n\nOne depicting the average number of properties within a range of 50m, 100m, 250m, 500m, 750m\nAnother one with the increase of average neighbors for the same distances above"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-points",
    "href": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-points",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.5 Cluster membership (points)",
    "text": "4.5 Cluster membership (points)\nWe can use the spatial configuration of observations to classify them as part of clusters or not, which can then be encoded, for example, as dummy variables in a model.\nThese magic numbers need to be pre-set and you can play with both min_pct (or min_pts directly) and eps to see how they affect the results (spoiler: a lot!).\n\nfrom sklearn.cluster import DBSCAN\n\nmin_pct = 2\nmin_pts = len(db) * min_pct // 100\neps = 500\n\nWe will illustrate it with a minimum number of points of min_pct % of the sample and a maximum radious of eps metres.\n\nmodel = DBSCAN(min_samples=min_pts, eps=eps)\nmodel.fit(\n    db.assign(\n        x=db.geometry.x\n    ).assign(\n        y=db.geometry.y\n    )[['x', 'y']]\n);\n\nWe will attach the labels to db for easy access:\n\ndb[\"labels\"] = model.labels_\n\nWe can define boundaries to turn point clusters into polygons if that fits our needs better:\nThe code in the next cell is a bit more advanced than expected for this course, but is used here as an illustration.\n\nfrom pysal.lib import cg\n\nboundaries = []\ncl_ids = [i for i in db[\"labels\"].unique() if i!=-1]\nfor cl_id in cl_ids:\n    sub = db.query(f\"labels == {cl_id}\")\n    cluster_boundaries = cg.alpha_shape_auto(\n        np.array(\n            [sub.geometry.x, sub.geometry.y]\n        ).T,\n    )\n    boundaries.append(cluster_boundaries)\nboundaries = geopandas.GeoSeries(\n    boundaries, index=cl_ids, crs=db.crs\n)\n\nAnd we can see what the clusters look like:\n\nfig, ax = plt.subplots()\n\ndb.to_crs(\n    epsg=3857\n).plot(\n    markersize=0.1, color=\"lime\", ax=ax\n)\nboundaries.to_crs(\n    epsg=3857\n).plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\n\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.CartoDB.DarkMatterNoLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How does the map above change if you require 5% of points instead of 2% for a candidate cluster to be considered so?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-polygons",
    "href": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-polygons",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.6 Cluster membership (polygons)",
    "text": "4.6 Cluster membership (polygons)\nWe can take a similar approach as above if we have polygon geographies instead of points. Rather than using DBSCAN, here we can rely on local indicators of spatial association (LISAs) to pick up spatial concentrations of high or low values.\nFor the illustration, we will aggregate the location of Airbnb properties to a regular hexagonal grid, similar to how we generated it when transferring from polygons to polygons. First we create a polygon covering the extent of points:\n\none = geopandas.GeoSeries(\n    [cg.alpha_shape_auto(\n        np.array(\n            [db.geometry.x, db.geometry.y]\n        ).T,\n    )],\n    crs=db.crs\n)\n\nThen we can tessellate:\n\nabb_hex = tobler.util.h3fy(\n    one, resolution=8\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAnd obtain a count of points in each polygon:\n\ncounts = geopandas.sjoin(\n    db, abb_hex\n).groupby(\n    \"index_right\"\n).size()\n\nabb_hex[\"count\"] = counts\nabb_hex[\"count\"] = abb_hex[\"count\"].fillna(0)\n\nfig, ax = plt.subplots()\n\nabb_hex.plot(\"count\", scheme=\"fisherjenks\", ax=ax)\n\nplt.show()\n\n\n\n\nTo identify spatial clusters, we rely on esda:\n\nfrom pysal.explore import esda\n\nAnd compute the LISA statistics:\n\nw = weights.Queen.from_dataframe(abb_hex)\nlisa = esda.Moran_Local(abb_hex[\"count\"], w)\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_25959/2473509840.py:1: FutureWarning:\n\n`use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n\n\n\nFor a visual inspection of the clusters, splot:\n\nfrom pysal.viz import splot\nfrom splot.esda import lisa_cluster\n\n\nlisa_cluster(lisa, abb_hex, p=0.01)\nplt.show()\n\n\n\n\nAnd, if we want to extract the labels for each polygon, we can do so from the lisa object:\n\nlisa.q * (lisa.p_sim &lt; 0.01)\n\narray([3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n       3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3,\n       0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n       0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 0, 0,\n       0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n       0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3])"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#next-steps",
    "href": "quarto-notebooks/spatial-feature-ii.html#next-steps",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.7 Next steps",
    "text": "4.7 Next steps\nIf you want a bit more background into some of the techniques reviewed in this block, the following might be of interest:\n\nThis block of the GDS Course (Pietrostefani and Cabrera-Arnau 2024) will introduce you to more techniques like the LISAs seen above to explore the spatial dimension of the statistical properties of your data. If you want a more detailed read, this Chapter of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) will do just that.\nThis block of the GDS Course (Pietrostefani and Cabrera-Arnau 2024) will introduce you to more techniques for exploring point patterns. If you want a more comprehensive read, this Chapter of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) will do just that.\n\n\n\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html",
    "href": "quarto-notebooks/spatial-data.html",
    "title": "1  Spatial data",
    "section": "",
    "text": "1.1 Packages and modules\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#data",
    "href": "quarto-notebooks/spatial-data.html#data",
    "title": "1  Spatial data",
    "section": "1.2 Data",
    "text": "1.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\n\n1.2.1 Points\nAssuming you have the file locally on the path ../data/:\n\npts = geopandas.read_file(\"../data/madrid_abb.gpkg\")\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes, points are provided as separate columns in an otherwise non-spatial table. For example imagine we have an object cols with a column named X for longitude and Y for latitude. Then, we can convert those into proper geometries by running pts = geopandas.GeoSeries( geopandas.points_from_xy(cols[\"X\"], cols[\"Y\"]).\n\n\nLet’s explore the points dataset that we loaded above.\n\npts.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 18399 entries, 0 to 18398\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   price            18399 non-null  object  \n 1   price_usd        18399 non-null  float64 \n 2   log1p_price_usd  18399 non-null  float64 \n 3   accommodates     18399 non-null  int64   \n 4   bathrooms        18399 non-null  object  \n 5   bedrooms         18399 non-null  float64 \n 6   beds             18399 non-null  float64 \n 7   neighbourhood    18399 non-null  object  \n 8   room_type        18399 non-null  object  \n 9   property_type    18399 non-null  object  \n 10  WiFi             18399 non-null  object  \n 11  Coffee           18399 non-null  object  \n 12  Gym              18399 non-null  object  \n 13  Parking          18399 non-null  object  \n 14  km_to_retiro     18399 non-null  float64 \n 15  geometry         18399 non-null  geometry\ndtypes: float64(5), geometry(1), int64(1), object(9)\nmemory usage: 2.2+ MB\n\n\n\npts.head()\n\n\n\n\n\n\n\n\nprice\nprice_usd\nlog1p_price_usd\naccommodates\nbathrooms\nbedrooms\nbeds\nneighbourhood\nroom_type\nproperty_type\nWiFi\nCoffee\nGym\nParking\nkm_to_retiro\ngeometry\n\n\n\n\n0\n$60.00\n60.0\n4.110874\n2\n1 shared bath\n1.0\n1.0\nHispanoamérica\nPrivate room\nPrivate room in apartment\n1\n0\n0\n0\n5.116664\nPOINT (-3.67688 40.45724)\n\n\n1\n$31.00\n31.0\n3.465736\n1\n1 bath\n1.0\n1.0\nCármenes\nPrivate room\nPrivate room in apartment\n1\n1\n0\n1\n5.563869\nPOINT (-3.74084 40.40341)\n\n\n2\n$60.00\n60.0\n4.110874\n6\n2 baths\n3.0\n5.0\nLegazpi\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n3.048442\nPOINT (-3.69304 40.38695)\n\n\n3\n$115.00\n115.0\n4.753590\n4\n1.5 baths\n2.0\n3.0\nJusticia\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n2.075484\nPOINT (-3.69764 40.41995)\n\n\n4\n$26.00\n26.0\n3.295837\n1\n1 private bath\n1.0\n1.0\nLegazpi\nPrivate room\nPrivate room in house\n1\n0\n0\n0\n2.648058\nPOINT (-3.69011 40.38985)\n\n\n\n\n\n\n\n\n\n1.2.2 Lines\nAssuming you have the file locally on the path ../data/:\n\nlines = geopandas.read_file(\"../data/arturo_streets.gpkg\")\n\n\nlines.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 66499 entries, 0 to 66498\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   OGC_FID             66499 non-null  object  \n 1   dm_id               66499 non-null  object  \n 2   dist_barri          66483 non-null  object  \n 3   average_quality     66499 non-null  float64 \n 4   population_density  66499 non-null  float64 \n 5   X                   66499 non-null  float64 \n 6   Y                   66499 non-null  float64 \n 7   value               5465 non-null   float64 \n 8   geometry            66499 non-null  geometry\ndtypes: float64(5), geometry(1), object(3)\nmemory usage: 4.6+ MB\n\n\n\nlines.loc[0, \"geometry\"]\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Print descriptive statistics for population_density and average_quality.\n\n\n\n\n1.2.3 Polygons\nAssuming you have the file locally on the path ../data/:\n\npolys = geopandas.read_file(\"../data/neighbourhoods.geojson\")\n\n\npolys.head()\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n0\nPalacio\nCentro\nMULTIPOLYGON (((-3.70584 40.42030, -3.70625 40...\n\n\n1\nEmbajadores\nCentro\nMULTIPOLYGON (((-3.70384 40.41432, -3.70277 40...\n\n\n2\nCortes\nCentro\nMULTIPOLYGON (((-3.69796 40.41929, -3.69645 40...\n\n\n3\nJusticia\nCentro\nMULTIPOLYGON (((-3.69546 40.41898, -3.69645 40...\n\n\n4\nUniversidad\nCentro\nMULTIPOLYGON (((-3.70107 40.42134, -3.70155 40...\n\n\n\n\n\n\n\n\npolys.query(\"neighbourhood_group == 'Retiro'\")\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n13\nPacífico\nRetiro\nMULTIPOLYGON (((-3.67015 40.40654, -3.67017 40...\n\n\n14\nAdelfas\nRetiro\nMULTIPOLYGON (((-3.67283 40.39468, -3.67343 40...\n\n\n15\nEstrella\nRetiro\nMULTIPOLYGON (((-3.66506 40.40647, -3.66512 40...\n\n\n16\nIbiza\nRetiro\nMULTIPOLYGON (((-3.66916 40.41796, -3.66927 40...\n\n\n17\nJerónimos\nRetiro\nMULTIPOLYGON (((-3.67874 40.40751, -3.67992 40...\n\n\n18\nNiño Jesús\nRetiro\nMULTIPOLYGON (((-3.66994 40.40850, -3.67012 40...\n\n\n\n\n\n\n\n\npolys.neighbourhood_group.unique()\n\narray(['Centro', 'Arganzuela', 'Retiro', 'Salamanca', 'Chamartín',\n       'Moratalaz', 'Tetuán', 'Chamberí', 'Fuencarral - El Pardo',\n       'Moncloa - Aravaca', 'Puente de Vallecas', 'Latina', 'Carabanchel',\n       'Usera', 'Ciudad Lineal', 'Hortaleza', 'Villaverde',\n       'Villa de Vallecas', 'Vicálvaro', 'San Blas - Canillejas',\n       'Barajas'], dtype=object)"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#surfaces",
    "href": "quarto-notebooks/spatial-data.html#surfaces",
    "title": "1  Spatial data",
    "section": "1.3 Surfaces",
    "text": "1.3 Surfaces\nAssuming you have the file locally on the path ../data/:\n\nsat = rioxarray.open_rasterio(\"../data/madrid_scene_s2_10_tc.tif\")\n\n\nsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 3681, x: 3129)&gt;\n[34553547 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 3681x: 3129...[34553547 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0, 424825.0,\n       424835.0, 424845.0, 424855.0,\n       ...\n       455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0, 456015.0,\n       456025.0, 456035.0, 456045.0],\n      dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n       4499305.0, 4499295.0, 4499285.0, 4499275.0,\n       ...\n       4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n       4462595.0, 4462585.0, 4462575.0, 4462565.0],\n      dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(band=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3681, x: 3129)&gt;\n[11517849 values with dtype=uint8]\nCoordinates:\n    band         int64 1\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 3681x: 3129...[11517849 values with dtype=uint8]Coordinates: (4)band()int641array(1)x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0, 424825.0,\n       424835.0, 424845.0, 424855.0,\n       ...\n       455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0, 456015.0,\n       456025.0, 456035.0, 456045.0],\n      dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n       4499305.0, 4499295.0, 4499285.0, 4499275.0,\n       ...\n       4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n       4462595.0, 4462585.0, 4462575.0, 4462565.0],\n      dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(\n    x=slice(430000, 440000),  # x is ascending\n    y=slice(4480000, 4470000) # y is descending\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 1000, x: 1000)&gt;\n[3000000 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.3e+05 4.3e+05 4.3e+05 ... 4.4e+05 4.4e+05 4.4e+05\n  * y            (y) float64 4.48e+06 4.48e+06 4.48e+06 ... 4.47e+06 4.47e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 1000x: 1000...[3000000 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.3e+05 4.3e+05 ... 4.4e+05 4.4e+05array([430005., 430015., 430025., ..., 439975., 439985., 439995.])y(y)float644.48e+06 4.48e+06 ... 4.47e+06array([4479995., 4479985., 4479975., ..., 4470025., 4470015., 4470005.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([430005.0, 430015.0, 430025.0, 430035.0, 430045.0, 430055.0, 430065.0,\n       430075.0, 430085.0, 430095.0,\n       ...\n       439905.0, 439915.0, 439925.0, 439935.0, 439945.0, 439955.0, 439965.0,\n       439975.0, 439985.0, 439995.0],\n      dtype='float64', name='x', length=1000))yPandasIndexPandasIndex(Index([4479995.0, 4479985.0, 4479975.0, 4479965.0, 4479955.0, 4479945.0,\n       4479935.0, 4479925.0, 4479915.0, 4479905.0,\n       ...\n       4470095.0, 4470085.0, 4470075.0, 4470065.0, 4470055.0, 4470045.0,\n       4470035.0, 4470025.0, 4470015.0, 4470005.0],\n      dtype='float64', name='y', length=1000))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Subset sat to band 2 and the section within [444444, 455555] of Easting and [4470000, 4480000] of Northing.\n\nHow many pixels does it contain?\nWhat if you used bands 1 and 3 instead?"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#visualisation",
    "href": "quarto-notebooks/spatial-data.html#visualisation",
    "title": "1  Spatial data",
    "section": "1.4 Visualisation",
    "text": "1.4 Visualisation\nYou will need version 0.10.0 or greater of geopandas to use explore.\n\npolys.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfig, ax = plt.subplots()\npolys.plot(ax=ax)\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\nlines.plot(linewidth=0.1, color=\"black\", ax=ax)\n#contextily.add_basemap(ax, crs=lines.crs)\nplt.show()\n\n\n\n\nSee more basemap options here.\n\nfig, ax = plt.subplots()\npts.plot(color=\"red\", figsize=(12, 12), markersize=0.1, ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs = pts.crs,\n    source = contextily.providers.CartoDB.DarkMatter\n)\nplt.show()\n\n\n\n\n\nsat.plot.imshow(figsize=(12, 12))\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsat.plot.imshow(ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs=sat.rio.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=11,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Make three plots of sat, plotting one single band in each."
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#spatial-operations",
    "href": "quarto-notebooks/spatial-data.html#spatial-operations",
    "title": "1  Spatial data",
    "section": "1.5 Spatial operations",
    "text": "1.5 Spatial operations\n\n1.5.1 (Re-)Projections\n\npts.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.crs\n\nCRS.from_epsg(32630)\n\n\n\npts.to_crs(sat.rio.crs).crs\n\n&lt;Projected CRS: EPSG:32630&gt;\nName: WGS 84 / UTM zone 30N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.reproject(pts.crs).rio.crs\n\nCRS.from_epsg(4326)\n\n\n\n# All into Web Mercator (EPSG:3857)\nfig, ax = plt.subplots(1, figsize=(12, 12))\n\n## Satellite image\nsat.rio.reproject(\n    \"EPSG:3857\"\n).plot.imshow(\n    ax=ax\n)\n\n## Neighbourhoods\npolys.to_crs(epsg=3857).plot(\n    linewidth=1, \n    edgecolor=\"xkcd:lime\", \n    facecolor=\"none\",\n    ax=ax\n)\n\n## Labels\ncontextily.add_basemap( # No need to reproject\n    ax,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n)\n\nplt.show()\n\n\n\n\n\n\n1.5.2 Centroids\nNote the warning that geometric operations with non-projected CRS object result in biases.\n\npolys.centroid\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_7098/2101097851.py:1: UserWarning:\n\nGeometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n\n\n\n0      POINT (-3.71398 40.41543)\n1      POINT (-3.70237 40.40925)\n2      POINT (-3.69674 40.41485)\n3      POINT (-3.69657 40.42367)\n4      POINT (-3.70698 40.42568)\n                 ...            \n123    POINT (-3.59135 40.45656)\n124    POINT (-3.59723 40.48441)\n125    POINT (-3.55847 40.47613)\n126    POINT (-3.57889 40.47471)\n127    POINT (-3.60718 40.46415)\nLength: 128, dtype: geometry\n\n\nIt is therefore important to re-project these geometries to a projected crs such as we did with with pts before.\n\npolys = polys.to_crs(sat.rio.crs)\n\nNow, we can compute centroids without warnings:\n\npolys.centroid\n\n0      POINT (439425.451 4474112.019)\n1      POINT (440404.977 4473418.085)\n2      POINT (440887.707 4474036.547)\n3      POINT (440909.920 4475014.820)\n4      POINT (440028.666 4475245.024)\n                    ...              \n123    POINT (449860.280 4478601.086)\n124    POINT (449382.527 4481695.863)\n125    POINT (452661.832 4480754.248)\n126    POINT (450929.735 4480608.573)\n127    POINT (448523.423 4479452.348)\nLength: 128, dtype: geometry\n\n\n\nfig, ax = plt.subplots()\npolys.plot(color=\"purple\", ax=ax)\npolys.centroid.plot(\n    ax=ax, color=\"lime\", markersize=1\n)\n\nplt.show()\n\n\n\n\n\n\n1.5.3 Spatial joins\nMore information about spatial joins in geopandas is available on its documentation page.\nLet’s ensure that the geometries we are looking to join are in the same projection.\n\nlines = lines.to_crs(polys.crs)\n\n\nsj = geopandas.sjoin(\n    lines,\n    polys\n)\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\nfig, ax = plt.subplots()\n\n# Subset of lines\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).plot(color=\"xkcd:bright turquoise\", ax=ax)\n\n# Subset of line centroids\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).centroid.plot(\n    color=\"xkcd:bright violet\", markersize=7, ax=ax\n)\n\n# Local basemap\ncontextily.add_basemap(\n    ax,\n    crs=sj.crs,\n    source=\"../data/madrid_scene_s2_10_tc.tif\",\n    alpha=0.5\n)\n\nplt.show()\n\n\n\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\n\n1.5.4 Areas\nTo compute areas of polygons, use a projected crs (we already transformed polys to the same projection as sat, which is a projected crs).\n\nareas = polys.area * 1e-6 # Km2\nareas.head()\n\n0    1.471037\n1    1.033253\n2    0.592049\n3    0.742031\n4    0.947616\ndtype: float64\n\n\n\n\n1.5.5 Distances\nWe can give geopandas.tools.geocode() a string or a set of strings corresponding to addresses. It will geocode it and return a GeoDataFrame of the resulting point geometries\n\ncemfi = geopandas.tools.geocode(\n    \"Calle Casado del Alisal, 5, Madrid\"\n).to_crs(sat.rio.crs)\n\n\ncemfi\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (441477.245 4473939.537)\n5, Calle Casado del Alisal, 28014, Calle Casad...\n\n\n\n\n\n\n\nWe can compute the distance between the point for cemfi and the centroids of all the polygons in polys ensuring they both are in the same crs:\n\npolys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry\n)\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_7098/176561454.py:3: UserWarning:\n\nThe indices of the two GeoSeries are different.\n\n\n\n0      1491.338749\n1              NaN\n2              NaN\n3              NaN\n4              NaN\n          ...     \n123            NaN\n124            NaN\n125            NaN\n126            NaN\n127            NaN\nLength: 128, dtype: float64\n\n\n\nd2cemfi = polys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry[0] # NO index\n)\nd2cemfi.head()\n\n0    1491.338749\n1     565.418135\n2     278.121017\n3     650.926572\n4    1196.771601\ndtype: float64\n\n\nMake a map, colouring the polygons according the the distance of their centroid to cemfi:\n\nfig, ax = plt.subplots()\n\npolys.assign(\n    dist=d2cemfi/1000\n).plot(\"dist\", legend=True, ax=ax)\n\ncemfi.to_crs(\n    polys.crs\n).plot(\n    marker=\"*\", \n    markersize=15, \n    color=\"r\", \n    label=\"CEMFI\", \n    ax=ax\n)\n\nax.legend()\nax.set_title(\n    \"Distance to CEMFI\"\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#next-steps",
    "href": "quarto-notebooks/spatial-data.html#next-steps",
    "title": "1  Spatial data",
    "section": "1.6 Next steps",
    "text": "1.6 Next steps\nIf you are interested in following up on some of the topics explored in this block, the following pointers might be useful:\n\nAlthough we have seen here geopandas only, all non-geographic operations on geo-tables are really thanks to pandas, the workhorse for tabular data in Python. Their official documentation is an excellent first stop. If you prefer a book, (McKinney 2013) is a great one.\nFor more detail on geographic operations on geo-tables, the Geopandas official documentation is a great place to continue the journey.\nSurfaces, as covered here, are really an example of multi-dimensional labelled arrays. The library we use, xarray represents the cutting edge for working with these data structures in Python, and their documentation is a great place to wrap your head around how data of this type can be manipulated. For geographic extensions (CRS handling, reprojections, etc.), we have used rioxarray under the hood, and its documentation is also well worth checking.\n\n\n\n\n\nMcKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. 1st ed. Paperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html",
    "href": "quarto-notebooks/geovisualisation.html",
    "title": "2  Geovisualisation",
    "section": "",
    "text": "2.1 Packages and modules\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport seaborn as sns\nfrom pysal.viz import mapclassify as mc\nfrom legendgram import legendgram\nimport matplotlib.pyplot as plt\nimport palettable.matplotlib as palmpl\nfrom splot.mapping import vba_choropleth",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Geovisualisation</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#data",
    "href": "quarto-notebooks/geovisualisation.html#data",
    "title": "2  Geovisualisation",
    "section": "2.2 Data",
    "text": "2.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\ndb = geopandas.read_file(\"../data/cambodia_regional.gpkg\")\n\nQuick visualisation:\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    edgecolor=\"red\",\n    facecolor=\"none\",\n    linewidth=2,\n    alpha=0.25,\n    figsize=(9, 9),\n    ax=ax\n)\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.Esri.NatGeoWorldMap\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\ndb.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 198 entries, 0 to 197\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_name   198 non-null    object  \n 1   adm2_altnm  122 non-null    object  \n 2   motor_mean  198 non-null    float64 \n 3   walk_mean   198 non-null    float64 \n 4   no2_mean    198 non-null    float64 \n 5   geometry    198 non-null    geometry\ndtypes: float64(3), geometry(1), object(2)\nmemory usage: 9.4+ KB\n\n\nWe will use the average measurement of nitrogen dioxide (no2_mean) by region throughout the block.\nTo make visualisation a bit easier below, we create an additional column with values rescaled:\n\ndb[\"no2_viz\"] = db[\"no2_mean\"] * 1e5\n\nThis way, numbers are larger and will fit more easily on legends:\n\ndb[[\"no2_mean\", \"no2_viz\"]].describe()\n\n\n\n\n\n\n\n\nno2_mean\nno2_viz\n\n\n\n\ncount\n198.000000\n198.000000\n\n\nmean\n0.000032\n3.236567\n\n\nstd\n0.000017\n1.743538\n\n\nmin\n0.000014\n1.377641\n\n\n25%\n0.000024\n2.427438\n\n\n50%\n0.000029\n2.922031\n\n\n75%\n0.000034\n3.390426\n\n\nmax\n0.000123\n12.323324"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#choropleths",
    "href": "quarto-notebooks/geovisualisation.html#choropleths",
    "title": "2  Geovisualisation",
    "section": "2.3 Choropleths",
    "text": "2.3 Choropleths\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    \"no2_viz\", \n    legend=True,\n    figsize=(12, 9),\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n2.3.1 A classiffication problem\n\ndb[\"no2_viz\"].unique().shape\n\n(198,)\n\n\n\nsns.displot(\n    db, x=\"no2_viz\", kde=True, aspect=2\n)\n\nplt.show()\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\n\n2.3.2 How to assign colors?\n\n\n\n\n\n\nImportant\n\n\n\nTo build an intuition behind each classification algorithm more easily, we create a helper method (plot_classi) that generates a visualisation of a given classification.\n\n\n\ndef plot_classi(classi, col, db):\n    \"\"\"\n    Illustrate a classiffication\n    ...\n    \n    Arguments\n    ---------\n    classi : mapclassify.classifiers\n             Classification object\n    col    : str\n             Column name used for `classi`\n    db     : geopandas.GeoDataFrame\n             Geo-table with data for\n             the classification    \n    \"\"\"\n    f, ax = plt.subplots(figsize=(12, 6))\n    ax.set_title(classi.name)\n    # KDE\n    sns.kdeplot(\n        db[col], fill=True, ax=ax\n    )\n    for i in range(0, len(classi.bins)-1):\n        ax.axvline(classi.bins[i], color=\"red\")\n    # Map\n    aux = f.add_axes([.6, .45, .32, .4])\n    db.assign(lbls=classi.yb).plot(\n        \"lbls\", cmap=\"viridis\", ax=aux\n    )\n    aux.set_axis_off()\n    plt.show()\n    return None\n\n\nEqual intervals\n\n\nclassi = mc.EqualInterval(db[\"no2_viz\"], k=7)\nclassi\n\nEqualInterval\n\n   Interval      Count\n----------------------\n[ 1.38,  2.94] |   103\n( 2.94,  4.50] |    80\n( 4.50,  6.07] |     6\n( 6.07,  7.63] |     1\n( 7.63,  9.20] |     3\n( 9.20, 10.76] |     0\n(10.76, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nQuantiles\n\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\nclassi\n\nQuantiles\n\n   Interval      Count\n----------------------\n[ 1.38,  2.24] |    29\n( 2.24,  2.50] |    28\n( 2.50,  2.76] |    28\n( 2.76,  3.02] |    28\n( 3.02,  3.35] |    28\n( 3.35,  3.76] |    28\n( 3.76, 12.32] |    29\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nFisher-Jenks\n\n\nclassi = mc.FisherJenks(db[\"no2_viz\"], k=7)\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nNow let’s dig into the internals of classi:\n\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nclassi.k\n\n7\n\n\n\nclassi.bins\n\narray([ 2.05617382,  2.6925931 ,  3.30281182,  4.19124954,  5.63804861,\n        9.19190206, 12.32332434])\n\n\n\nclassi.yb\n\narray([2, 3, 3, 1, 1, 2, 1, 1, 1, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 1, 2, 0,\n       0, 4, 2, 1, 3, 1, 0, 0, 0, 1, 2, 2, 6, 5, 4, 2, 1, 3, 2, 3, 2, 1,\n       2, 3, 2, 3, 1, 1, 3, 1, 2, 3, 3, 1, 3, 3, 1, 0, 1, 1, 3, 2, 0, 0,\n       2, 1, 0, 0, 0, 2, 0, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 3, 1, 1, 1, 1,\n       2, 1, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1, 2, 3, 3, 2, 0, 3,\n       1, 0, 1, 2, 1, 1, 2, 1, 2, 6, 5, 6, 2, 2, 3, 6, 3, 4, 3, 4, 2, 3,\n       0, 2, 5, 6, 4, 5, 2, 2, 2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2,\n       1, 1, 3, 4, 2, 1, 3, 1, 2, 3, 4, 0, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2,\n       2, 2, 0, 0, 1, 2, 3, 3, 3, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1])\n\n\n\n\n2.3.3 How many colors?\nThe code used to generate the next figure uses more advanced features than planned for this course.\nIf you want to inspect it, look at the code cell below.\n\nvals = [3, 5, 7, 9, 12, 15]\nalgos = [\"equal_interval\", \"quantiles\", \"fisherjenks\"]\nf, axs = plt.subplots(\n    len(algos), len(vals), figsize=(3*len(vals), 3*len(algos))\n)\nfor i in range(len(algos)):\n    for j in range(len(vals)):\n        db.plot(\n            \"no2_viz\", scheme=algos[i], k=vals[j], ax=axs[i, j]\n        )\n        axs[i, j].set_axis_off()\n        if i==0:\n            axs[i, j].set_title(f\"k={vals[j]}\")\n        if j==0:\n            axs[i, j].text(\n                -0.1, \n                0.5, \n                algos[i], \n                horizontalalignment='center',\n                verticalalignment='center', \n                transform=axs[i, j].transAxes,\n                rotation=90\n            )\n\nplt.show()\n\n\n\n\n\n\n2.3.4 Using the right color\nFor a “safe” choice, make sure to visit ColorBrewer\n\n Categories, non-ordered\n Graduated, sequential\n Graduated, divergent\n\n\n\n2.3.5 Choropleths on Geo-Tables\n\n2.3.5.1 Streamlined\nHow can we create classifications from data on geo-tables? Two ways:\n\nDirectly within plot (only for some algorithms)\n\n\nfig, ax = plt.subplots()\ndb.plot(\n    \"no2_viz\", scheme=\"quantiles\", k=7, legend=True, ax=ax\n)\nplt.show()\n\n\n\n\nSee this tutorial for more details on fine tuning choropleths manually.\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Create an equal interval map with five bins for no2_viz .\n\n\n\n\n2.3.5.2 Manual approach\nThis is valid for any algorithm and provides much more flexibility at the cost of effort.\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\nfig, ax = plt.subplots()\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nplt.show()\n\n\n\n\n\n\n2.3.5.3 Value by alpha mapping\n\ndb['area_inv'] = 1 / db.to_crs(epsg=5726).area\n\n\nfig, ax = plt.subplots()\ndb.plot('area_inv', scheme='quantiles', ax=ax)\nax.set_title('area_inv')\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n# Set up figure and axis\nfig, ax = plt.subplots(1, figsize=(12, 9))\n# VBA choropleth\nvba_choropleth(\n    'no2_viz',          # Column for color \n    'area_inv',         # Column for transparency (alpha)\n    db,                 # Geo-table\n    rgb_mapclassify={   # Options for color classification\n        'classifier': 'quantiles', 'k':5\n    },\n    alpha_mapclassify={ # Options for alpha classification\n        'classifier': 'quantiles', 'k':5\n    },\n    legend=True,        # Add legend\n    ax=ax               # Axis\n)\n# Add boundary lines\ndb.plot(color='none', linewidth=0.05, ax=ax)\n\nplt.show()\n\n\n\n\nSee here for more examples of value-by-alpha (VBA) mapping.\n\n\n2.3.5.4 Legendgrams\nLegendgrams are a way to more closely connect the statistical characteristics of your data to the map display.\n\n\n\n\n\n\nWarning\n\n\n\nLegendgram is in an experimental development stage, so the code is a bit more involved and less stable. Use at your own risk!\n\n\nHere is an example:\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nlegendgram(\n    fig,                   # Figure object\n    ax,                  # Axis object of the map\n    db[\"no2_viz\"],       # Values for the histogram\n    classi.bins,         # Bin boundaries\n    pal=palmpl.Viridis_7,# color palette (as palettable object)\n    legend_size=(.5,.2), # legend size in fractions of the axis\n    loc = 'lower right', # matplotlib-style legend locations\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Give Task I and II from the GDS course a go.\n\n\n\n\n\n2.3.6 Choropleths on surfaces\nAssuming you have the file locally on the path ../data/:\n\ngrid = rioxarray.open_rasterio(\n  \"../data/cambodia_s5_no2.tif\"\n  ).sel(band=1)\n\n\ngrid_masked = grid.where(grid != grid.rio.nodata)\n\n\nImplicit continuous equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", ax=ax)\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", robust=True, ax=ax)\n\nplt.show()\n\n\n\n\n\nDiscrete equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", levels=7, ax=ax)\n\nplt.show()\n\n\n\n\n\nCombining with mapclassify\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.Quantiles(\n    grid_nona.to_series().dropna(), k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.FisherJenksSampled(\n    grid_nona.to_series().dropna().values, k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.StdMean(\n    grid_nona.to_series().dropna().values\n)\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.BoxPlot(\n    grid_nona.to_series().dropna().values\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Read the satellite image for Madrid used in the Chapter 1 and create three choropleths, one for each band, using the colormapsReds, Greens, Blues.\nPlay with different classification algorithms.\n\nDo the results change notably?\nIf so, why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#next-steps",
    "href": "quarto-notebooks/geovisualisation.html#next-steps",
    "title": "2  Geovisualisation",
    "section": "2.4 Next steps",
    "text": "2.4 Next steps\nIf you are interested in statistical maps based on classification, here are two recommendations to check out next:\n\nOn the technical side, the documentation for mapclassify (including its tutorials) provides more detail and illustrates more classification algorithms than those reviewed in this block.\nOn a more conceptual note, Cynthia Brewer’s “Designing better maps” (Brewer 2015) is an excellent blueprint for good map making.\n\n\n\n\n\nArribas-Bel, Dani. 2019. “A Course on Geographic Data Science.” The Journal of Open Source Education 2 (14). https://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS Users. ESRI press.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html",
    "href": "quarto-notebooks/spatial-feature-i.html",
    "title": "3  Spatial feature engineering (part I)",
    "section": "",
    "text": "3.1 Packages and modules\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport numpy as np\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial feature engineering (part I)</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#data",
    "href": "quarto-notebooks/spatial-feature-i.html#data",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.2 Data",
    "text": "3.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\nregions = geopandas.read_file(\"../data/cambodia_regional.gpkg\")\ncities = geopandas.read_file(\"../data/cambodian_cities.geojson\")\npollution = rioxarray.open_rasterio(\n    \"../data/cambodia_s5_no2.tif\"\n).sel(band=1)\nfriction = rioxarray.open_rasterio(\n    \"../data/cambodia_2020_motorized_friction_surface.tif\"\n).sel(band=1)\n\nCheck both geo-tables and the surface are in the same CRS:\n\n(\n    regions.crs.to_epsg() ==\n    cities.crs.to_epsg() ==\n    pollution.rio.crs.to_epsg()\n)\n\nTrue"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#polygons-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#polygons-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.3 Polygons to points",
    "text": "3.3 Polygons to points\nIn which region is a city?\n\nsj = geopandas.sjoin(cities, regions)\n\n\n#   City name | Region name\nsj[[\"UC_NM_MN\", \"adm2_name\"]]\n\n\n\n\n\n\n\n\nUC_NM_MN\nadm2_name\n\n\n\n\n0\nSampov Lun\nSampov Lun\n\n\n1\nKhum Pech Chenda\nPhnum Proek\n\n\n2\nPoipet\nPaoy Paet\n\n\n3\nSisophon\nSerei Saophoan\n\n\n4\nBattambang\nBattambang\n\n\n5\nSiem Reap\nSiem Reap\n\n\n6\nSihanoukville\nPreah Sihanouk\n\n\n7\nN/A\nTrapeang Prasat\n\n\n8\nKampong Chhnang\nKampong Chhnang\n\n\n9\nPhnom Penh\nTuol Kouk\n\n\n10\nKampong Cham\nKampong Cham\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties and neighbourhoods datasets, can you determine the neighbourhood group of the first ten properties?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.4 Points to polygons",
    "text": "3.4 Points to polygons\nIf we were after the number of cities per region, it is a similar approach, with a (groupby) twist at the end.\n\nWe set_index to align both tables\nWe assign to create a new column\n\nIf you want no missing values, you can fillna(0) since you know missing data are zeros.\n\nregions.set_index(\n    \"adm2_name\"\n).assign(\n    city_count=sj.groupby(\"adm2_name\").size()\n).info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 198 entries, Mongkol Borei to Administrative unit not available\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_altnm  122 non-null    object  \n 1   motor_mean  198 non-null    float64 \n 2   walk_mean   198 non-null    float64 \n 3   no2_mean    198 non-null    float64 \n 4   geometry    198 non-null    geometry\n 5   city_count  11 non-null     float64 \ndtypes: float64(4), geometry(1), object(1)\nmemory usage: 18.9+ KB\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties, can you compute how many properties each neighbourhood group has?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.5 Surface to points",
    "text": "3.5 Surface to points\nConsider attaching to each city in cities the pollution level, as expressed in pollution.\nThe code for generating the next figure is a bit more advanced as it fiddles with text, but if you want to explore it you can look at the code cell below.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    ax=ax, add_colorbar=False\n)\n\nfor i, row in cities.iterrows():\n    plt.text(\n        row.geometry.x,\n        row.geometry.y,\n        row[\"UC_NM_MN\"],\n        fontdict={\"color\": \"white\"},\n    )\n    \ncities.plot(ax=ax, color=\"r\")\n\nplt.show()\n\n\n\n\n\nfrom rasterstats import point_query\n\ncity_pollution = point_query(\n    cities,\n    pollution.values,\n    affine=pollution.rio.transform(),\n    nodata=pollution.rio.nodata\n)\ncity_pollution\n\n[3.9397064813333136e-05,\n 3.4949825609644426e-05,\n 3.825255125820345e-05,\n 4.103826573585785e-05,\n 3.067677208474005e-05,\n 5.108273256655399e-05,\n 2.2592785882580366e-05,\n 4.050414400882722e-05,\n 2.4383652926989897e-05,\n 0.0001285838935209779,\n 3.258245740282522e-05]\n\n\nAnd we can map these on the city locations:\n\nfig, ax = plt.subplots()\n\ncities.assign(\n    pollution=city_pollution\n).plot(\n    \"pollution\", \n    cmap=\"YlOrRd\",\n    legend=True,\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax,\n    crs=cities.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you calculate the pollution level at the centroid of each Cambodian region in the regional aggregates dataset? how does it compare to their average value?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.6 Surface to polygons",
    "text": "3.6 Surface to polygons\nInstead of transferring to points, we want to aggregate all the information in a surface that falls within a polygon.\nFor this case, we will use the motorised friction surface. The question we are asking thus is: what is the average degree of friction of each region? Or, in other words: what regions are harder to get through with motorised transport?\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nfriction.plot.imshow(\n    add_colorbar=False, ax=ax\n)\nregions.plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\ncontextily.add_basemap(\n    ax, \n    crs=regions.crs,\n    source=contextily.providers.CartoDB.DarkMatterOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\nAgain, we can rely on rasterstats. The output is returned from zonal_stats as a list of dicts. To make it more manageable, we convert it into a pandas.DataFrame.\n\nfrom rasterstats import zonal_stats\n\nregional_friction = pandas.DataFrame(\n    zonal_stats(\n        regions,\n        friction.values,\n        affine=friction.rio.transform(),\n        nodata=friction.rio.nodata\n    ),\n    index=regions.index\n)\nregional_friction.head()\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.001200\n0.037000\n0.006494\n979\n\n\n1\n0.001200\n0.060000\n0.007094\n1317\n\n\n2\n0.001200\n0.024112\n0.006878\n324\n\n\n3\n0.001333\n0.060000\n0.009543\n758\n\n\n4\n0.001200\n0.060132\n0.008619\n55\n\n\n\n\n\n\n\nThis can then also be mapped onto the polygon geography:\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nregions.to_crs(\n    epsg=3857\n).join(\n    regional_friction\n).plot(\n    \"mean\", scheme=\"quantiles\", ax=ax\n)\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above to obtain the average friction for each region using the walking surface (cambodia_2020_walking_friction_surface.tif)."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-surface",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-surface",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.7 Surface to surface",
    "text": "3.7 Surface to surface\nIf we want to align the pollution surface with that of friction, we need to resample them to make them “fit on the same frame”.\n\npollution.shape\n\n(138, 152)\n\n\n\nfriction.shape\n\n(574, 636)\n\n\nThis involves either moving one surface to the frame of the other one, or both into an entirely new one. For the sake of the illustration, we will do the latter and select a frame that is 300 by 400 pixels. Note this involves stretching (upsampling) pollution, while compressing (downsampling) friction.\n\n# Define dimensions\ndimX, dimY = 300, 400\nminx, miny, maxx, maxy = pollution.rio.bounds()\n# Create XY indices\nys = np.linspace(miny, maxy, dimY)\nxs = np.linspace(minx, maxx, dimX)\n# Set up placeholder array\ncanvas = xarray.DataArray(\n    np.zeros((dimY, dimX)),\n    coords=[ys, xs],\n    dims=[\"y\", \"x\"]\n).rio.write_crs(4326) # Add CRS\n\n\ncvs_pollution = pollution.rio.reproject_match(canvas)\ncvs_friction = friction.rio.reproject_match(canvas)\n\n\ncvs_pollution.shape\n\n(400, 300)\n\n\n\ncvs_pollution.shape == cvs_friction.shape\n\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Trasfer the pollution surface to the frame of friction, and viceversa,\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe following methods involve modelling and are thus more sophisticated. Take these as a conceptual introduction with an empirical illustration, but keep in mind there are extense literatures on each of them and these cover some of the simplest cases."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.8 Points to points",
    "text": "3.8 Points to points\nSee this section of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) for more details on the technique.\nFor this exampe, we will assume that, instead of a surface with pollution values, we only have available a sample of points and we would like to obtain estimates for other locations.\nFor that we will first generate 100 random points within the extent of pollution which we will take as the location of our measurement stations.\nThe code in this cell contains bits that are a bit more advanced, do not despair if not everything makes sense!\n\nnp.random.seed(123456)\n\nbb = pollution.rio.bounds()\nstation_xs = np.random.uniform(bb[0], bb[2], 100)\nstation_ys = np.random.uniform(bb[1], bb[3], 100)\nstations = geopandas.GeoSeries(\n    geopandas.points_from_xy(station_xs, station_ys),\n    crs=\"EPSG:4326\"\n)\n\nOur station values come from the pollution surface, but we assume we do not have access to the latter, and we would like to obtain estimates for the location of the cities:\n\nfig, ax = plt.subplots(1, figsize=(6, 6))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    add_colorbar=False, cmap=\"Blues\", ax=ax\n)\n\nstations.plot(ax=ax, color=\"red\", label=\"Stations\")\ncities.plot(ax=ax, color=\"lime\", label=\"Cities\")\n\nax.set_title(\"Pollution sampling\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\nWe will need the location and the pollution measurements for every station as separate arrays. Before we do that, since we will be calculating distances, we convert our coordinates to a system expressed in metres.\n\nstations_mt = stations.to_crs(epsg=5726)\nstation_xys = np.array(\n    [stations_mt.geometry.x, stations_mt.geometry.y]\n).T\n\nWe also need to extract the pollution measurements for each station location:\n\nstation_measurements = np.array(\n    point_query(\n        stations,\n        pollution.values,\n        affine=pollution.rio.transform(),\n        nodata=pollution.rio.nodata\n    )\n)\n\nAnd finally, we will also need the locations of each city expressed in the same coordinate system:\n\ncities_mt = cities.to_crs(epsg=5726)\ncity_xys = np.array(\n    [cities_mt.geometry.x, cities_mt.geometry.y]\n).T\n\nFor this illustration, we will use a \\(k\\)-nearest neighbors regression that estimates the value for each target point (cities in our case) as the average weighted by distance of its \\(k\\) nearest neigbours. In this illustration we will use \\(k=10\\).\nNote how sklearn relies only on array data structures, hence why we first had to express all the required information in that format.\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor(\n    n_neighbors=10, weights=\"distance\"\n).fit(station_xys, station_measurements)\n\nOnce we have trained the model, we can use it to obtain predictions for each city location:\n\npredictions = model.predict(city_xys)\n\nThese can be compared with the originally observed values:\n\np2p_comparison = pandas.DataFrame(\n    {\n        \"Observed\": city_pollution,\n        \"Predicted\": predictions\n    },\n    index=cities[\"UC_NM_MN\"]\n)\n\n\nfig, ax = plt.subplots(1)\np2p_comparison[\"Observed\"].plot.kde(ax=ax)\np2p_comparison[\"Predicted\"].plot.kde(ax=ax)\nax.set_axis_off()\nplt.legend(frameon=False, fontsize=20)\nplt.show()\n\n\n\n\n\np2p_comparison\n\n\n\n\n\n\n\n\nObserved\nPredicted\n\n\nUC_NM_MN\n\n\n\n\n\n\nSampov Lun\n0.000039\n0.000027\n\n\nKhum Pech Chenda\n0.000035\n0.000025\n\n\nPoipet\n0.000038\n0.000030\n\n\nSisophon\n0.000041\n0.000030\n\n\nBattambang\n0.000031\n0.000027\n\n\nSiem Reap\n0.000051\n0.000027\n\n\nSihanoukville\n0.000023\n0.000019\n\n\nN/A\n0.000041\n0.000028\n\n\nKampong Chhnang\n0.000024\n0.000032\n\n\nPhnom Penh\n0.000129\n0.000042\n\n\nKampong Cham\n0.000033\n0.000033\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above with \\(k=15\\) and \\(k=5\\). Do results change? Why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-surface",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-surface",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.9 Points to surface",
    "text": "3.9 Points to surface\nImagine we do not have a surface like pollution but we need it. In this context, if you have measurements from some locations, such as in stations, we can use the approach reviewed above to generate a surface. The trick to do this is to realise that we can generate a uniform grid of target locations that we can then express as a surface.\nWe will set as our target locations those of the pixels in the target surface we have seen above:\n\ncanvas_mt = canvas.rio.reproject(5726)\n\n\nxy_pairs = canvas_mt.to_series().index\nxys = np.array(\n    [\n        xy_pairs.get_level_values(\"x\"),\n        xy_pairs.get_level_values(\"y\")\n    ]\n).T\n\nTo obtain pollution estimates at each location, we can predict with model:\n\npredictions_grid = model.predict(xys)\n\nAnd with these at hand, we can convert them into a surface:\n\npredictions_series = pandas.DataFrame(\n    {\"predictions_grid\": predictions_grid}\n).join(\n    pandas.DataFrame(xys, columns=[\"x\", \"y\"])\n).set_index([\"y\", \"x\"])\n\npredictions_surface = xarray.DataArray().from_series(\n    predictions_series[\"predictions_grid\"]\n).rio.write_crs(canvas_mt.rio.crs)\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 6))\n\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot(ax=axs[0])\naxs[0].set_title(\"Observed\")\n\npredictions_surface.where(\n    predictions_surface&gt;0\n).rio.reproject_match(\n    cvs_pollution\n).plot(ax=axs[1])\naxs[1].set_title(\"Predicted\")\n\nplt.show()\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(9, 4))\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, label=\"Observed\"\n)\npredictions_surface.rio.reproject_match(\n    cvs_pollution\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, color=\"g\", label=\"predicted\"\n)\nplt.legend()\nplt.show()\n\n\n\n\nRoom for improvement but, remember this was a rough first pass!\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Train a model with pollution measurements from each city location and generate a surface from it. How does the output compare to the one above? Why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#polygons-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#polygons-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.10 Polygons to polygons",
    "text": "3.10 Polygons to polygons\nIn this final example, we transfer data from a polygon geography to another polygon geography. Effectively, we re-apportion values from one set of areas to another based on the extent of shared area.\nOur illustration will cover how to move pollution estimates from regions into a uniform hexagonal grid we will first create.\n\nimport tobler\n\nhex_grid = tobler.util.h3fy(\n    regions, resolution=5\n)\n\nNot that pollution is expressed as an intesive (rate) variable. We need to recognise this when specifying the interpolation model:\n\npollution_hex = tobler.area_weighted.area_interpolate(\n    regions.assign(geometry=regions.buffer(0)).to_crs(epsg=5726),\n    hex_grid.to_crs(epsg=5726), \n    intensive_variables=[\"no2_mean\"]\n)\n\nAnd the results look like:\n\nf, axs = plt.subplots(1, 3, figsize=(12, 4))\n\nregions.plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[0]\n)\naxs[0].set_axis_off()\n\nhex_grid.plot(\n    facecolor=\"none\", edgecolor=\"red\", ax=axs[1]\n)\naxs[1].set_axis_off()\n\npollution_hex.to_crs(epsg=4326).plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[2]\n)\naxs[2].set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analytis using resolution = 4. How is the result different? Why?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#next-steps",
    "href": "quarto-notebooks/spatial-feature-i.html#next-steps",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.11 Next steps",
    "text": "3.11 Next steps\nIf you are interested in learning more about spatial feature engineering through map matching, the following pointers might be useful to delve deeper into specific types of “data transfer”:\n\nThe datashader library is a great option to transfer geo-tables into surfaces, providing tooling to perform these operations in a highly efficient and performant way.\nWhen aggregating surfaces into geo-tables, the library rasterstats contains most if not all of the machinery you will need.\nFor transfers from polygon to polygon geographies, tobler is your friend. Its official documentation contains examples for different use cases.\n\n\n\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html",
    "href": "quarto-notebooks/openstreetmap.html",
    "title": "5  OpenStreetMap",
    "section": "",
    "text": "5.1 Packages and modules\nimport geopandas\nimport contextily\nimport matplotlib.pyplot as plt\nfrom IPython.display import GeoJSON",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>OpenStreetMap</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#data",
    "href": "quarto-notebooks/openstreetmap.html#data",
    "title": "5  OpenStreetMap",
    "section": "5.2 Data",
    "text": "5.2 Data\nSince some of the query options we will discuss involve pre-defined extents, we will read the Madrid neighbourhoods dataset first.\nAssuming you have the file locally on the path ../data/:\n\nneis = geopandas.read_file(\"../data/neighbourhoods.geojson\")\n\nTo make some of the examples below computationally easier on OpenStreetMap servers, we will single out the smallest neighborhood:\n\nareas = neis.to_crs(\n    epsg=32630\n).area\n\nsmallest = neis[areas == areas.min()]\nsmallest\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n98\nAtalaya\nCiudad Lineal\nMULTIPOLYGON (((-3.66195 40.46338, -3.66364 40...\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nsmallest.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#osmnx",
    "href": "quarto-notebooks/openstreetmap.html#osmnx",
    "title": "5  OpenStreetMap",
    "section": "5.3 osmnx",
    "text": "5.3 osmnx\nLet’s import one more package, osmnx, designed to easily download, model, analyse, and visualise street networks and other geospatial features from OpenStreetMap.\n\nimport osmnx as ox\n\nHere is a trick (courtesy of Martin Fleischmann to pin all your queries to OpenStreetMap to a specific date, so results are always reproducible, even if the map changes in the meantime.\n\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of the methods covered here rely on the osmnx.features module. Check out its reference here.\n\n\nThere are two broad areas to keep in mind when querying data on OpenStreetMap through osmnx:\n\nThe interface to specify the extent of the search.\nThe nature of the entities being queried. Here, the interface relies entirely on OpenStreetMap’s tagging system. Given the distributed nature of the project, this is variable, but a good place to start is:\n\n\nhttps://wiki.openstreetmap.org/wiki/Tags\n\nGenerally, the interface we will follow involves the following:\nreceived_entities = ox.features_from_XXX(\n    &lt;extent&gt;, tags={&lt;key&gt;: True/&lt;value(s)&gt;}, ...\n)\nThe &lt;extent&gt; can take several forms. We can print out the available forms:\n\n[i for i in dir(ox) if \"features_from_\" in i]\n\n['features_from_address',\n 'features_from_bbox',\n 'features_from_place',\n 'features_from_point',\n 'features_from_polygon',\n 'features_from_xml']\n\n\nThe tags follow the official feature spec."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#buildings",
    "href": "quarto-notebooks/openstreetmap.html#buildings",
    "title": "5  OpenStreetMap",
    "section": "5.4 Buildings",
    "text": "5.4 Buildings\n\nblgs = ox.features_from_polygon(\n    smallest.squeeze().geometry, tags={\"building\": True}\n)\n\n\nfig, ax = plt.subplots()\n\nblgs.plot(ax=ax)\n\nplt.show()\n\n\n\n\n\nblgs.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 115 entries, ('way', 442595762) to ('way', 577690922)\nData columns (total 27 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   name              2 non-null      object  \n 1   amenity           2 non-null      object  \n 2   geometry          115 non-null    geometry\n 3   nodes             115 non-null    object  \n 4   building          115 non-null    object  \n 5   addr:housenumber  21 non-null     object  \n 6   addr:postcode     3 non-null      object  \n 7   addr:street       9 non-null      object  \n 8   denomination      1 non-null      object  \n 9   phone             2 non-null      object  \n 10  religion          1 non-null      object  \n 11  source            1 non-null      object  \n 12  source:date       1 non-null      object  \n 13  url               1 non-null      object  \n 14  wheelchair        1 non-null      object  \n 15  building:levels   11 non-null     object  \n 16  addr:city         8 non-null      object  \n 17  addr:country      6 non-null      object  \n 18  wikidata          1 non-null      object  \n 19  website           1 non-null      object  \n 20  country           1 non-null      object  \n 21  diplomatic        1 non-null      object  \n 22  name:en           1 non-null      object  \n 23  name:fr           1 non-null      object  \n 24  name:ko           1 non-null      object  \n 25  office            1 non-null      object  \n 26  target            1 non-null      object  \ndtypes: geometry(1), object(26)\nmemory usage: 29.7+ KB\n\n\n\nblgs.head()\n\n\n\n\n\n\n\n\n\nname\namenity\ngeometry\nnodes\nbuilding\naddr:housenumber\naddr:postcode\naddr:street\ndenomination\nphone\n...\naddr:country\nwikidata\nwebsite\ncountry\ndiplomatic\nname:en\nname:fr\nname:ko\noffice\ntarget\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nway\n442595762\nNaN\nNaN\nPOLYGON ((-3.66377 40.46317, -3.66363 40.46322...\n[4402722774, 4402722775, 4402722776, 440272277...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595763\nNaN\nNaN\nPOLYGON ((-3.66394 40.46346, -3.66415 40.46339...\n[4402722778, 4402722779, 4402722780, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595764\nNaN\nNaN\nPOLYGON ((-3.66379 40.46321, -3.66401 40.46314...\n[4402722782, 4402722783, 4402722784, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595765\nNaN\nNaN\nPOLYGON ((-3.66351 40.46356, -3.66294 40.46371...\n[4402722786, 4402722787, 4402722788, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442596830\nNaN\nNaN\nPOLYGON ((-3.66293 40.46289, -3.66281 40.46294...\n[4402729658, 4402729659, 4402729660, 440272966...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 27 columns\n\n\n\nIf you want to visit the entity online, you can do so at:\n\nhttps://www.openstreetmap.org/&lt;unique_id&gt;\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Extract the building footprints for the Sol neighbourhood in neis."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#other-polygons",
    "href": "quarto-notebooks/openstreetmap.html#other-polygons",
    "title": "5  OpenStreetMap",
    "section": "5.5 Other polygons",
    "text": "5.5 Other polygons\n\npark = ox.features_from_place(\n    \"Parque El Retiro, Madrid\", tags={\"leisure\": \"park\"}\n)\n\n\nfig, ax = plt.subplots()\n\npark.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#points-of-interest",
    "href": "quarto-notebooks/openstreetmap.html#points-of-interest",
    "title": "5  OpenStreetMap",
    "section": "5.6 Points of interest",
    "text": "5.6 Points of interest\nBars around Atocha station:\n\nbars = ox.features_from_address(\n    \"Puerta de Atocha, Madrid\", tags={\"amenity\": \"bar\"}, dist=1500\n)\n\nWe can quickly explore with GeoJSON:\n\nbars.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd stores within Malasaña:\n\nshops = ox.features_from_address(\n    \"Malasaña, Madrid, Spain\", # Boundary to search within\n    tags={\n        \"shop\": True,\n        \"landuse\": [\"retail\", \"commercial\"],\n        \"building\": \"retail\"\n    },\n    dist=1000\n)\n\nshops.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe use features_from_place for delineated areas (“polygonal entities”):\n\ncs = ox.features_from_place(\n    \"Madrid, Spain\",\n    tags={\"amenity\": \"charging_station\"}\n)\n\ncs.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nSimilarly, we can work with location data. For example, searches around a given point:\n\nbakeries = ox.features_from_point(\n    (40.418881103417675, -3.6920446157455444),\n    tags={\"shop\": \"bakery\", \"craft\": \"bakery\"},\n    dist=500\n)\n\nbakeries.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n\nHow many music shops does OSM record within 750 metres of Puerta de Alcalá?\n\n- Are there more restaurants or clothing shops within the polygon that represents the Pacífico neighbourhood in neis table?"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#streets",
    "href": "quarto-notebooks/openstreetmap.html#streets",
    "title": "5  OpenStreetMap",
    "section": "5.7 Streets",
    "text": "5.7 Streets\nStreet data can be obtained as another type of entity, as above; or as a graph object.\n\n5.7.1 Geo-tables\n\ncentro = ox.features_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n    tags={\"highway\": True}\n)\n\nWe can get a quick peak into what is returned (in grey), compared to the region we used for the query:\n\nfig, ax = plt.subplots()\n\nneis.query(\n    \"neighbourhood == 'Sol'\"\n).plot(color=\"k\", ax=ax)\n\ncentro.plot(\n    ax=ax, \n    color=\"0.5\", \n    linewidth=0.2, \n    markersize=0.5\n)\n\nplt.show()\n\n\n\n\nThis however will return all sorts of things:\n\ncentro.geometry\n\nelement_type  osmid    \nnode          21734214                             POINT (-3.70427 40.41662)\n              21734250                             POINT (-3.70802 40.41612)\n              21734252                             POINT (-3.70847 40.41677)\n              21968134                             POINT (-3.69945 40.41786)\n              21968197                             POINT (-3.70054 40.41645)\n                                                 ...                        \nway           907553665    LINESTRING (-3.70686 40.41380, -3.70719 40.41369)\n              909056211    LINESTRING (-3.70705 40.42021, -3.70680 40.42020)\nrelation      5662178      POLYGON ((-3.70948 40.41551, -3.70952 40.41563...\n              7424032      POLYGON ((-3.70243 40.41716, -3.70242 40.41714...\n              8765884      POLYGON ((-3.70636 40.41475, -3.70635 40.41481...\nName: geometry, Length: 609, dtype: geometry\n\n\n\n\n5.7.2 Spatial graphs\nThe graph_from_XXX() functions return clean, processed graph objects for the street network. Available options are:\n\n[i for i in dir(ox) if \"graph_from_\" in i]\n\n['graph_from_address',\n 'graph_from_bbox',\n 'graph_from_gdfs',\n 'graph_from_place',\n 'graph_from_point',\n 'graph_from_polygon',\n 'graph_from_xml']\n\n\nHere is an example:\n\ncentro_gr = ox.graph_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n)\n\nThis is indeed a graph object (as defined by the networkx package):\n\ncentro_gr\n\n&lt;networkx.classes.multidigraph.MultiDiGraph at 0x16b6806d0&gt;\n\n\nTo visualise it, there are several plotting options:\n\n[i for i in dir(ox) if \"plot_graph\" in i]\n\n['plot_graph', 'plot_graph_folium', 'plot_graph_route', 'plot_graph_routes']\n\n\nFor example:\n\nox.plot_figure_ground(centro_gr)\nplt.show()\n\n\n\n\n\nox.graph_to_gdfs(centro_gr, nodes=False).explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How many bookshops are within a 50m radious of the Paseo de la Castellana?\nThis one involves the following steps:\n\nExtracting the street segment for Paseo de la Castellana\nDrawing a 50m buffer around it\nQuerying OSM for bookshops"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#next-steps",
    "href": "quarto-notebooks/openstreetmap.html#next-steps",
    "title": "5  OpenStreetMap",
    "section": "5.8 Next steps",
    "text": "5.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nParts of the block are inspired and informed by Geoff Boeing’s excellent course on Urban Data Science\nMore in depth content about osmnx is available in the official examples collection\nBoeing (2020) {cite}boeing2020exploring illustrates how OpenStreetMap can be used to analyse urban form (Open Access)\n\n\n\n\n\nAnderson, Jennings, Dipto Sarkar, and Leysia Palen. 2019. “Corporate Editors in the Evolving Landscape of OpenStreetMap.” ISPRS International Journal of Geo-Information 8 (5): 232."
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html",
    "href": "quarto-notebooks/transport-costs.html",
    "title": "6  Transport costs",
    "section": "",
    "text": "6.1 Packages and modules\nimport momepy\nimport geopandas\nimport contextily\nimport xarray, rioxarray\nimport osmnx as ox\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n/Users/carmen/anaconda3/envs/geo-env-new/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)",
    "crumbs": [
      "Contents",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transport costs</span>"
    ]
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#data",
    "href": "quarto-notebooks/transport-costs.html#data",
    "title": "6  Transport costs",
    "section": "6.2 Data",
    "text": "6.2 Data\nAssuming you have the file locally on the path ../data/:\n\nstreets = geopandas.read_file(\"../data/arturo_streets.gpkg\")\nabbs = geopandas.read_file(\"../data/madrid_abb.gpkg\")\nneis = geopandas.read_file(\"../data/neighbourhoods.geojson\")"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#pandana-graphs",
    "href": "quarto-notebooks/transport-costs.html#pandana-graphs",
    "title": "6  Transport costs",
    "section": "6.3 pandana graphs",
    "text": "6.3 pandana graphs\n\nimport pandana\n\nBefore building the routing network, we convert to graph and back in momepy to “clean” the network and ensure it complies with requirements for routing.\n\nnodes, edges = momepy.nx_to_gdf( # Convert back to geo-table\n    momepy.gdf_to_nx(            # Convert to a clean NX graph\n        streets.explode(index_parts='True')        # We \"explode\" to avoid multi-part rows\n    )\n)\nnodes = nodes.set_index(\"nodeID\") # Reindex nodes on ID\n\nOnce we have nodes and edges “clean” from the graph representation, we can build a pandana.Network object we will use for routing:\n\nstreets_pdn = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"mm_len\"]]\n)\n\nstreets_pdn\n\nGenerating contraction hierarchies with 8 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\n&lt;pandana.network.Network at 0x16584a850&gt;"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#shortest-path-routing",
    "href": "quarto-notebooks/transport-costs.html#shortest-path-routing",
    "title": "6  Transport costs",
    "section": "6.4 Shortest-path routing",
    "text": "6.4 Shortest-path routing\nHow do I go from A to B?\nFor example, from the first Airbnb in the geo-table…\n\nfirst = abbs.loc[[0], :].to_crs(streets.crs)\n\n…to Puerta del Sol.\n\nimport geopy\ngeopy.geocoders.options.default_user_agent = \"gds4eco\"\nsol = geopandas.tools.geocode(\n    \"Puerta del Sol, Madrid\", geopy.Nominatim\n).to_crs(streets.crs)\nsol\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (440284.049 4474264.421)\nPuerta del Sol, Barrio de los Austrias, Sol, C...\n\n\n\n\n\n\n\nFirst we snap locations to the network:\n\npt_nodes = streets_pdn.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\npt_nodes\n\n0     3071\n1    35729\nName: node_id, dtype: int64\n\n\nThen we can route the shortest path:\n\nroute_nodes = streets_pdn.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\nroute_nodes\n\narray([ 3071,  3476,  8268,  8266,  8267, 18695, 18693,  1432,  1430,\n         353,  8175,  8176, 18121, 17476, 16858, 14322, 16857, 17810,\n       44795, 41220, 41217, 41221, 41652, 18924, 18928, 48943, 18931,\n       21094, 21095, 23219, 15398, 15399, 15400, 47446, 47447, 23276,\n       47448, 23259, 23260, 23261, 27951, 27952, 27953, 48327, 11950,\n       11949, 11944, 19475, 19476, 27333, 30088, 43294, 11940, 11941,\n       11942, 48325, 37484, 48316, 15893, 15890, 15891, 29954, 25453,\n        7341, 34991, 23608, 28217, 21648, 21649, 21651, 39075, 25108,\n       25102, 25101, 25100, 48518, 47287, 34623, 31187, 29615, 48556,\n       22844, 48553, 48555, 40922, 40921, 40923, 48585, 46372, 46371,\n       46370, 45675, 45676, 38778, 38777, 19144, 20498, 20497, 20499,\n       47737, 42303, 42302, 35730, 35727, 35729])\n\n\nWith this information, we can build the route line manually.\nThe code to generate the route involves writing a function and is a bit more advanced than expected for this course. If this looks too complicated, do not despair.\n\nfrom shapely.geometry import LineString\n\ndef route_nodes_to_line(nodes, network):\n    pts = network.nodes_df.loc[nodes, :]\n    s = geopandas.GeoDataFrame(\n        {\"src_node\": [nodes[0]], \"tgt_node\": [nodes[1]]},\n        geometry=[LineString(pts.values)],\n        crs=streets.crs\n    )\n    return s\n\nWe can calculate the route:\n\nroute = route_nodes_to_line(route_nodes, streets_pdn)\n\nAnd we get it back as a geo-table (with one row):\n\nroute\n\n\n\n\n\n\n\n\nsrc_node\ntgt_node\ngeometry\n\n\n\n\n0\n3071\n3476\nLINESTRING (442606.507 4478714.516, 442597.100...\n\n\n\n\n\n\n\nPlease note this builds a simplified line for the route, not one that is based on the original geometries.\n\nfig, ax = plt.subplots()\n\nroute.plot(\n    figsize=(9, 9),\n    color=\"red\",\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=route.crs,\n    source=contextily.providers.CartoDB.Voyager,\n    zoom=14\n)\n\nplt.show()\n\n\n\n\nBut distance calculations are based on the original network). If we wanted to obtain the length of the route:\n\nroute_len = streets_pdn.shortest_path_length(\n    pt_nodes[0], pt_nodes[1]\n)\nround(route_len / 1000, 3) # Dist in Km\n\n5.458\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n- What is the network distance between CEMFI and Puerta del Sol?\n- BONUS I: how much longer is it than if you could fly in a straight line?\n- BONUS II: if one walks at a speed of 5 Km/h, how long does the walk take you?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#weighted-routing",
    "href": "quarto-notebooks/transport-costs.html#weighted-routing",
    "title": "6  Transport costs",
    "section": "6.5 Weighted routing",
    "text": "6.5 Weighted routing\nHow do I go from A to B passing by the “best” buildings?\nThis is really an extension of standard routing that takes advantage of the flexibility of pandana.Network objects.\nNote that the route we defined above, does not pass by the “best” buildings.\n\nbb = route.total_bounds\n\nfig, ax = plt.subplots()\n\nstreets.cx[\n    bb[0]: bb[2], bb[1]:bb[3]\n].plot(\n    \"average_quality\", scheme=\"quantiles\", ax=ax\n)\n\nroute.plot(color=\"r\", linewidth=2.5, ax=ax)\n\nax.set_title(\"Mean Building Quality\")\nax.set_axis_off()\n\nplt.show()\n\n\n\n\nThe overall process to achieve this is the very similar; the main difference is, when we build the Network object, to replace distance (mm_len) with a measure that combines distance and building quality. Note that we want to maximise building quality, but the routing algorithms use a minimisation function. Hence, our composite index will need to reflect that.\nThe strategy is divided in the following steps:\n\nRe-scale distance between 0 and 1\nBuild a measure inverse to building quality in the \\([0, 1]\\) range\nGenerate a combined measure (wdist) by picking a weighting parameter\nBuild a new Network object that incorporates wdist instead of distance\nCompute route between the two points of interest\n\nFor 1., we can use the scaler in scikit-learn:\n\nfrom sklearn.preprocessing import minmax_scale\n\nThen generate and attach to edges a scaled version of mm_len:\n\nedges[\"scaled_dist\"] = minmax_scale(edges[\"mm_len\"])\n\nWe can compare distance with scaled distance. The correlation should be perfect, the scaling is only a change of scale or unit.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\"mm_len\", \"scaled_dist\", ax=ax)\nax.set_title(\"Distance Vs Scaled Distance\")\nplt.show()\n\n\n\n\nWe move on to 2., with a similar approach. We will use the negative of the building quality average (average_quality):\n\nedges[\"scaled_inv_bquality\"] = minmax_scale(\n    -edges[\"average_quality\"]\n)\n\nAnd again, we can plot the relation between building quality and the scaled quality.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\n    \"average_quality\", \"scaled_inv_bquality\", ax=ax\n)\nax.set_title(\"Quality Vs Inv. Scaled Quality\")\nplt.show()\n\n\n\n\nTaking 1. and 2. into 3. we can build wdist. For this example, we will give each dimension the same weight (0.5), but this is at discretion of the researcher.\n\nw = 0.5\nedges[\"wdist\"] = (\n    edges[\"scaled_dist\"] * w +\n    edges[\"scaled_inv_bquality\"] * (1-w)\n)\n\nNow we can recreate the Network object based on our new measure (4.) and provide routing. Since it is the same process as with distance, we will do it all in one go:\n\n# Build new graph object\nw_graph = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"wdist\"]]\n)\n# Snap locations to their nearest node\npt_nodes = w_graph.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\n# Generate route\nw_route_nodes = w_graph.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\n# Build LineString\nw_route = route_nodes_to_line(\n    w_route_nodes, w_graph\n)\n\nGenerating contraction hierarchies with 8 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\nNow we are ready to display it on a map:\n\nfig, ax = plt.subplots()\n# Building quality\nstreets.plot(\n    \"average_quality\", \n    scheme=\"quantiles\", \n    cmap=\"magma\",\n    linewidth=0.5,\n    figsize=(9, 9), \n    ax=ax\n)\n# Shortest route\nroute.plot(\n    color=\"xkcd:orange red\", linewidth=3, ax=ax, label=\"Shortest\"\n)\n# Weighted route\nw_route.plot(\n    color=\"xkcd:easter green\", linewidth=3, ax=ax, label=\"Weighted\"\n)\n# Styling\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n1. Explore the differences in the output of weighted routing if you change the weight between distance and the additional constrain.\n2. Recreate weighted routing using the linearity of street segments. How can you go from A to B avoiding long streets?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#proximity",
    "href": "quarto-notebooks/transport-costs.html#proximity",
    "title": "6  Transport costs",
    "section": "6.6 Proximity",
    "text": "6.6 Proximity\nWhat is the nearest internet cafe for Airbnb’s without WiFi?\nFirst we identify Airbnb’s without WiFi:\n\nno_wifi = abbs.query(\n    \"WiFi == '0'\"\n).to_crs(streets.crs)\n\nThen pull WiFi spots in Madrid from OpenStreetMap:\n\nicafes = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"amenity\": \"internet_cafe\"}\n).to_crs(streets.crs).reset_index()\n\n\nfig, ax = plt.subplots()\n\nno_wifi.plot(\n    color=\"red\", \n    markersize=1,\n    alpha=0.5,\n    label=\"Airbnb no WiFi\",\n    figsize=(9, 9),\n    ax=ax\n)\n\nicafes.plot(\n    ax=ax, color=\"lime\", label=\"Internet cafes\"\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=no_wifi.crs,\n    source=contextily.providers.CartoDB.Voyager\n)\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\nThe logic for this operation is the following:\n\nAdd the points of interest (POIs, the internet cafes) to the network object (streets_pdn)\nFind the nearest node to each POI\nFind the nearest node to each Airbnb without WiFi\nConnect each Airbnb to its nearest internet cafe\n\nWe can add the internet cafes to the network object (1.) with the set_pois method. Note we set maxitems=1 because we are only going to query for the nearest cafe. This will make computations much faster.\n\nstreets_pdn.set_pois(\n    category=\"Internet cafes\", # Our name for the layer in the `Network` object\n    maxitems=1,                # Use to count only nearest cafe\n    maxdist=100000,            # 100km so everything is included\n    x_col=icafes.geometry.x,   # X coords of cafes\n    y_col=icafes.geometry.y,   # Y coords of cafes\n)\n\nOnce the cafes are added to the network, we can find the nearest one to each node (2.). Note there are some nodes for which we can’t find a nearest cafe. These are related to disconnected parts of the network.\n\ncafe2nnode = streets_pdn.nearest_pois(\n    100000,              # Max distance to look for\n    \"Internet cafes\",    # POIs to look for\n    num_pois=1,          # No. of POIs to include\n    include_poi_ids=True # Store POI ID\n).join(# Then add the internet cafee IDs and name\n    icafes[['osmid', 'name']],\n    on=\"poi1\"\n).rename(# Rename the distance from node to cafe\n    columns={1: \"dist2icafe\"}\n)\ncafe2nnode.head()\n\n\n\n\n\n\n\n\ndist2icafe\npoi1\nosmid\nname\n\n\nnodeID\n\n\n\n\n\n\n\n\n0\n5101.421875\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n1\n5190.265137\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n2\n5252.475098\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n3\n5095.101074\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n4\n5676.117188\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n\n\n\n\n\nTo make things easier down the line, we can link cafe2nnode to the cafe IDs. And we can also link Airbnb’s to nodes (3.) following a similar approach as we have seen above:\n\nabbs_nnode = streets_pdn.get_node_ids(\n    no_wifi.geometry.x, no_wifi.geometry.y\n)\nabbs_nnode.head()\n\n26      8872\n50     10905\n62     41158\n63     34257\n221    32215\nName: node_id, dtype: int64\n\n\nFinally, we can bring together both to find out what is the nearest internet cafe for each Airbnb (4.).\n\nabb_icafe = no_wifi[\n    [\"geometry\"]     # Keep only geometries of ABBs w/o WiFi\n].assign(\n    nnode=abbs_nnode # Attach to thse ABBs the nearest node in the network\n).join(              # Join to each ABB the nearest cafe using node IDs\n    cafe2nnode, \n    on=\"nnode\"\n)\nabb_icafe.head()\n\n\n\n\n\n\n\n\ngeometry\nnnode\ndist2icafe\npoi1\nosmid\nname\n\n\n\n\n26\nPOINT (443128.256 4483599.841)\n8872\n4926.223145\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n50\nPOINT (441885.677 4475916.602)\n10905\n1876.392944\n19.0\n6.922981e+09\nLocutorio\n\n\n62\nPOINT (440439.640 4476480.771)\n41158\n1164.812988\n17.0\n5.573414e+09\nNaN\n\n\n63\nPOINT (438485.311 4471714.377)\n34257\n1466.537964\n5.0\n2.304485e+09\nNaN\n\n\n221\nPOINT (439941.104 4473117.914)\n32215\n354.268005\n15.0\n5.412145e+09\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate distances to nearest internet cafe for ABBs with WiFi. On average, which of the two groups (with and without WiFi) are closer to internet cafes?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#accessibility",
    "href": "quarto-notebooks/transport-costs.html#accessibility",
    "title": "6  Transport costs",
    "section": "6.7 Accessibility",
    "text": "6.7 Accessibility\nThis flips the previous question on its head and, instead of asking what is the nearest POI to a given point, along the network (irrespective of distance), it asks how many POIs can I access within a network-based distance radius?\n\nparks = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"leisure\": \"park\"}\n).to_crs(streets.crs)\n\n\nFor example, how many parks are within 500m(-euclidean) of an Airbnb?\n\nWe draw a radius of 500m around each AirBnb:\n\nbuffers = geopandas.GeoDataFrame(\n    geometry=abbs.to_crs(\n        streets.crs\n    ).buffer(\n        500\n    )\n)\n\nThen intersect it with the location of parks, and count by buffer (ie. Airbnb):\n\npark_count = geopandas.sjoin(\n    parks, buffers\n).groupby(\n    \"index_right\"\n).size()\n\n\nHow many parks are within 500m(-network) of an Airbnb?\n\nWe need to approach this as a calculation within the network. The logic of steps thus looks like:\n\nUse the aggregation module in pandana to count the number of parks within 500m of each node in the network\nExtract the counts for the nodes nearest to Airbnb properties\nAssign park counts to each Airbnb\n\nWe can set up the aggregate engine (1.). This involves three steps:\n\nObtain nearest node for each park\n\n\nparks_nnode = streets_pdn.get_node_ids(\n    parks.centroid.x, parks.centroid.y\n)\n\n\nInsert the parks’ nearest node through set so it can be “aggregated”\n\n\nstreets_pdn.set(\n    parks_nnode, name=\"Parks\"\n)\n\n\n“Aggregate” for a distance of 500m, effectively counting the number of parks within 500m of each node\n\n\nparks_by_node = streets_pdn.aggregate(\n    distance=500, type=\"count\", name=\"Parks\"\n)\nparks_by_node.head()\n\nnodeID\n0    5.0\n1    5.0\n2    6.0\n3    8.0\n4    1.0\ndtype: float64\n\n\nAt this point, we have the number of parks within 500m of every node in the network. To identify those that correspond to each Airbnb (3.), we first pull out the nearest nodes to each ABB:\n\nabbs_xys = abbs.to_crs(streets.crs).geometry\nabbs_nnode = streets_pdn.get_node_ids(\n    abbs_xys.x, abbs_xys.y\n)\n\nAnd use the list to assign the count of the nearest node to each Airbnb:\n\npark_count_network = abbs_nnode.map(\n    parks_by_node\n)\npark_count_network.head()\n\n0     4.0\n1     9.0\n2     5.0\n3     0.0\n4    12.0\nName: node_id, dtype: float64\n\n\n\nFor which areas do both differ most?\n\nWe can compare the two counts above to explore to what extent the street layout is constraining access to nearby parks.\n\npark_comp = geopandas.GeoDataFrame(\n    {\n        \"Euclidean\": park_count, \n        \"Network\": park_count_network\n    },\n    geometry=abbs.geometry,\n    crs=abbs.crs\n)\n\n\nfig, ax = plt.subplots()\npark_comp.plot.scatter(\"Euclidean\", \"Network\", ax=ax)\nax.axline([0, 0], [1, 1], color='red') #45-degree line\nplt.show()\n\n\n\n\nNote there are a few cases where there are more network counts than Euclidean. These are due to the slight inaccuracies introduced by calculating network distances from nodes rather than the locations themselves.\nGeographically:\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Euclidean count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[0]\n)\ncontextily.add_basemap(\n    axs[0], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[0].set_axis_off()\naxs[0].set_title(\"Euclidean Distances\")\n\n# Count difference\nwith_parks = park_comp.query(\n    \"(Network &gt; 0) & (Euclidean &gt; 0)\"\n)\ncount_diff = 100 * (\n    with_parks[\"Euclidean\"] - \n    with_parks[\"Network\"]\n) / with_parks[\"Euclidean\"]\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=count_diff\n).dropna().plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[1]\n)\ncontextily.add_basemap(\n    axs[1], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[1].set_axis_off()\naxs[1].set_title(\"Count Difference (%)\")\n\n# Network count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count_network\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[2]\n)\ncontextily.add_basemap(\n    axs[2], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[2].set_axis_off()\naxs[2].set_title(\"Network Distances\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate accessibility to other ABBs from each ABB through the network. How many ABBs can you access within 500m of each ABB?\nNote you will need to use the locations of ABBs both as the source and the target for routing in this case."
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#next-steps",
    "href": "quarto-notebooks/transport-costs.html#next-steps",
    "title": "6  Transport costs",
    "section": "6.8 Next steps",
    "text": "6.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nThe pandana tutorial and documentation are excellent places to get a more detailed and comprehensive view into the functionality of the library"
  },
  {
    "objectID": "quarto-notebooks/references.html",
    "href": "quarto-notebooks/references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, Jennings, Dipto Sarkar, and Leysia Palen. 2019.\n“Corporate Editors in the Evolving Landscape of\nOpenStreetMap.” ISPRS International Journal of\nGeo-Information 8 (5): 232.\n\n\nArribas-Bel, Dani. 2019. “A Course on Geographic Data\nScience.” The Journal of Open Source Education 2 (14).\nhttps://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS\nUsers. ESRI press.\n\n\nMcKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython. 1st ed.\nPaperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024.\n“A Course in Geographic\nData Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming.\nGeographic Data Science with PySAL and the PyData Stack. CRC\npress."
  },
  {
    "objectID": "quarto-notebooks/environPy.html",
    "href": "quarto-notebooks/environPy.html",
    "title": "Environment",
    "section": "",
    "text": "Coding language\nThis course is primarily designed to introduce Geographic Data Science using Python as the core programming language. All course materials, assignments, and exercises are built with Python in mind, ensuring consistency and clarity throughout the learning process. Python was selected for its versatility, extensive libraries, and widespread use in the Geographic Data Science field, making it an excellent choice for both beginners and advanced users. If you are curious about conducting similar geospatial analyses in R, you can access additional resources here. However, for this course, all work should be completed in Python and following the environment setup that we introduce below.",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "quarto-notebooks/environPy.html#reproducing-code-in-this-course",
    "href": "quarto-notebooks/environPy.html#reproducing-code-in-this-course",
    "title": "Environment",
    "section": "Reproducing code in this course",
    "text": "Reproducing code in this course\nTo run the analysis and reproduce the code in Python, you will need to set up your Python environment according to the following instructions. Please follow the instructions according to your operating system.\n\n\n\n\n\n\nNote\n\n\n\nEven if you have used Python before and have set up your own environment, we very much recommend following the set up described below to ensure you can run the code smoothly.\n\n\nFollow these instructions and test your installation prior to the first session of the course. Setting up the Python environment is necessary for:\n\nExecuting the Jupyter notebooks of the sessions of the course.\nPreparing your own Jupyter notebooks.\n\nTo learn more about Jupyter notebooks, please visit this site.",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "quarto-notebooks/environPy.html#set-up-python",
    "href": "quarto-notebooks/environPy.html#set-up-python",
    "title": "Environment",
    "section": "Set up Python",
    "text": "Set up Python\n\nInstallation of Miniconda\n\nInstall Miniconda on your personal laptop: Follow the instructions here.\nDuring the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it gds4eco. You can save it wherever you want, but remember to be tidy with your folder structure!\nDownload the data to run and render the Jupyter notebooks. To learn how to download folders from github see the next section named ‘Download data folders from GitHub’.\nUnzip the folders and store the nested folders into a subfolder named data within the folder gds4eco.\nCreate another subfolder named jupyter-notebooks within gds4eco, this is where you will store the Jupyter notebooks for each session.\n\nThe folder structure should look like:\ngds4eco/\n├── data/\n└── jupyter-notebooks/\n\n\nSet up the Python Environment\n\nMS WindowsMac\n\n\n\nDownload the gds4eco.yml from GitHub by cliciking Download raw file, top right at this page.\nSave it in the folder gds4eco created before.\nType in the search bar and find the Anaconda Prompt (miniconda 3) in your personal computer. Launch it. The terminal should appear.\n\n\n\nIn the Anaconda Terminal write: conda env create -n gds4eco --file M:\\gds4eco\\gds4eco.yml and press Enter; if the file is located elsewhere you’ll need to use the corresponding file path.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nIn the Anaconda Terminal write conda activate gds4eco and press Enter. This activates your working environment.\n\n\n\n\nDownload the gds4eco.yml from GitHub by clicking Download raw file, top right at this page.\nSave it in the folder gds4eco created before.\nType in the search bar and open the Terminal.\nIn the Terminal write conda env create -n gds4eco --file gds4eco.yml and press Enter. This will need to be modified according to where you placed the gds4eco folder. For example, Carmen has named her folder gds4eco and it’s in her Documents folder, so intead of gds4eco.yml, she will write Users/carmen/Documents/gds4eco/gds4eco.yml. If Carmen had created the gds4eco folder on your desktop, the path would be Users/carmen/Desktop/gds4eco/gds4eco.yml, and so on.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\n\n\n\n\n\n\nStart a jupyter notebook\n\nMS WindowsMac\n\n\n\nDownload the Jupyter Notebook of the session from GitHub.\nSave the file in the jupyter-noteooks folder within your geo4eco folder on your machine.\nType in the search bar, find and open the Anaconda Prompt (miniconda 3).\nIn the Anaconda Terminal write and run conda activate geo4eco.\nIn the Anaconda Terminal write and run jupyter notebook. This should open Jupyter Notebook in your default browser.\nNavigate to your course folder and double click on the notebook that you downloaded.\nYou can now work on your own copy of the notebook.\n\n\n\n\nDownload the Jupyter Notebook of the session from GitHub\nSave the file in the jupyter-notebooks folder within your gds4eco folder on your machine.\nType in the search bar, find and open the Terminal.\nIn the Terminal write and run conda activate gds4eco.\nIn the Terminal write and run jupyter notebook.\nThis should open Jupyter Notebook in your default browser.\nNavigate to your folder. You can now work on your copy of the notebook.",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "quarto-notebooks/environPy.html#py-basics",
    "href": "quarto-notebooks/environPy.html#py-basics",
    "title": "Environment",
    "section": "Py Basics",
    "text": "Py Basics\nPlease refer to the tutorials from learnpython.org for an introduction to coding in Python. We particularly recommend the tutorials listed under the “Learn the Basics” section.",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "quarto-notebooks/environPy.html#resources",
    "href": "quarto-notebooks/environPy.html#resources",
    "title": "Environment",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python.\nPython for Geographic Data Analysis\nA course in Geographic Data Science, with R and Python.",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "quarto-notebooks/datasets.html",
    "href": "quarto-notebooks/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Madrid",
    "crumbs": [
      "Epilogue",
      "Datasets"
    ]
  },
  {
    "objectID": "quarto-notebooks/datasets.html#madrid",
    "href": "quarto-notebooks/datasets.html#madrid",
    "title": "Datasets",
    "section": "Madrid",
    "text": "Madrid\n\nAirbnb properties\nThis dataset has been sourced from the course “Spatial Modelling for Data Scientists”. The file imported here corresponds to the v0.1.0 version.\nThis dataset contains a pre-processed set of properties advertised on the AirBnb website within the region of Madrid (Spain), together with house characteristics.\n\nData file madrid_abb.gpkg.\nCode used to generate the file [URL].\nFurhter information [URL].\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nAirbnb neighbourhoods\nThis dataset has been directly sourced from the website Inside Airbnb. The file was imported on February 10th 2021.\nThis dataset contains neighbourhood boundaries for the city of Madrid, as provided by Inside Airbnb.\n\nData file neighbourhoods.geojson.\nFurhter information: [URL].\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nArturo\nThis dataset contains the street layout of Madrid as well as scores of habitability, where available, associated with street segments. The data originate from the Arturo Project, by 300,000Km/s, and the available file here is a slimmed down version of their official street layout distributed by the project.\n\nData file download arturo_streets.gpkg.\nCode used to generate the file [Page], borrowed from here.\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nSentinel 2 - 120m mosaic\nThis dataset contains four scenes for the region of Madrid (Spain) extracted from the Digital Twin Sandbox Sentinel-2 collection, by the SentinelHub. Each scene corresponds to the following dates in 2019:\n\nJanuary 1st\nApril 1st\nJuly 10th\nNovember 17th\n\nEach scene includes red, green, blue and near-infrared bands.\n\nData files (Jan 1st madrid_scene_s2_120_2019-1-1.tif, Apr 1st madrid_scene_s2_120_2019-4-1.tif, Jul 10th madrid_scene_s2_120_2019-7-10.tif, Nov 27th madrid_scene_s2_120_2019-11-27.tif)\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nSentinel 2 - 10m GHS composite\nThis dataset contains a scene for the region of Madrid (Spain) extracted from the GHS Composite S2, by the European Commission.\n\nData file madrid_scene_s2_10_tc.tif\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication."
  },
  {
    "objectID": "quarto-notebooks/datasets.html#cambodia",
    "href": "quarto-notebooks/datasets.html#cambodia",
    "title": "Datasets",
    "section": "Cambodia",
    "text": "Cambodia\n\nPollution\nSurface with \\(NO_2\\) measurements (tropospheric column) information attached from Sentinel 5.\n\nData file cambodia_s5_no2.tif\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\n\n\nFriction surfaces\nThis dataset is an extraction of the following two data products by Weiss et al. (2020) {cite}weiss2020global and distributed through the Malaria Atlas Project:\n\nGlobal friction surface enumerating land-based travel walking-only speed without access to motorized transport for a nominal year 2019 (Minutes required to travel one metre)\nGlobal friction surface enumerating land-based travel speed with access to motorized transport for a nominal year 2019 (Minutes required to travel one metre)\n\nEach is provided on a separate fie.\n\n️ Data files (cambodia_2020_motorized_friction_surface.tif and cambodia_2020_walking_friction_surface.tif)\nCode used to generate the file [Page]\n️ Furhter information [URL]\n\n\n\nRegional aggregates\nThis dataset relies on boundaries from the Humanitarian Data Exchange. The file is provided by the World Food Programme through the Humanitarian Data Exchange and was accessed on February 15th 2021.`\nPollution and friction aggregated at Level 2 (municipality) administrative boundaries for Cambodia.\n\nData file cambodia_regional.gpkg\nCode used to generate the file [Page], borrowed from here\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nCambodian cities\nExtract from the Urban Centre Database (UCDB), version 1.2, of the centroid for Cambodian cities.\n\n️ Data file cambodian_cities.geojson\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication."
  },
  {
    "objectID": "quarto-notebooks/download.html",
    "href": "quarto-notebooks/download.html",
    "title": "Download data folders from GitHub",
    "section": "",
    "text": "Go to https://download-directory.github.io\n\n\n\nGo to the folder you need for your Lab. For example copy: https://github.com/pietrostefani/gds/tree/main/data/London\n\n\n\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "Introduction",
      "Download data folders from GitHub"
    ]
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-feature-ii.html#packages-and-modules",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.1 Packages and modules",
    "text": "4.1 Packages and modules\n\nimport pandas, geopandas\nimport numpy as np\nimport contextily\nimport tobler\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-data.html#packages-and-modules",
    "title": "1  Spatial data",
    "section": "1.1 Packages and modules",
    "text": "1.1 Packages and modules\n\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#packages-and-modules",
    "href": "quarto-notebooks/geovisualisation.html#packages-and-modules",
    "title": "2  Geovisualisation",
    "section": "2.1 Packages and modules",
    "text": "2.1 Packages and modules\n\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport seaborn as sns\nfrom pysal.viz import mapclassify as mc\nfrom legendgram import legendgram\nimport matplotlib.pyplot as plt\nimport palettable.matplotlib as palmpl\nfrom splot.mapping import vba_choropleth"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#packages-and-modules",
    "href": "quarto-notebooks/transport-costs.html#packages-and-modules",
    "title": "6  Transport costs",
    "section": "6.1 Packages and modules",
    "text": "6.1 Packages and modules\n\nimport momepy\nimport geopandas\nimport contextily\nimport xarray, rioxarray\nimport osmnx as ox\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-feature-i.html#packages-and-modules",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.1 Packages and modules",
    "text": "3.1 Packages and modules\n\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#packages-and-modules",
    "href": "quarto-notebooks/openstreetmap.html#packages-and-modules",
    "title": "5  OpenStreetMap",
    "section": "5.1 Packages and modules",
    "text": "5.1 Packages and modules\n\nimport geopandas\nimport contextily\nimport matplotlib.pyplot as plt\nfrom IPython.display import GeoJSON"
  }
]