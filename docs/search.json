[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic Data Science for Applied Economists",
    "section": "",
    "text": "Welcome\n\nThis is the website for the “Geographic Data Science for applied economists using Python”. This module is delivered by Dr. Carmen Cabrera-Arnau from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. The course material has been designed by Prof. Dani Arribas-Bel, Prof. Diego Puga and Dr. Carmen Cabrera-Arnau.\n\n\n\n\n\n\nContact\n\n\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk - Lecturer in Geographic Data Science - Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, UK.\n\n\nDani Arribas-Bel - D.Arribas-Bel [at] liverpool.ac.uk - Professor in Geographic Data Science - Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, UK.\n\n\nDiego Puga - diego.puga [at] cemfi.es - Professor - CEMFI, Casado del Alisal 5, 28014 Madrid, Spain."
  },
  {
    "objectID": "quarto-notebooks/overview.html#aims",
    "href": "quarto-notebooks/overview.html#aims",
    "title": "Overview",
    "section": "Aims",
    "text": "Aims\nThe course has three primary aims:\n\nEquip students with essential skills in Geographic Data Science (GDS), improving their statistical and numerical understanding and familiarity with fundamental programming concepts and modern computational tools for GDS;\nOffer a thorough overview of key methodologies used by Geographic Data Scientists, along with insights on how and when to apply them;\nEmphasise practical applications of these techniques within real-world geographical and applied settings."
  },
  {
    "objectID": "quarto-notebooks/overview.html#learning-outcomes",
    "href": "quarto-notebooks/overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this course, students will be able to:\n\nIllustrate advanced techniques in GIS/GDS and use them programmatically for importing, manipulating, and analysing data in various formats.\nExplain the rationale and mechanics behind key methodological approaches in GDS, both from analytical and visual perspectives.\nAssess the suitability of specific techniques, their capabilities, and how they can address relevant questions.\nImplement various spatial analysis methods and interpret the outcomes, transforming raw data into meaningful insights.\nIndependently handle new datasets using GIS/GDS tools in a programmatic manner.\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations."
  },
  {
    "objectID": "quarto-notebooks/environPy.html#coding-language",
    "href": "quarto-notebooks/environPy.html#coding-language",
    "title": "Environment",
    "section": "Coding language",
    "text": "Coding language\nThis course is primarily designed to introduce Geographic Data Science using Python as the core programming language. All course materials, assignments, and exercises are built with Python in mind, ensuring consistency and clarity throughout the learning process. Python was selected for its versatility, extensive libraries, and widespread use in the Geographic Data Science field, making it an excellent choice for both beginners and advanced users. If you are curious about conducting similar geospatial analyses in R, you can access additional resources here. However, for this course, all work should be completed in Python and following the environment setup that we introduce below."
  },
  {
    "objectID": "quarto-notebooks/environPy.html#reproducing-code-in-this-course",
    "href": "quarto-notebooks/environPy.html#reproducing-code-in-this-course",
    "title": "Environment",
    "section": "Reproducing code in this course",
    "text": "Reproducing code in this course\nTo run the analysis and reproduce the code in Python, you will need to set up your Python environment according to the following instructions. Please follow the instructions according to your operating system.\n\n\n\n\n\n\nNote\n\n\n\nEven if you have used Python before and have set up your own environment, we very much recommend following the set up described below to ensure you can run the code smoothly.\n\n\nFollow these instructions and test your installation prior to the first session of the course. Setting up the Python environment is necessary for:\n\nExecuting the Jupyter notebooks of the sessions of the course.\nPreparing your own Jupyter notebooks.\n\nTo learn more about Jupyter notebooks, please visit this site."
  },
  {
    "objectID": "quarto-notebooks/environPy.html#set-up-python",
    "href": "quarto-notebooks/environPy.html#set-up-python",
    "title": "Environment",
    "section": "Set up Python",
    "text": "Set up Python\n\nInstallation of Miniconda\n\nInstall Miniconda on your personal laptop: Follow the instructions here.\nDuring the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it gds4eco. You can save it wherever you want, but remember to be tidy with your folder structure!\nDownload the data to run and render the Jupyter notebooks. To learn how to download folders from github see the next section named ‘Download data folders from GitHub’.\nUnzip the folders and store the nested folders into a subfolder named data within the folder gds4eco.\nCreate another subfolder named jupyter-notebooks within gds4eco, this is where you will store the Jupyter notebooks for each session.\n\nThe folder structure should look like:\ngds4eco/\n├── data/\n└── jupyter-notebooks/\n\n\nSet up the Python Environment\n\nMS WindowsMac\n\n\n\nDownload the gds4eco.yml from GitHub by cliciking Download raw file, top right at this page.\nSave it in the folder gds4eco created before.\nType in the search bar and find the Anaconda Prompt (miniconda 3) in your personal computer. Launch it. The terminal should appear.\n\n\n\nIn the Anaconda Terminal write: conda env create -n gds4eco --file M:\\gds4eco\\gds4eco.yml and press Enter; if the file is located elsewhere you’ll need to use the corresponding file path.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nIn the Anaconda Terminal write conda activate gds4eco and press Enter. This activates your working environment.\n\n\n\n\nDownload the gds4eco.yml from GitHub by clicking Download raw file, top right at this page.\nSave it in the folder gds4eco created before.\nType in the search bar and open the Terminal.\nIn the Terminal write conda env create -n gds4eco --file gds4eco.yml and press Enter. This will need to be modified according to where you placed the gds4eco folder. For example, Carmen has named her folder gds4eco and it’s in her Documents folder, so intead of gds4eco.yml, she will write Users/carmen/Documents/gds4eco/gds4eco.yml. If Carmen had created the gds4eco folder on your desktop, the path would be Users/carmen/Desktop/gds4eco/gds4eco.yml, and so on.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\n\n\n\n\n\n\nStart a jupyter notebook\n\nMS WindowsMac\n\n\n\nDownload the Jupyter Notebook of the session from GitHub.\nSave the file in the jupyter-noteooks folder within your geo4eco folder on your machine.\nType in the search bar, find and open the Anaconda Prompt (miniconda 3).\nIn the Anaconda Terminal write and run conda activate geo4eco.\nIn the Anaconda Terminal write and run jupyter notebook. This should open Jupyter Notebook in your default browser.\nNavigate to your course folder and double click on the notebook that you downloaded.\nYou can now work on your own copy of the notebook.\n\n\n\n\nDownload the Jupyter Notebook of the session from GitHub\nSave the file in the jupyter-notebooks folder within your gds4eco folder on your machine.\nType in the search bar, find and open the Terminal.\nIn the Terminal write and run conda activate gds4eco.\nIn the Terminal write and run jupyter notebook.\nThis should open Jupyter Notebook in your default browser.\nNavigate to your folder. You can now work on your copy of the notebook."
  },
  {
    "objectID": "quarto-notebooks/environPy.html#py-basics",
    "href": "quarto-notebooks/environPy.html#py-basics",
    "title": "Environment",
    "section": "Py Basics",
    "text": "Py Basics\nPlease refer to the tutorials from learnpython.org for an introduction to coding in Python. We particularly recommend the tutorials listed under the “Learn the Basics” section."
  },
  {
    "objectID": "quarto-notebooks/environPy.html#resources",
    "href": "quarto-notebooks/environPy.html#resources",
    "title": "Environment",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python.\nPython for Geographic Data Analysis.\nA course in Geographic Data Science, with R and Python."
  },
  {
    "objectID": "quarto-notebooks/download.html",
    "href": "quarto-notebooks/download.html",
    "title": "Download data folders from GitHub",
    "section": "",
    "text": "Go to https://download-directory.github.io\n\n\n\nGo to the folder you need for your Lab. For example copy: https://github.com/pietrostefani/gds/tree/main/data/London\n\n\n\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-data.html#packages-and-modules",
    "title": "1  Spatial data",
    "section": "1.1 Packages and modules",
    "text": "1.1 Packages and modules\n\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#data",
    "href": "quarto-notebooks/spatial-data.html#data",
    "title": "1  Spatial data",
    "section": "1.2 Data",
    "text": "1.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\n\n1.2.1 Points\nAssuming you have the file locally on the path ../data/:\n\npts = geopandas.read_file(\"../data/madrid_abb.gpkg\")\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes, points are provided as separate columns in an otherwise non-spatial table. For example imagine we have an object cols with a column named X for longitude and Y for latitude. Then, we can convert those into proper geometries by running pts = geopandas.GeoSeries( geopandas.points_from_xy(cols[\"X\"], cols[\"Y\"]).\n\n\nLet’s explore the points dataset that we loaded above.\n\npts.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 18399 entries, 0 to 18398\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   price            18399 non-null  object  \n 1   price_usd        18399 non-null  float64 \n 2   log1p_price_usd  18399 non-null  float64 \n 3   accommodates     18399 non-null  int64   \n 4   bathrooms        18399 non-null  object  \n 5   bedrooms         18399 non-null  float64 \n 6   beds             18399 non-null  float64 \n 7   neighbourhood    18399 non-null  object  \n 8   room_type        18399 non-null  object  \n 9   property_type    18399 non-null  object  \n 10  WiFi             18399 non-null  object  \n 11  Coffee           18399 non-null  object  \n 12  Gym              18399 non-null  object  \n 13  Parking          18399 non-null  object  \n 14  km_to_retiro     18399 non-null  float64 \n 15  geometry         18399 non-null  geometry\ndtypes: float64(5), geometry(1), int64(1), object(9)\nmemory usage: 2.2+ MB\n\n\n\npts.head()\n\n\n\n\n\n\n\n\nprice\nprice_usd\nlog1p_price_usd\naccommodates\nbathrooms\nbedrooms\nbeds\nneighbourhood\nroom_type\nproperty_type\nWiFi\nCoffee\nGym\nParking\nkm_to_retiro\ngeometry\n\n\n\n\n0\n$60.00\n60.0\n4.110874\n2\n1 shared bath\n1.0\n1.0\nHispanoamérica\nPrivate room\nPrivate room in apartment\n1\n0\n0\n0\n5.116664\nPOINT (-3.67688 40.45724)\n\n\n1\n$31.00\n31.0\n3.465736\n1\n1 bath\n1.0\n1.0\nCármenes\nPrivate room\nPrivate room in apartment\n1\n1\n0\n1\n5.563869\nPOINT (-3.74084 40.40341)\n\n\n2\n$60.00\n60.0\n4.110874\n6\n2 baths\n3.0\n5.0\nLegazpi\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n3.048442\nPOINT (-3.69304 40.38695)\n\n\n3\n$115.00\n115.0\n4.753590\n4\n1.5 baths\n2.0\n3.0\nJusticia\nEntire home/apt\nEntire apartment\n1\n1\n0\n1\n2.075484\nPOINT (-3.69764 40.41995)\n\n\n4\n$26.00\n26.0\n3.295837\n1\n1 private bath\n1.0\n1.0\nLegazpi\nPrivate room\nPrivate room in house\n1\n0\n0\n0\n2.648058\nPOINT (-3.69011 40.38985)\n\n\n\n\n\n\n\n\n\n1.2.2 Lines\nAssuming you have the file locally on the path ../data/:\n\nlines = geopandas.read_file(\"../data/arturo_streets.gpkg\")\n\n\nlines.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 66499 entries, 0 to 66498\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   OGC_FID             66499 non-null  object  \n 1   dm_id               66499 non-null  object  \n 2   dist_barri          66483 non-null  object  \n 3   average_quality     66499 non-null  float64 \n 4   population_density  66499 non-null  float64 \n 5   X                   66499 non-null  float64 \n 6   Y                   66499 non-null  float64 \n 7   value               5465 non-null   float64 \n 8   geometry            66499 non-null  geometry\ndtypes: float64(5), geometry(1), object(3)\nmemory usage: 4.6+ MB\n\n\n\nlines.loc[0, \"geometry\"]\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Print descriptive statistics for population_density and average_quality.\n\n\n\n\n1.2.3 Polygons\nAssuming you have the file locally on the path ../data/:\n\npolys = geopandas.read_file(\"../data/neighbourhoods.geojson\")\n\n\npolys.head()\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n0\nPalacio\nCentro\nMULTIPOLYGON (((-3.70584 40.42030, -3.70625 40...\n\n\n1\nEmbajadores\nCentro\nMULTIPOLYGON (((-3.70384 40.41432, -3.70277 40...\n\n\n2\nCortes\nCentro\nMULTIPOLYGON (((-3.69796 40.41929, -3.69645 40...\n\n\n3\nJusticia\nCentro\nMULTIPOLYGON (((-3.69546 40.41898, -3.69645 40...\n\n\n4\nUniversidad\nCentro\nMULTIPOLYGON (((-3.70107 40.42134, -3.70155 40...\n\n\n\n\n\n\n\n\npolys.query(\"neighbourhood_group == 'Retiro'\")\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n13\nPacífico\nRetiro\nMULTIPOLYGON (((-3.67015 40.40654, -3.67017 40...\n\n\n14\nAdelfas\nRetiro\nMULTIPOLYGON (((-3.67283 40.39468, -3.67343 40...\n\n\n15\nEstrella\nRetiro\nMULTIPOLYGON (((-3.66506 40.40647, -3.66512 40...\n\n\n16\nIbiza\nRetiro\nMULTIPOLYGON (((-3.66916 40.41796, -3.66927 40...\n\n\n17\nJerónimos\nRetiro\nMULTIPOLYGON (((-3.67874 40.40751, -3.67992 40...\n\n\n18\nNiño Jesús\nRetiro\nMULTIPOLYGON (((-3.66994 40.40850, -3.67012 40...\n\n\n\n\n\n\n\n\npolys.neighbourhood_group.unique()\n\narray(['Centro', 'Arganzuela', 'Retiro', 'Salamanca', 'Chamartín',\n       'Moratalaz', 'Tetuán', 'Chamberí', 'Fuencarral - El Pardo',\n       'Moncloa - Aravaca', 'Puente de Vallecas', 'Latina', 'Carabanchel',\n       'Usera', 'Ciudad Lineal', 'Hortaleza', 'Villaverde',\n       'Villa de Vallecas', 'Vicálvaro', 'San Blas - Canillejas',\n       'Barajas'], dtype=object)"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#surfaces",
    "href": "quarto-notebooks/spatial-data.html#surfaces",
    "title": "1  Spatial data",
    "section": "1.3 Surfaces",
    "text": "1.3 Surfaces\nAssuming you have the file locally on the path ../data/:\n\nsat = rioxarray.open_rasterio(\"../data/madrid_scene_s2_10_tc.tif\")\n\n\nsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 3681, x: 3129)&gt;\n[34553547 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 3681x: 3129...[34553547 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0, 424825.0,\n       424835.0, 424845.0, 424855.0,\n       ...\n       455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0, 456015.0,\n       456025.0, 456035.0, 456045.0],\n      dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n       4499305.0, 4499295.0, 4499285.0, 4499275.0,\n       ...\n       4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n       4462595.0, 4462585.0, 4462575.0, 4462565.0],\n      dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(band=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3681, x: 3129)&gt;\n[11517849 values with dtype=uint8]\nCoordinates:\n    band         int64 1\n  * x            (x) float64 4.248e+05 4.248e+05 4.248e+05 ... 4.56e+05 4.56e+05\n  * y            (y) float64 4.499e+06 4.499e+06 ... 4.463e+06 4.463e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayy: 3681x: 3129...[11517849 values with dtype=uint8]Coordinates: (4)band()int641array(1)x(x)float644.248e+05 4.248e+05 ... 4.56e+05array([424765., 424775., 424785., ..., 456025., 456035., 456045.])y(y)float644.499e+06 4.499e+06 ... 4.463e+06array([4499365., 4499355., 4499345., ..., 4462585., 4462575., 4462565.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([424765.0, 424775.0, 424785.0, 424795.0, 424805.0, 424815.0, 424825.0,\n       424835.0, 424845.0, 424855.0,\n       ...\n       455955.0, 455965.0, 455975.0, 455985.0, 455995.0, 456005.0, 456015.0,\n       456025.0, 456035.0, 456045.0],\n      dtype='float64', name='x', length=3129))yPandasIndexPandasIndex(Index([4499365.0, 4499355.0, 4499345.0, 4499335.0, 4499325.0, 4499315.0,\n       4499305.0, 4499295.0, 4499285.0, 4499275.0,\n       ...\n       4462655.0, 4462645.0, 4462635.0, 4462625.0, 4462615.0, 4462605.0,\n       4462595.0, 4462585.0, 4462575.0, 4462565.0],\n      dtype='float64', name='y', length=3681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nsat.sel(\n    x=slice(430000, 440000),  # x is ascending\n    y=slice(4480000, 4470000) # y is descending\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 3, y: 1000, x: 1000)&gt;\n[3000000 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 4.3e+05 4.3e+05 4.3e+05 ... 4.4e+05 4.4e+05 4.4e+05\n  * y            (y) float64 4.48e+06 4.48e+06 4.48e+06 ... 4.47e+06 4.47e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 3y: 1000x: 1000...[3000000 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float644.3e+05 4.3e+05 ... 4.4e+05 4.4e+05array([430005., 430015., 430025., ..., 439975., 439985., 439995.])y(y)float644.48e+06 4.48e+06 ... 4.47e+06array([4479995., 4479985., 4479975., ..., 4470025., 4470015., 4470005.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 30Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 30N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32630\"]]GeoTransform :424760.0 10.0 0.0 4499370.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([430005.0, 430015.0, 430025.0, 430035.0, 430045.0, 430055.0, 430065.0,\n       430075.0, 430085.0, 430095.0,\n       ...\n       439905.0, 439915.0, 439925.0, 439935.0, 439945.0, 439955.0, 439965.0,\n       439975.0, 439985.0, 439995.0],\n      dtype='float64', name='x', length=1000))yPandasIndexPandasIndex(Index([4479995.0, 4479985.0, 4479975.0, 4479965.0, 4479955.0, 4479945.0,\n       4479935.0, 4479925.0, 4479915.0, 4479905.0,\n       ...\n       4470095.0, 4470085.0, 4470075.0, 4470065.0, 4470055.0, 4470045.0,\n       4470035.0, 4470025.0, 4470015.0, 4470005.0],\n      dtype='float64', name='y', length=1000))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Subset sat to band 2 and the section within [444444, 455555] of Easting and [4470000, 4480000] of Northing.\n\nHow many pixels does it contain?\nWhat if you used bands 1 and 3 instead?"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#visualisation",
    "href": "quarto-notebooks/spatial-data.html#visualisation",
    "title": "1  Spatial data",
    "section": "1.4 Visualisation",
    "text": "1.4 Visualisation\nYou will need version 0.10.0 or greater of geopandas to use explore.\n\npolys.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nfig, ax = plt.subplots()\npolys.plot(ax=ax)\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\nlines.plot(linewidth=0.1, color=\"black\", ax=ax)\n#contextily.add_basemap(ax, crs=lines.crs)\nplt.show()\n\n\n\n\nSee more basemap options here.\n\nfig, ax = plt.subplots()\npts.plot(color=\"red\", figsize=(12, 12), markersize=0.1, ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs = pts.crs,\n    source = contextily.providers.CartoDB.DarkMatter\n)\nplt.show()\n\n\n\n\n\nsat.plot.imshow(figsize=(12, 12))\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsat.plot.imshow(ax=ax)\ncontextily.add_basemap(\n    ax,\n    crs=sat.rio.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=11,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Make three plots of sat, plotting one single band in each."
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#spatial-operations",
    "href": "quarto-notebooks/spatial-data.html#spatial-operations",
    "title": "1  Spatial data",
    "section": "1.5 Spatial operations",
    "text": "1.5 Spatial operations\n\n1.5.1 (Re-)Projections\n\npts.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.crs\n\nCRS.from_epsg(32630)\n\n\n\npts.to_crs(sat.rio.crs).crs\n\n&lt;Projected CRS: EPSG:32630&gt;\nName: WGS 84 / UTM zone 30N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nsat.rio.reproject(pts.crs).rio.crs\n\nCRS.from_epsg(4326)\n\n\n\n# All into Web Mercator (EPSG:3857)\nfig, ax = plt.subplots(1, figsize=(12, 12))\n\n## Satellite image\nsat.rio.reproject(\n    \"EPSG:3857\"\n).plot.imshow(\n    ax=ax\n)\n\n## Neighbourhoods\npolys.to_crs(epsg=3857).plot(\n    linewidth=1, \n    edgecolor=\"xkcd:lime\", \n    facecolor=\"none\",\n    ax=ax\n)\n\n## Labels\ncontextily.add_basemap( # No need to reproject\n    ax,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n)\n\nplt.show()\n\n\n\n\n\n\n1.5.2 Centroids\nNote the warning that geometric operations with non-projected CRS object result in biases.\n\npolys.centroid\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_7098/2101097851.py:1: UserWarning:\n\nGeometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n\n\n\n0      POINT (-3.71398 40.41543)\n1      POINT (-3.70237 40.40925)\n2      POINT (-3.69674 40.41485)\n3      POINT (-3.69657 40.42367)\n4      POINT (-3.70698 40.42568)\n                 ...            \n123    POINT (-3.59135 40.45656)\n124    POINT (-3.59723 40.48441)\n125    POINT (-3.55847 40.47613)\n126    POINT (-3.57889 40.47471)\n127    POINT (-3.60718 40.46415)\nLength: 128, dtype: geometry\n\n\nIt is therefore important to re-project these geometries to a projected crs such as we did with with pts before.\n\npolys = polys.to_crs(sat.rio.crs)\n\nNow, we can compute centroids without warnings:\n\npolys.centroid\n\n0      POINT (439425.451 4474112.019)\n1      POINT (440404.977 4473418.085)\n2      POINT (440887.707 4474036.547)\n3      POINT (440909.920 4475014.820)\n4      POINT (440028.666 4475245.024)\n                    ...              \n123    POINT (449860.280 4478601.086)\n124    POINT (449382.527 4481695.863)\n125    POINT (452661.832 4480754.248)\n126    POINT (450929.735 4480608.573)\n127    POINT (448523.423 4479452.348)\nLength: 128, dtype: geometry\n\n\n\nfig, ax = plt.subplots()\npolys.plot(color=\"purple\", ax=ax)\npolys.centroid.plot(\n    ax=ax, color=\"lime\", markersize=1\n)\n\nplt.show()\n\n\n\n\n\n\n1.5.3 Spatial joins\nMore information about spatial joins in geopandas is available on its documentation page.\nLet’s ensure that the geometries we are looking to join are in the same projection.\n\nlines = lines.to_crs(polys.crs)\n\n\nsj = geopandas.sjoin(\n    lines,\n    polys\n)\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\nfig, ax = plt.subplots()\n\n# Subset of lines\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).plot(color=\"xkcd:bright turquoise\", ax=ax)\n\n# Subset of line centroids\nsj.query(\n    \"neighbourhood == 'Jerónimos'\"\n).centroid.plot(\n    color=\"xkcd:bright violet\", markersize=7, ax=ax\n)\n\n# Local basemap\ncontextily.add_basemap(\n    ax,\n    crs=sj.crs,\n    source=\"../data/madrid_scene_s2_10_tc.tif\",\n    alpha=0.5\n)\n\nplt.show()\n\n\n\n\n\nsj.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 69420 entries, 0 to 66438\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   OGC_FID              69420 non-null  object  \n 1   dm_id                69420 non-null  object  \n 2   dist_barri           69414 non-null  object  \n 3   average_quality      69420 non-null  float64 \n 4   population_density   69420 non-null  float64 \n 5   X                    69420 non-null  float64 \n 6   Y                    69420 non-null  float64 \n 7   value                5769 non-null   float64 \n 8   geometry             69420 non-null  geometry\n 9   index_right          69420 non-null  int64   \n 10  neighbourhood        69420 non-null  object  \n 11  neighbourhood_group  69420 non-null  object  \ndtypes: float64(5), geometry(1), int64(1), object(5)\nmemory usage: 6.9+ MB\n\n\n\n\n1.5.4 Areas\nTo compute areas of polygons, use a projected crs (we already transformed polys to the same projection as sat, which is a projected crs).\n\nareas = polys.area * 1e-6 # Km2\nareas.head()\n\n0    1.471037\n1    1.033253\n2    0.592049\n3    0.742031\n4    0.947616\ndtype: float64\n\n\n\n\n1.5.5 Distances\nWe can give geopandas.tools.geocode() a string or a set of strings corresponding to addresses. It will geocode it and return a GeoDataFrame of the resulting point geometries\n\ncemfi = geopandas.tools.geocode(\n    \"Calle Casado del Alisal, 5, Madrid\"\n).to_crs(sat.rio.crs)\n\n\ncemfi\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (441477.245 4473939.537)\n5, Calle Casado del Alisal, 28014, Calle Casad...\n\n\n\n\n\n\n\nWe can compute the distance between the point for cemfi and the centroids of all the polygons in polys ensuring they both are in the same crs:\n\npolys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry\n)\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_7098/176561454.py:3: UserWarning:\n\nThe indices of the two GeoSeries are different.\n\n\n\n0      1491.338749\n1              NaN\n2              NaN\n3              NaN\n4              NaN\n          ...     \n123            NaN\n124            NaN\n125            NaN\n126            NaN\n127            NaN\nLength: 128, dtype: float64\n\n\n\nd2cemfi = polys.to_crs(\n    cemfi.crs\n).distance(\n    cemfi.geometry[0] # NO index\n)\nd2cemfi.head()\n\n0    1491.338749\n1     565.418135\n2     278.121017\n3     650.926572\n4    1196.771601\ndtype: float64\n\n\nMake a map, colouring the polygons according the the distance of their centroid to cemfi:\n\nfig, ax = plt.subplots()\n\npolys.assign(\n    dist=d2cemfi/1000\n).plot(\"dist\", legend=True, ax=ax)\n\ncemfi.to_crs(\n    polys.crs\n).plot(\n    marker=\"*\", \n    markersize=15, \n    color=\"r\", \n    label=\"CEMFI\", \n    ax=ax\n)\n\nax.legend()\nax.set_title(\n    \"Distance to CEMFI\"\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/spatial-data.html#next-steps",
    "href": "quarto-notebooks/spatial-data.html#next-steps",
    "title": "1  Spatial data",
    "section": "1.6 Next steps",
    "text": "1.6 Next steps\nIf you are interested in following up on some of the topics explored in this block, the following pointers might be useful:\n\nAlthough we have seen here geopandas only, all non-geographic operations on geo-tables are really thanks to pandas, the workhorse for tabular data in Python. Their official documentation is an excellent first stop. If you prefer a book, (McKinney 2013) is a great one.\nFor more detail on geographic operations on geo-tables, the Geopandas official documentation is a great place to continue the journey.\nSurfaces, as covered here, are really an example of multi-dimensional labelled arrays. The library we use, xarray represents the cutting edge for working with these data structures in Python, and their documentation is a great place to wrap your head around how data of this type can be manipulated. For geographic extensions (CRS handling, reprojections, etc.), we have used rioxarray under the hood, and its documentation is also well worth checking.\n\n\n\n\n\nMcKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. 1st ed. Paperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#packages-and-modules",
    "href": "quarto-notebooks/geovisualisation.html#packages-and-modules",
    "title": "2  Geovisualisation",
    "section": "2.1 Packages and modules",
    "text": "2.1 Packages and modules\n\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport seaborn as sns\nfrom pysal.viz import mapclassify as mc\nfrom legendgram import legendgram\nimport matplotlib.pyplot as plt\nimport palettable.matplotlib as palmpl\nfrom splot.mapping import vba_choropleth"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#data",
    "href": "quarto-notebooks/geovisualisation.html#data",
    "title": "2  Geovisualisation",
    "section": "2.2 Data",
    "text": "2.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\ndb = geopandas.read_file(\"../data/cambodia_regional.gpkg\")\n\nQuick visualisation:\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    edgecolor=\"red\",\n    facecolor=\"none\",\n    linewidth=2,\n    alpha=0.25,\n    figsize=(9, 9),\n    ax=ax\n)\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.Esri.NatGeoWorldMap\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\ndb.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 198 entries, 0 to 197\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_name   198 non-null    object  \n 1   adm2_altnm  122 non-null    object  \n 2   motor_mean  198 non-null    float64 \n 3   walk_mean   198 non-null    float64 \n 4   no2_mean    198 non-null    float64 \n 5   geometry    198 non-null    geometry\ndtypes: float64(3), geometry(1), object(2)\nmemory usage: 9.4+ KB\n\n\nWe will use the average measurement of nitrogen dioxide (no2_mean) by region throughout the block.\nTo make visualisation a bit easier below, we create an additional column with values rescaled:\n\ndb[\"no2_viz\"] = db[\"no2_mean\"] * 1e5\n\nThis way, numbers are larger and will fit more easily on legends:\n\ndb[[\"no2_mean\", \"no2_viz\"]].describe()\n\n\n\n\n\n\n\n\nno2_mean\nno2_viz\n\n\n\n\ncount\n198.000000\n198.000000\n\n\nmean\n0.000032\n3.236567\n\n\nstd\n0.000017\n1.743538\n\n\nmin\n0.000014\n1.377641\n\n\n25%\n0.000024\n2.427438\n\n\n50%\n0.000029\n2.922031\n\n\n75%\n0.000034\n3.390426\n\n\nmax\n0.000123\n12.323324"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#choropleths",
    "href": "quarto-notebooks/geovisualisation.html#choropleths",
    "title": "2  Geovisualisation",
    "section": "2.3 Choropleths",
    "text": "2.3 Choropleths\n\nfig, ax = plt.subplots()\ndb.to_crs(\n    epsg=3857\n).plot(\n    \"no2_viz\", \n    legend=True,\n    figsize=(12, 9),\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n2.3.1 A classiffication problem\n\ndb[\"no2_viz\"].unique().shape\n\n(198,)\n\n\n\nsns.displot(\n    db, x=\"no2_viz\", kde=True, aspect=2\n)\n\nplt.show()\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\n\n2.3.2 How to assign colors?\n\n\n\n\n\n\nImportant\n\n\n\nTo build an intuition behind each classification algorithm more easily, we create a helper method (plot_classi) that generates a visualisation of a given classification.\n\n\n\ndef plot_classi(classi, col, db):\n    \"\"\"\n    Illustrate a classiffication\n    ...\n    \n    Arguments\n    ---------\n    classi : mapclassify.classifiers\n             Classification object\n    col    : str\n             Column name used for `classi`\n    db     : geopandas.GeoDataFrame\n             Geo-table with data for\n             the classification    \n    \"\"\"\n    f, ax = plt.subplots(figsize=(12, 6))\n    ax.set_title(classi.name)\n    # KDE\n    sns.kdeplot(\n        db[col], fill=True, ax=ax\n    )\n    for i in range(0, len(classi.bins)-1):\n        ax.axvline(classi.bins[i], color=\"red\")\n    # Map\n    aux = f.add_axes([.6, .45, .32, .4])\n    db.assign(lbls=classi.yb).plot(\n        \"lbls\", cmap=\"viridis\", ax=aux\n    )\n    aux.set_axis_off()\n    plt.show()\n    return None\n\n\nEqual intervals\n\n\nclassi = mc.EqualInterval(db[\"no2_viz\"], k=7)\nclassi\n\nEqualInterval\n\n   Interval      Count\n----------------------\n[ 1.38,  2.94] |   103\n( 2.94,  4.50] |    80\n( 4.50,  6.07] |     6\n( 6.07,  7.63] |     1\n( 7.63,  9.20] |     3\n( 9.20, 10.76] |     0\n(10.76, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nQuantiles\n\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\nclassi\n\nQuantiles\n\n   Interval      Count\n----------------------\n[ 1.38,  2.24] |    29\n( 2.24,  2.50] |    28\n( 2.50,  2.76] |    28\n( 2.76,  3.02] |    28\n( 3.02,  3.35] |    28\n( 3.35,  3.76] |    28\n( 3.76, 12.32] |    29\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nFisher-Jenks\n\n\nclassi = mc.FisherJenks(db[\"no2_viz\"], k=7)\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nplot_classi(classi, \"no2_viz\", db)\n\n\n\n\n\nNow let’s dig into the internals of classi:\n\nclassi\n\nFisherJenks\n\n   Interval      Count\n----------------------\n[ 1.38,  2.06] |    20\n( 2.06,  2.69] |    58\n( 2.69,  3.30] |    62\n( 3.30,  4.19] |    42\n( 4.19,  5.64] |     7\n( 5.64,  9.19] |     4\n( 9.19, 12.32] |     5\n\n\n\nclassi.k\n\n7\n\n\n\nclassi.bins\n\narray([ 2.05617382,  2.6925931 ,  3.30281182,  4.19124954,  5.63804861,\n        9.19190206, 12.32332434])\n\n\n\nclassi.yb\n\narray([2, 3, 3, 1, 1, 2, 1, 1, 1, 0, 0, 3, 2, 1, 1, 1, 3, 1, 1, 1, 2, 0,\n       0, 4, 2, 1, 3, 1, 0, 0, 0, 1, 2, 2, 6, 5, 4, 2, 1, 3, 2, 3, 2, 1,\n       2, 3, 2, 3, 1, 1, 3, 1, 2, 3, 3, 1, 3, 3, 1, 0, 1, 1, 3, 2, 0, 0,\n       2, 1, 0, 0, 0, 2, 0, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 3, 1, 1, 1, 1,\n       2, 1, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1, 2, 3, 3, 2, 0, 3,\n       1, 0, 1, 2, 1, 1, 2, 1, 2, 6, 5, 6, 2, 2, 3, 6, 3, 4, 3, 4, 2, 3,\n       0, 2, 5, 6, 4, 5, 2, 2, 2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2,\n       1, 1, 3, 4, 2, 1, 3, 1, 2, 3, 4, 0, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2,\n       2, 2, 0, 0, 1, 2, 3, 3, 3, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1])\n\n\n\n\n2.3.3 How many colors?\nThe code used to generate the next figure uses more advanced features than planned for this course.\nIf you want to inspect it, look at the code cell below.\n\nvals = [3, 5, 7, 9, 12, 15]\nalgos = [\"equal_interval\", \"quantiles\", \"fisherjenks\"]\nf, axs = plt.subplots(\n    len(algos), len(vals), figsize=(3*len(vals), 3*len(algos))\n)\nfor i in range(len(algos)):\n    for j in range(len(vals)):\n        db.plot(\n            \"no2_viz\", scheme=algos[i], k=vals[j], ax=axs[i, j]\n        )\n        axs[i, j].set_axis_off()\n        if i==0:\n            axs[i, j].set_title(f\"k={vals[j]}\")\n        if j==0:\n            axs[i, j].text(\n                -0.1, \n                0.5, \n                algos[i], \n                horizontalalignment='center',\n                verticalalignment='center', \n                transform=axs[i, j].transAxes,\n                rotation=90\n            )\n\nplt.show()\n\n\n\n\n\n\n2.3.4 Using the right color\nFor a “safe” choice, make sure to visit ColorBrewer\n\n Categories, non-ordered\n Graduated, sequential\n Graduated, divergent\n\n\n\n2.3.5 Choropleths on Geo-Tables\n\n2.3.5.1 Streamlined\nHow can we create classifications from data on geo-tables? Two ways:\n\nDirectly within plot (only for some algorithms)\n\n\nfig, ax = plt.subplots()\ndb.plot(\n    \"no2_viz\", scheme=\"quantiles\", k=7, legend=True, ax=ax\n)\nplt.show()\n\n\n\n\nSee this tutorial for more details on fine tuning choropleths manually.\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Create an equal interval map with five bins for no2_viz .\n\n\n\n\n2.3.5.2 Manual approach\nThis is valid for any algorithm and provides much more flexibility at the cost of effort.\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\nfig, ax = plt.subplots()\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nplt.show()\n\n\n\n\n\n\n2.3.5.3 Value by alpha mapping\n\ndb['area_inv'] = 1 / db.to_crs(epsg=5726).area\n\n\nfig, ax = plt.subplots()\ndb.plot('area_inv', scheme='quantiles', ax=ax)\nax.set_title('area_inv')\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n# Set up figure and axis\nfig, ax = plt.subplots(1, figsize=(12, 9))\n# VBA choropleth\nvba_choropleth(\n    'no2_viz',          # Column for color \n    'area_inv',         # Column for transparency (alpha)\n    db,                 # Geo-table\n    rgb_mapclassify={   # Options for color classification\n        'classifier': 'quantiles', 'k':5\n    },\n    alpha_mapclassify={ # Options for alpha classification\n        'classifier': 'quantiles', 'k':5\n    },\n    legend=True,        # Add legend\n    ax=ax               # Axis\n)\n# Add boundary lines\ndb.plot(color='none', linewidth=0.05, ax=ax)\n\nplt.show()\n\n\n\n\nSee here for more examples of value-by-alpha (VBA) mapping.\n\n\n2.3.5.4 Legendgrams\nLegendgrams are a way to more closely connect the statistical characteristics of your data to the map display.\n\n\n\n\n\n\nWarning\n\n\n\nLegendgram is in an experimental development stage, so the code is a bit more involved and less stable. Use at your own risk!\n\n\nHere is an example:\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\nclassi = mc.Quantiles(db[\"no2_viz\"], k=7)\n\ndb.assign(\n    classes=classi.yb\n).plot(\"classes\", ax=ax)\n\nlegendgram(\n    fig,                   # Figure object\n    ax,                  # Axis object of the map\n    db[\"no2_viz\"],       # Values for the histogram\n    classi.bins,         # Bin boundaries\n    pal=palmpl.Viridis_7,# color palette (as palettable object)\n    legend_size=(.5,.2), # legend size in fractions of the axis\n    loc = 'lower right', # matplotlib-style legend locations\n)\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Give Task I and II from the GDS course a go.\n\n\n\n\n\n2.3.6 Choropleths on surfaces\nAssuming you have the file locally on the path ../data/:\n\ngrid = rioxarray.open_rasterio(\n  \"../data/cambodia_s5_no2.tif\"\n  ).sel(band=1)\n\n\ngrid_masked = grid.where(grid != grid.rio.nodata)\n\n\nImplicit continuous equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", ax=ax)\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", robust=True, ax=ax)\n\nplt.show()\n\n\n\n\n\nDiscrete equal interval\n\n\nfig, ax = plt.subplots()\n\ngrid.where(\n    grid != grid.rio.nodata\n).plot(cmap=\"viridis\", levels=7, ax=ax)\n\nplt.show()\n\n\n\n\n\nCombining with mapclassify\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.Quantiles(\n    grid_nona.to_series().dropna(), k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.FisherJenksSampled(\n    grid_nona.to_series().dropna().values, k=7\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"viridis\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.StdMean(\n    grid_nona.to_series().dropna().values\n)\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\ngrid_nona = grid.where(\n    grid != grid.rio.nodata\n)\n\nclassi = mc.BoxPlot(\n    grid_nona.to_series().dropna().values\n)\n\nfig, ax = plt.subplots()\n\ngrid_nona.plot(\n    cmap=\"coolwarm\", levels=classi.bins, ax=ax\n)\nplt.title(classi.name)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Read the satellite image for Madrid used in the Chapter 1 and create three choropleths, one for each band, using the colormapsReds, Greens, Blues.\nPlay with different classification algorithms.\n\nDo the results change notably?\nIf so, why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/geovisualisation.html#next-steps",
    "href": "quarto-notebooks/geovisualisation.html#next-steps",
    "title": "2  Geovisualisation",
    "section": "2.4 Next steps",
    "text": "2.4 Next steps\nIf you are interested in statistical maps based on classification, here are two recommendations to check out next:\n\nOn the technical side, the documentation for mapclassify (including its tutorials) provides more detail and illustrates more classification algorithms than those reviewed in this block.\nOn a more conceptual note, Cynthia Brewer’s “Designing better maps” (Brewer 2015) is an excellent blueprint for good map making.\n\n\n\n\n\nArribas-Bel, Dani. 2019. “A Course on Geographic Data Science.” The Journal of Open Source Education 2 (14). https://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS Users. ESRI press.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-feature-i.html#packages-and-modules",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.1 Packages and modules",
    "text": "3.1 Packages and modules\n\nimport pandas\nimport geopandas\nimport xarray, rioxarray\nimport contextily\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#data",
    "href": "quarto-notebooks/spatial-feature-i.html#data",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.2 Data",
    "text": "3.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\nregions = geopandas.read_file(\"../data/cambodia_regional.gpkg\")\ncities = geopandas.read_file(\"../data/cambodian_cities.geojson\")\npollution = rioxarray.open_rasterio(\n    \"../data/cambodia_s5_no2.tif\"\n).sel(band=1)\nfriction = rioxarray.open_rasterio(\n    \"../data/cambodia_2020_motorized_friction_surface.tif\"\n).sel(band=1)\n\nCheck both geo-tables and the surface are in the same CRS:\n\n(\n    regions.crs.to_epsg() ==\n    cities.crs.to_epsg() ==\n    pollution.rio.crs.to_epsg()\n)\n\nTrue"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#polygons-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#polygons-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.3 Polygons to points",
    "text": "3.3 Polygons to points\nIn which region is a city?\n\nsj = geopandas.sjoin(cities, regions)\n\n\n#   City name | Region name\nsj[[\"UC_NM_MN\", \"adm2_name\"]]\n\n\n\n\n\n\n\n\nUC_NM_MN\nadm2_name\n\n\n\n\n0\nSampov Lun\nSampov Lun\n\n\n1\nKhum Pech Chenda\nPhnum Proek\n\n\n2\nPoipet\nPaoy Paet\n\n\n3\nSisophon\nSerei Saophoan\n\n\n4\nBattambang\nBattambang\n\n\n5\nSiem Reap\nSiem Reap\n\n\n6\nSihanoukville\nPreah Sihanouk\n\n\n7\nN/A\nTrapeang Prasat\n\n\n8\nKampong Chhnang\nKampong Chhnang\n\n\n9\nPhnom Penh\nTuol Kouk\n\n\n10\nKampong Cham\nKampong Cham\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties and neighbourhoods datasets, can you determine the neighbourhood group of the first ten properties?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.4 Points to polygons",
    "text": "3.4 Points to polygons\nIf we were after the number of cities per region, it is a similar approach, with a (groupby) twist at the end.\n\nWe set_index to align both tables\nWe assign to create a new column\n\nIf you want no missing values, you can fillna(0) since you know missing data are zeros.\n\nregions.set_index(\n    \"adm2_name\"\n).assign(\n    city_count=sj.groupby(\"adm2_name\").size()\n).info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 198 entries, Mongkol Borei to Administrative unit not available\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   adm2_altnm  122 non-null    object  \n 1   motor_mean  198 non-null    float64 \n 2   walk_mean   198 non-null    float64 \n 3   no2_mean    198 non-null    float64 \n 4   geometry    198 non-null    geometry\n 5   city_count  11 non-null     float64 \ndtypes: float64(4), geometry(1), object(1)\nmemory usage: 18.9+ KB\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Using the Madrid AirBnb properties, can you compute how many properties each neighbourhood group has?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.5 Surface to points",
    "text": "3.5 Surface to points\nConsider attaching to each city in cities the pollution level, as expressed in pollution.\nThe code for generating the next figure is a bit more advanced as it fiddles with text, but if you want to explore it you can look at the code cell below.\n\nf, ax = plt.subplots(1, figsize=(9, 9))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    ax=ax, add_colorbar=False\n)\n\nfor i, row in cities.iterrows():\n    plt.text(\n        row.geometry.x,\n        row.geometry.y,\n        row[\"UC_NM_MN\"],\n        fontdict={\"color\": \"white\"},\n    )\n    \ncities.plot(ax=ax, color=\"r\")\n\nplt.show()\n\n\n\n\n\nfrom rasterstats import point_query\n\ncity_pollution = point_query(\n    cities,\n    pollution.values,\n    affine=pollution.rio.transform(),\n    nodata=pollution.rio.nodata\n)\ncity_pollution\n\n[3.9397064813333136e-05,\n 3.4949825609644426e-05,\n 3.825255125820345e-05,\n 4.103826573585785e-05,\n 3.067677208474005e-05,\n 5.108273256655399e-05,\n 2.2592785882580366e-05,\n 4.050414400882722e-05,\n 2.4383652926989897e-05,\n 0.0001285838935209779,\n 3.258245740282522e-05]\n\n\nAnd we can map these on the city locations:\n\nfig, ax = plt.subplots()\n\ncities.assign(\n    pollution=city_pollution\n).plot(\n    \"pollution\", \n    cmap=\"YlOrRd\",\n    legend=True,\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax,\n    crs=cities.crs,\n    source=contextily.providers.CartoDB.VoyagerOnlyLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you calculate the pollution level at the centroid of each Cambodian region in the regional aggregates dataset? how does it compare to their average value?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.6 Surface to polygons",
    "text": "3.6 Surface to polygons\nInstead of transferring to points, we want to aggregate all the information in a surface that falls within a polygon.\nFor this case, we will use the motorised friction surface. The question we are asking thus is: what is the average degree of friction of each region? Or, in other words: what regions are harder to get through with motorised transport?\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nfriction.plot.imshow(\n    add_colorbar=False, ax=ax\n)\nregions.plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\ncontextily.add_basemap(\n    ax, \n    crs=regions.crs,\n    source=contextily.providers.CartoDB.DarkMatterOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\nAgain, we can rely on rasterstats. The output is returned from zonal_stats as a list of dicts. To make it more manageable, we convert it into a pandas.DataFrame.\n\nfrom rasterstats import zonal_stats\n\nregional_friction = pandas.DataFrame(\n    zonal_stats(\n        regions,\n        friction.values,\n        affine=friction.rio.transform(),\n        nodata=friction.rio.nodata\n    ),\n    index=regions.index\n)\nregional_friction.head()\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.001200\n0.037000\n0.006494\n979\n\n\n1\n0.001200\n0.060000\n0.007094\n1317\n\n\n2\n0.001200\n0.024112\n0.006878\n324\n\n\n3\n0.001333\n0.060000\n0.009543\n758\n\n\n4\n0.001200\n0.060132\n0.008619\n55\n\n\n\n\n\n\n\nThis can then also be mapped onto the polygon geography:\n\nfig, ax = plt.subplots(1, figsize=(9, 9))\nregions.to_crs(\n    epsg=3857\n).join(\n    regional_friction\n).plot(\n    \"mean\", scheme=\"quantiles\", ax=ax\n)\ncontextily.add_basemap(\n    ax, \n    source=contextily.providers.CartoDB.VoyagerOnlyLabels,\n    zoom=7\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above to obtain the average friction for each region using the walking surface (cambodia_2020_walking_friction_surface.tif)."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#surface-to-surface",
    "href": "quarto-notebooks/spatial-feature-i.html#surface-to-surface",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.7 Surface to surface",
    "text": "3.7 Surface to surface\nIf we want to align the pollution surface with that of friction, we need to resample them to make them “fit on the same frame”.\n\npollution.shape\n\n(138, 152)\n\n\n\nfriction.shape\n\n(574, 636)\n\n\nThis involves either moving one surface to the frame of the other one, or both into an entirely new one. For the sake of the illustration, we will do the latter and select a frame that is 300 by 400 pixels. Note this involves stretching (upsampling) pollution, while compressing (downsampling) friction.\n\n# Define dimensions\ndimX, dimY = 300, 400\nminx, miny, maxx, maxy = pollution.rio.bounds()\n# Create XY indices\nys = np.linspace(miny, maxy, dimY)\nxs = np.linspace(minx, maxx, dimX)\n# Set up placeholder array\ncanvas = xarray.DataArray(\n    np.zeros((dimY, dimX)),\n    coords=[ys, xs],\n    dims=[\"y\", \"x\"]\n).rio.write_crs(4326) # Add CRS\n\n\ncvs_pollution = pollution.rio.reproject_match(canvas)\ncvs_friction = friction.rio.reproject_match(canvas)\n\n\ncvs_pollution.shape\n\n(400, 300)\n\n\n\ncvs_pollution.shape == cvs_friction.shape\n\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Trasfer the pollution surface to the frame of friction, and viceversa,\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe following methods involve modelling and are thus more sophisticated. Take these as a conceptual introduction with an empirical illustration, but keep in mind there are extense literatures on each of them and these cover some of the simplest cases."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-points",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-points",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.8 Points to points",
    "text": "3.8 Points to points\nSee this section of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) for more details on the technique.\nFor this exampe, we will assume that, instead of a surface with pollution values, we only have available a sample of points and we would like to obtain estimates for other locations.\nFor that we will first generate 100 random points within the extent of pollution which we will take as the location of our measurement stations.\nThe code in this cell contains bits that are a bit more advanced, do not despair if not everything makes sense!\n\nnp.random.seed(123456)\n\nbb = pollution.rio.bounds()\nstation_xs = np.random.uniform(bb[0], bb[2], 100)\nstation_ys = np.random.uniform(bb[1], bb[3], 100)\nstations = geopandas.GeoSeries(\n    geopandas.points_from_xy(station_xs, station_ys),\n    crs=\"EPSG:4326\"\n)\n\nOur station values come from the pollution surface, but we assume we do not have access to the latter, and we would like to obtain estimates for the location of the cities:\n\nfig, ax = plt.subplots(1, figsize=(6, 6))\n\npollution.where(\n    pollution&gt;0\n).plot(\n    add_colorbar=False, cmap=\"Blues\", ax=ax\n)\n\nstations.plot(ax=ax, color=\"red\", label=\"Stations\")\ncities.plot(ax=ax, color=\"lime\", label=\"Cities\")\n\nax.set_title(\"Pollution sampling\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\nWe will need the location and the pollution measurements for every station as separate arrays. Before we do that, since we will be calculating distances, we convert our coordinates to a system expressed in metres.\n\nstations_mt = stations.to_crs(epsg=5726)\nstation_xys = np.array(\n    [stations_mt.geometry.x, stations_mt.geometry.y]\n).T\n\nWe also need to extract the pollution measurements for each station location:\n\nstation_measurements = np.array(\n    point_query(\n        stations,\n        pollution.values,\n        affine=pollution.rio.transform(),\n        nodata=pollution.rio.nodata\n    )\n)\n\nAnd finally, we will also need the locations of each city expressed in the same coordinate system:\n\ncities_mt = cities.to_crs(epsg=5726)\ncity_xys = np.array(\n    [cities_mt.geometry.x, cities_mt.geometry.y]\n).T\n\nFor this illustration, we will use a \\(k\\)-nearest neighbors regression that estimates the value for each target point (cities in our case) as the average weighted by distance of its \\(k\\) nearest neigbours. In this illustration we will use \\(k=10\\).\nNote how sklearn relies only on array data structures, hence why we first had to express all the required information in that format.\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor(\n    n_neighbors=10, weights=\"distance\"\n).fit(station_xys, station_measurements)\n\nOnce we have trained the model, we can use it to obtain predictions for each city location:\n\npredictions = model.predict(city_xys)\n\nThese can be compared with the originally observed values:\n\np2p_comparison = pandas.DataFrame(\n    {\n        \"Observed\": city_pollution,\n        \"Predicted\": predictions\n    },\n    index=cities[\"UC_NM_MN\"]\n)\n\n\nfig, ax = plt.subplots(1)\np2p_comparison[\"Observed\"].plot.kde(ax=ax)\np2p_comparison[\"Predicted\"].plot.kde(ax=ax)\nax.set_axis_off()\nplt.legend(frameon=False, fontsize=20)\nplt.show()\n\n\n\n\n\np2p_comparison\n\n\n\n\n\n\n\n\nObserved\nPredicted\n\n\nUC_NM_MN\n\n\n\n\n\n\nSampov Lun\n0.000039\n0.000027\n\n\nKhum Pech Chenda\n0.000035\n0.000025\n\n\nPoipet\n0.000038\n0.000030\n\n\nSisophon\n0.000041\n0.000030\n\n\nBattambang\n0.000031\n0.000027\n\n\nSiem Reap\n0.000051\n0.000027\n\n\nSihanoukville\n0.000023\n0.000019\n\n\nN/A\n0.000041\n0.000028\n\n\nKampong Chhnang\n0.000024\n0.000032\n\n\nPhnom Penh\n0.000129\n0.000042\n\n\nKampong Cham\n0.000033\n0.000033\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analysis above with \\(k=15\\) and \\(k=5\\). Do results change? Why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#points-to-surface",
    "href": "quarto-notebooks/spatial-feature-i.html#points-to-surface",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.9 Points to surface",
    "text": "3.9 Points to surface\nImagine we do not have a surface like pollution but we need it. In this context, if you have measurements from some locations, such as in stations, we can use the approach reviewed above to generate a surface. The trick to do this is to realise that we can generate a uniform grid of target locations that we can then express as a surface.\nWe will set as our target locations those of the pixels in the target surface we have seen above:\n\ncanvas_mt = canvas.rio.reproject(5726)\n\n\nxy_pairs = canvas_mt.to_series().index\nxys = np.array(\n    [\n        xy_pairs.get_level_values(\"x\"),\n        xy_pairs.get_level_values(\"y\")\n    ]\n).T\n\nTo obtain pollution estimates at each location, we can predict with model:\n\npredictions_grid = model.predict(xys)\n\nAnd with these at hand, we can convert them into a surface:\n\npredictions_series = pandas.DataFrame(\n    {\"predictions_grid\": predictions_grid}\n).join(\n    pandas.DataFrame(xys, columns=[\"x\", \"y\"])\n).set_index([\"y\", \"x\"])\n\npredictions_surface = xarray.DataArray().from_series(\n    predictions_series[\"predictions_grid\"]\n).rio.write_crs(canvas_mt.rio.crs)\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 6))\n\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot(ax=axs[0])\naxs[0].set_title(\"Observed\")\n\npredictions_surface.where(\n    predictions_surface&gt;0\n).rio.reproject_match(\n    cvs_pollution\n).plot(ax=axs[1])\naxs[1].set_title(\"Predicted\")\n\nplt.show()\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(9, 4))\ncvs_pollution.where(\n    cvs_pollution&gt;0\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, label=\"Observed\"\n)\npredictions_surface.rio.reproject_match(\n    cvs_pollution\n).plot.hist(\n    bins=100, alpha=0.5, ax=ax, color=\"g\", label=\"predicted\"\n)\nplt.legend()\nplt.show()\n\n\n\n\nRoom for improvement but, remember this was a rough first pass!\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Train a model with pollution measurements from each city location and generate a surface from it. How does the output compare to the one above? Why do you think that is?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#polygons-to-polygons",
    "href": "quarto-notebooks/spatial-feature-i.html#polygons-to-polygons",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.10 Polygons to polygons",
    "text": "3.10 Polygons to polygons\nIn this final example, we transfer data from a polygon geography to another polygon geography. Effectively, we re-apportion values from one set of areas to another based on the extent of shared area.\nOur illustration will cover how to move pollution estimates from regions into a uniform hexagonal grid we will first create.\n\nimport tobler\n\nhex_grid = tobler.util.h3fy(\n    regions, resolution=5\n)\n\nNot that pollution is expressed as an intesive (rate) variable. We need to recognise this when specifying the interpolation model:\n\npollution_hex = tobler.area_weighted.area_interpolate(\n    regions.assign(geometry=regions.buffer(0)).to_crs(epsg=5726),\n    hex_grid.to_crs(epsg=5726), \n    intensive_variables=[\"no2_mean\"]\n)\n\nAnd the results look like:\n\nf, axs = plt.subplots(1, 3, figsize=(12, 4))\n\nregions.plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[0]\n)\naxs[0].set_axis_off()\n\nhex_grid.plot(\n    facecolor=\"none\", edgecolor=\"red\", ax=axs[1]\n)\naxs[1].set_axis_off()\n\npollution_hex.to_crs(epsg=4326).plot(\n    \"no2_mean\", scheme=\"quantiles\", k=12, ax=axs[2]\n)\naxs[2].set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Replicate the analytis using resolution = 4. How is the result different? Why?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-i.html#next-steps",
    "href": "quarto-notebooks/spatial-feature-i.html#next-steps",
    "title": "3  Spatial feature engineering (part I)",
    "section": "3.11 Next steps",
    "text": "3.11 Next steps\nIf you are interested in learning more about spatial feature engineering through map matching, the following pointers might be useful to delve deeper into specific types of “data transfer”:\n\nThe datashader library is a great option to transfer geo-tables into surfaces, providing tooling to perform these operations in a highly efficient and performant way.\nWhen aggregating surfaces into geo-tables, the library rasterstats contains most if not all of the machinery you will need.\nFor transfers from polygon to polygon geographies, tobler is your friend. Its official documentation contains examples for different use cases.\n\n\n\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#packages-and-modules",
    "href": "quarto-notebooks/spatial-feature-ii.html#packages-and-modules",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.1 Packages and modules",
    "text": "4.1 Packages and modules\n\nimport pandas, geopandas\nimport numpy as np\nimport contextily\nimport tobler\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#data",
    "href": "quarto-notebooks/spatial-feature-ii.html#data",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.2 Data",
    "text": "4.2 Data\nIf you want to read more about the data sources behind this dataset, head to the Datasets section.\nAssuming you have the file locally on the path ../data/:\n\npts = geopandas.read_file(\"../data/madrid_abb.gpkg\")\n\nWe will be working with a modified version of pts:\n\nSince we will require distance calculations, we will switch to the Spanish official projection\nTo make calculations in the illustration near-instantaneous, we will work with a smaller (random) sample of Airbnb properties (10% of the total)\n\n\ndb = pts.sample(\n    frac=0.1, random_state=123\n).to_crs(epsg=25830)\n\nAs you can see in the description, the new CRS is expressed in metres:\n\ndb.crs\n\n&lt;Projected CRS: EPSG:25830&gt;\nName: ETRS89 / UTM zone 30N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Europe between 6°W and 0°W: Faroe Islands offshore; Ireland - offshore; Jan Mayen - offshore; Norway including Svalbard - offshore; Spain - onshore and offshore.\n- bounds: (-6.0, 35.26, 0.01, 80.49)\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#distance-buffers",
    "href": "quarto-notebooks/spatial-feature-ii.html#distance-buffers",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.3 Distance buffers",
    "text": "4.3 Distance buffers\nHow many Airbnb’s are within 500m of each Airbnb?\n\nfrom pysal.lib import weights\n\nUsing DistanceBand, we can build a spatial weights matrix that assigns 1 to each observation within 500m, and 0 otherwise.\n\nw500m = weights.DistanceBand.from_dataframe(\n    db, threshold=500, binary=True\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/util.py:826: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 86 disconnected components.\n There are 47 islands with ids: 6878, 16772, 15006, 1336, 3168, 15193, 1043, 5257, 4943, 12849, 10609, 11309, 10854, 10123, 3388, 9380, 10288, 13071, 3523, 15316, 3856, 205, 7720, 10454, 18307, 3611, 12405, 10716, 14813, 15467, 1878, 16597, 14329, 7933, 16215, 13525, 13722, 11932, 14456, 8848, 15197, 8277, 9922, 13072, 13852, 5922, 17151.\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/distance.py:844: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 86 disconnected components.\n There are 47 islands with ids: 6878, 16772, 15006, 1336, 3168, 15193, 1043, 5257, 4943, 12849, 10609, 11309, 10854, 10123, 3388, 9380, 10288, 13071, 3523, 15316, 3856, 205, 7720, 10454, 18307, 3611, 12405, 10716, 14813, 15467, 1878, 16597, 14329, 7933, 16215, 13525, 13722, 11932, 14456, 8848, 15197, 8277, 9922, 13072, 13852, 5922, 17151.\n\n\n\nThe number of neighbors can be accessed through the cardinalities attribute:\n\nn_neis = pandas.Series(w500m.cardinalities)\nn_neis.head()\n\n11297    213\n2659       5\n16242     21\n15565      9\n14707    159\ndtype: int64\n\n\n\nfig, ax = plt.subplots()\n\ndb.assign(\n    n_neis=n_neis\n).plot(\"n_neis\", markersize=0.1, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate the number of AirBnb properties within 250m of each other property. What is the average?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#distance-rings",
    "href": "quarto-notebooks/spatial-feature-ii.html#distance-rings",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.4 Distance rings",
    "text": "4.4 Distance rings\nHow many Airbnb’s are between 500m and 1km of each Airbnb?\n\nw1km = weights.DistanceBand.from_dataframe(\n    db, threshold=1000, binary=True\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/util.py:826: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 20 disconnected components.\n There are 5 islands with ids: 4943, 12849, 15467, 13525, 11932.\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/distance.py:844: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 20 disconnected components.\n There are 5 islands with ids: 4943, 12849, 15467, 13525, 11932.\n\n\n\nNow, we could do simply a subtraction:\n\nn_ring_neis = pandas.Series(w1km.cardinalities) - n_neis\n\nOr, if we need to know which is which, we can use set operations on weights:\n\nw_ring = weights.w_difference(w1km, w500m, constrained=False)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/libpysal/weights/set_operations.py:242: UserWarning:\n\nThe weights matrix is not fully connected: \n There are 34 disconnected components.\n There are 23 islands with ids: 3744, 4143, 4857, 4943, 6986, 8345, 8399, 9062, 10592, 10865, 11574, 11613, 11785, 11840, 11932, 12015, 12635, 12714, 12849, 13091, 13317, 13525, 15467.\n\n\n\nAnd we can confirm they’re both the same:\n\n(pandas.Series(w_ring.cardinalities) - n_ring_neis).sum()\n\n0\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Can you create a plot with the following two lines?\n\nOne depicting the average number of properties within a range of 50m, 100m, 250m, 500m, 750m\nAnother one with the increase of average neighbors for the same distances above"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-points",
    "href": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-points",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.5 Cluster membership (points)",
    "text": "4.5 Cluster membership (points)\nWe can use the spatial configuration of observations to classify them as part of clusters or not, which can then be encoded, for example, as dummy variables in a model.\nThese magic numbers need to be pre-set and you can play with both min_pct (or min_pts directly) and eps to see how they affect the results (spoiler: a lot!).\n\nfrom sklearn.cluster import DBSCAN\n\nmin_pct = 2\nmin_pts = len(db) * min_pct // 100\neps = 500\n\nWe will illustrate it with a minimum number of points of min_pct % of the sample and a maximum radious of eps metres.\n\nmodel = DBSCAN(min_samples=min_pts, eps=eps)\nmodel.fit(\n    db.assign(\n        x=db.geometry.x\n    ).assign(\n        y=db.geometry.y\n    )[['x', 'y']]\n);\n\nWe will attach the labels to db for easy access:\n\ndb[\"labels\"] = model.labels_\n\nWe can define boundaries to turn point clusters into polygons if that fits our needs better:\nThe code in the next cell is a bit more advanced than expected for this course, but is used here as an illustration.\n\nfrom pysal.lib import cg\n\nboundaries = []\ncl_ids = [i for i in db[\"labels\"].unique() if i!=-1]\nfor cl_id in cl_ids:\n    sub = db.query(f\"labels == {cl_id}\")\n    cluster_boundaries = cg.alpha_shape_auto(\n        np.array(\n            [sub.geometry.x, sub.geometry.y]\n        ).T,\n    )\n    boundaries.append(cluster_boundaries)\nboundaries = geopandas.GeoSeries(\n    boundaries, index=cl_ids, crs=db.crs\n)\n\nAnd we can see what the clusters look like:\n\nfig, ax = plt.subplots()\n\ndb.to_crs(\n    epsg=3857\n).plot(\n    markersize=0.1, color=\"lime\", ax=ax\n)\nboundaries.to_crs(\n    epsg=3857\n).plot(\n    ax=ax, edgecolor=\"red\", facecolor=\"none\"\n)\n\ncontextily.add_basemap(\n    ax,\n    source=contextily.providers.CartoDB.DarkMatterNoLabels\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How does the map above change if you require 5% of points instead of 2% for a candidate cluster to be considered so?"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-polygons",
    "href": "quarto-notebooks/spatial-feature-ii.html#cluster-membership-polygons",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.6 Cluster membership (polygons)",
    "text": "4.6 Cluster membership (polygons)\nWe can take a similar approach as above if we have polygon geographies instead of points. Rather than using DBSCAN, here we can rely on local indicators of spatial association (LISAs) to pick up spatial concentrations of high or low values.\nFor the illustration, we will aggregate the location of Airbnb properties to a regular hexagonal grid, similar to how we generated it when transferring from polygons to polygons. First we create a polygon covering the extent of points:\n\none = geopandas.GeoSeries(\n    [cg.alpha_shape_auto(\n        np.array(\n            [db.geometry.x, db.geometry.y]\n        ).T,\n    )],\n    crs=db.crs\n)\n\nThen we can tessellate:\n\nabb_hex = tobler.util.h3fy(\n    one, resolution=8\n)\n\n/Users/carmen/anaconda3/envs/gds-python/lib/python3.11/site-packages/pyproj/crs/crs.py:1293: UserWarning:\n\nYou will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n\n\n\nAnd obtain a count of points in each polygon:\n\ncounts = geopandas.sjoin(\n    db, abb_hex\n).groupby(\n    \"index_right\"\n).size()\n\nabb_hex[\"count\"] = counts\nabb_hex[\"count\"] = abb_hex[\"count\"].fillna(0)\n\nfig, ax = plt.subplots()\n\nabb_hex.plot(\"count\", scheme=\"fisherjenks\", ax=ax)\n\nplt.show()\n\n\n\n\nTo identify spatial clusters, we rely on esda:\n\nfrom pysal.explore import esda\n\nAnd compute the LISA statistics:\n\nw = weights.Queen.from_dataframe(abb_hex)\nlisa = esda.Moran_Local(abb_hex[\"count\"], w)\n\n/var/folders/_n/krcxvsq92k7bdd1nfpk3_9c00000gn/T/ipykernel_25959/2473509840.py:1: FutureWarning:\n\n`use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n\n\n\nFor a visual inspection of the clusters, splot:\n\nfrom pysal.viz import splot\nfrom splot.esda import lisa_cluster\n\n\nlisa_cluster(lisa, abb_hex, p=0.01)\nplt.show()\n\n\n\n\nAnd, if we want to extract the labels for each polygon, we can do so from the lisa object:\n\nlisa.q * (lisa.p_sim &lt; 0.01)\n\narray([3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n       3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3,\n       0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n       0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 0, 0,\n       0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n       0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3])"
  },
  {
    "objectID": "quarto-notebooks/spatial-feature-ii.html#next-steps",
    "href": "quarto-notebooks/spatial-feature-ii.html#next-steps",
    "title": "4  Spatial feature engineering (part II)",
    "section": "4.7 Next steps",
    "text": "4.7 Next steps\nIf you want a bit more background into some of the techniques reviewed in this block, the following might be of interest:\n\nThis block of the GDS Course (Pietrostefani and Cabrera-Arnau 2024) will introduce you to more techniques like the LISAs seen above to explore the spatial dimension of the statistical properties of your data. If you want a more detailed read, this Chapter of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) will do just that.\nThis block of the GDS Course (Pietrostefani and Cabrera-Arnau 2024) will introduce you to more techniques for exploring point patterns. If you want a more comprehensive read, this Chapter of the GDS Book (Rey, Arribas-Bel, and Wolf forthcoming) will do just that.\n\n\n\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024. “A Course in Geographic Data Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming. Geographic Data Science with PySAL and the PyData Stack. CRC press."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#packages-and-modules",
    "href": "quarto-notebooks/openstreetmap.html#packages-and-modules",
    "title": "5  OpenStreetMap",
    "section": "5.1 Packages and modules",
    "text": "5.1 Packages and modules\n\nimport geopandas\nimport contextily\nimport matplotlib.pyplot as plt\nfrom IPython.display import GeoJSON"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#data",
    "href": "quarto-notebooks/openstreetmap.html#data",
    "title": "5  OpenStreetMap",
    "section": "5.2 Data",
    "text": "5.2 Data\nSince some of the query options we will discuss involve pre-defined extents, we will read the Madrid neighbourhoods dataset first.\nAssuming you have the file locally on the path ../data/:\n\nneis = geopandas.read_file(\"../data/neighbourhoods.geojson\")\n\nTo make some of the examples below computationally easier on OpenStreetMap servers, we will single out the smallest neighborhood:\n\nareas = neis.to_crs(\n    epsg=32630\n).area\n\nsmallest = neis[areas == areas.min()]\nsmallest\n\n\n\n\n\n\n\n\nneighbourhood\nneighbourhood_group\ngeometry\n\n\n\n\n98\nAtalaya\nCiudad Lineal\nMULTIPOLYGON (((-3.66195 40.46338, -3.66364 40...\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nsmallest.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#osmnx",
    "href": "quarto-notebooks/openstreetmap.html#osmnx",
    "title": "5  OpenStreetMap",
    "section": "5.3 osmnx",
    "text": "5.3 osmnx\nLet’s import one more package, osmnx, designed to easily download, model, analyse, and visualise street networks and other geospatial features from OpenStreetMap.\n\nimport osmnx as ox\n\nHere is a trick (courtesy of Martin Fleischmann to pin all your queries to OpenStreetMap to a specific date, so results are always reproducible, even if the map changes in the meantime.\n\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of the methods covered here rely on the osmnx.features module. Check out its reference here.\n\n\nThere are two broad areas to keep in mind when querying data on OpenStreetMap through osmnx:\n\nThe interface to specify the extent of the search.\nThe nature of the entities being queried. Here, the interface relies entirely on OpenStreetMap’s tagging system. Given the distributed nature of the project, this is variable, but a good place to start is:\n\n\nhttps://wiki.openstreetmap.org/wiki/Tags\n\nGenerally, the interface we will follow involves the following:\nreceived_entities = ox.features_from_XXX(\n    &lt;extent&gt;, tags={&lt;key&gt;: True/&lt;value(s)&gt;}, ...\n)\nThe &lt;extent&gt; can take several forms. We can print out the available forms:\n\n[i for i in dir(ox) if \"features_from_\" in i]\n\n['features_from_address',\n 'features_from_bbox',\n 'features_from_place',\n 'features_from_point',\n 'features_from_polygon',\n 'features_from_xml']\n\n\nThe tags follow the official feature spec."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#buildings",
    "href": "quarto-notebooks/openstreetmap.html#buildings",
    "title": "5  OpenStreetMap",
    "section": "5.4 Buildings",
    "text": "5.4 Buildings\n\nblgs = ox.features_from_polygon(\n    smallest.squeeze().geometry, tags={\"building\": True}\n)\n\n\nfig, ax = plt.subplots()\n\nblgs.plot(ax=ax)\n\nplt.show()\n\n\n\n\n\nblgs.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nMultiIndex: 115 entries, ('way', 442595762) to ('way', 577690922)\nData columns (total 27 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   name              2 non-null      object  \n 1   amenity           2 non-null      object  \n 2   geometry          115 non-null    geometry\n 3   nodes             115 non-null    object  \n 4   building          115 non-null    object  \n 5   addr:housenumber  21 non-null     object  \n 6   addr:postcode     3 non-null      object  \n 7   addr:street       9 non-null      object  \n 8   denomination      1 non-null      object  \n 9   phone             2 non-null      object  \n 10  religion          1 non-null      object  \n 11  source            1 non-null      object  \n 12  source:date       1 non-null      object  \n 13  url               1 non-null      object  \n 14  wheelchair        1 non-null      object  \n 15  building:levels   11 non-null     object  \n 16  addr:city         8 non-null      object  \n 17  addr:country      6 non-null      object  \n 18  wikidata          1 non-null      object  \n 19  website           1 non-null      object  \n 20  country           1 non-null      object  \n 21  diplomatic        1 non-null      object  \n 22  name:en           1 non-null      object  \n 23  name:fr           1 non-null      object  \n 24  name:ko           1 non-null      object  \n 25  office            1 non-null      object  \n 26  target            1 non-null      object  \ndtypes: geometry(1), object(26)\nmemory usage: 29.7+ KB\n\n\n\nblgs.head()\n\n\n\n\n\n\n\n\n\nname\namenity\ngeometry\nnodes\nbuilding\naddr:housenumber\naddr:postcode\naddr:street\ndenomination\nphone\n...\naddr:country\nwikidata\nwebsite\ncountry\ndiplomatic\nname:en\nname:fr\nname:ko\noffice\ntarget\n\n\nelement_type\nosmid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nway\n442595762\nNaN\nNaN\nPOLYGON ((-3.66377 40.46317, -3.66363 40.46322...\n[4402722774, 4402722775, 4402722776, 440272277...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595763\nNaN\nNaN\nPOLYGON ((-3.66394 40.46346, -3.66415 40.46339...\n[4402722778, 4402722779, 4402722780, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595764\nNaN\nNaN\nPOLYGON ((-3.66379 40.46321, -3.66401 40.46314...\n[4402722782, 4402722783, 4402722784, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442595765\nNaN\nNaN\nPOLYGON ((-3.66351 40.46356, -3.66294 40.46371...\n[4402722786, 4402722787, 4402722788, 440272278...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n442596830\nNaN\nNaN\nPOLYGON ((-3.66293 40.46289, -3.66281 40.46294...\n[4402729658, 4402729659, 4402729660, 440272966...\nyes\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 27 columns\n\n\n\nIf you want to visit the entity online, you can do so at:\n\nhttps://www.openstreetmap.org/&lt;unique_id&gt;\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Extract the building footprints for the Sol neighbourhood in neis."
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#other-polygons",
    "href": "quarto-notebooks/openstreetmap.html#other-polygons",
    "title": "5  OpenStreetMap",
    "section": "5.5 Other polygons",
    "text": "5.5 Other polygons\n\npark = ox.features_from_place(\n    \"Parque El Retiro, Madrid\", tags={\"leisure\": \"park\"}\n)\n\n\nfig, ax = plt.subplots()\n\npark.plot(\n    facecolor=\"none\", edgecolor=\"blue\", linewidth=2, ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=smallest.crs, \n    source=contextily.providers.OpenStreetMap.Mapnik\n)\n\nplt.show()"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#points-of-interest",
    "href": "quarto-notebooks/openstreetmap.html#points-of-interest",
    "title": "5  OpenStreetMap",
    "section": "5.6 Points of interest",
    "text": "5.6 Points of interest\nBars around Atocha station:\n\nbars = ox.features_from_address(\n    \"Puerta de Atocha, Madrid\", tags={\"amenity\": \"bar\"}, dist=1500\n)\n\nWe can quickly explore with GeoJSON:\n\nbars.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd stores within Malasaña:\n\nshops = ox.features_from_address(\n    \"Malasaña, Madrid, Spain\", # Boundary to search within\n    tags={\n        \"shop\": True,\n        \"landuse\": [\"retail\", \"commercial\"],\n        \"building\": \"retail\"\n    },\n    dist=1000\n)\n\nshops.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe use features_from_place for delineated areas (“polygonal entities”):\n\ncs = ox.features_from_place(\n    \"Madrid, Spain\",\n    tags={\"amenity\": \"charging_station\"}\n)\n\ncs.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nSimilarly, we can work with location data. For example, searches around a given point:\n\nbakeries = ox.features_from_point(\n    (40.418881103417675, -3.6920446157455444),\n    tags={\"shop\": \"bakery\", \"craft\": \"bakery\"},\n    dist=500\n)\n\nbakeries.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n\nHow many music shops does OSM record within 750 metres of Puerta de Alcalá?\n\n- Are there more restaurants or clothing shops within the polygon that represents the Pacífico neighbourhood in neis table?"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#streets",
    "href": "quarto-notebooks/openstreetmap.html#streets",
    "title": "5  OpenStreetMap",
    "section": "5.7 Streets",
    "text": "5.7 Streets\nStreet data can be obtained as another type of entity, as above; or as a graph object.\n\n5.7.1 Geo-tables\n\ncentro = ox.features_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n    tags={\"highway\": True}\n)\n\nWe can get a quick peak into what is returned (in grey), compared to the region we used for the query:\n\nfig, ax = plt.subplots()\n\nneis.query(\n    \"neighbourhood == 'Sol'\"\n).plot(color=\"k\", ax=ax)\n\ncentro.plot(\n    ax=ax, \n    color=\"0.5\", \n    linewidth=0.2, \n    markersize=0.5\n)\n\nplt.show()\n\n\n\n\nThis however will return all sorts of things:\n\ncentro.geometry\n\nelement_type  osmid    \nnode          21734214                             POINT (-3.70427 40.41662)\n              21734250                             POINT (-3.70802 40.41612)\n              21734252                             POINT (-3.70847 40.41677)\n              21968134                             POINT (-3.69945 40.41786)\n              21968197                             POINT (-3.70054 40.41645)\n                                                 ...                        \nway           907553665    LINESTRING (-3.70686 40.41380, -3.70719 40.41369)\n              909056211    LINESTRING (-3.70705 40.42021, -3.70680 40.42020)\nrelation      5662178      POLYGON ((-3.70948 40.41551, -3.70952 40.41563...\n              7424032      POLYGON ((-3.70243 40.41716, -3.70242 40.41714...\n              8765884      POLYGON ((-3.70636 40.41475, -3.70635 40.41481...\nName: geometry, Length: 609, dtype: geometry\n\n\n\n\n5.7.2 Spatial graphs\nThe graph_from_XXX() functions return clean, processed graph objects for the street network. Available options are:\n\n[i for i in dir(ox) if \"graph_from_\" in i]\n\n['graph_from_address',\n 'graph_from_bbox',\n 'graph_from_gdfs',\n 'graph_from_place',\n 'graph_from_point',\n 'graph_from_polygon',\n 'graph_from_xml']\n\n\nHere is an example:\n\ncentro_gr = ox.graph_from_polygon(\n    neis.query(\"neighbourhood == 'Sol'\").squeeze().geometry,\n)\n\nThis is indeed a graph object (as defined by the networkx package):\n\ncentro_gr\n\n&lt;networkx.classes.multidigraph.MultiDiGraph at 0x16b6806d0&gt;\n\n\nTo visualise it, there are several plotting options:\n\n[i for i in dir(ox) if \"plot_graph\" in i]\n\n['plot_graph', 'plot_graph_folium', 'plot_graph_route', 'plot_graph_routes']\n\n\nFor example:\n\nox.plot_figure_ground(centro_gr)\nplt.show()\n\n\n\n\n\nox.graph_to_gdfs(centro_gr, nodes=False).explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: How many bookshops are within a 50m radious of the Paseo de la Castellana?\nThis one involves the following steps:\n\nExtracting the street segment for Paseo de la Castellana\nDrawing a 50m buffer around it\nQuerying OSM for bookshops"
  },
  {
    "objectID": "quarto-notebooks/openstreetmap.html#next-steps",
    "href": "quarto-notebooks/openstreetmap.html#next-steps",
    "title": "5  OpenStreetMap",
    "section": "5.8 Next steps",
    "text": "5.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nParts of the block are inspired and informed by Geoff Boeing’s excellent course on Urban Data Science\nMore in depth content about osmnx is available in the official examples collection\nBoeing (2020) {cite}boeing2020exploring illustrates how OpenStreetMap can be used to analyse urban form (Open Access)\n\n\n\n\n\nAnderson, Jennings, Dipto Sarkar, and Leysia Palen. 2019. “Corporate Editors in the Evolving Landscape of OpenStreetMap.” ISPRS International Journal of Geo-Information 8 (5): 232."
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#packages-and-modules",
    "href": "quarto-notebooks/transport-costs.html#packages-and-modules",
    "title": "6  Transport costs",
    "section": "6.1 Packages and modules",
    "text": "6.1 Packages and modules\n\nimport momepy\nimport geopandas\nimport contextily\nimport xarray, rioxarray\nimport osmnx as ox\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nox.settings.overpass_settings = (\n    '[out:json][timeout:90][date:\"2021-03-07T00:00:00Z\"]'\n)"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#data",
    "href": "quarto-notebooks/transport-costs.html#data",
    "title": "6  Transport costs",
    "section": "6.2 Data",
    "text": "6.2 Data\nAssuming you have the file locally on the path ../data/:\n\nstreets = geopandas.read_file(\"../data/arturo_streets.gpkg\")\nabbs = geopandas.read_file(\"../data/madrid_abb.gpkg\")\nneis = geopandas.read_file(\"../data/neighbourhoods.geojson\")"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#pandana-graphs",
    "href": "quarto-notebooks/transport-costs.html#pandana-graphs",
    "title": "6  Transport costs",
    "section": "6.3 pandana graphs",
    "text": "6.3 pandana graphs\n\nimport pandana\n\nBefore building the routing network, we convert to graph and back in momepy to “clean” the network and ensure it complies with requirements for routing.\n\nnodes, edges = momepy.nx_to_gdf( # Convert back to geo-table\n    momepy.gdf_to_nx(            # Convert to a clean NX graph\n        streets.explode(index_parts='True')        # We \"explode\" to avoid multi-part rows\n    )\n)\nnodes = nodes.set_index(\"nodeID\") # Reindex nodes on ID\n\nOnce we have nodes and edges “clean” from the graph representation, we can build a pandana.Network object we will use for routing:\n\nstreets_pdn = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"mm_len\"]]\n)\n\nstreets_pdn\n\nGenerating contraction hierarchies with 8 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\n&lt;pandana.network.Network at 0x16584a850&gt;"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#shortest-path-routing",
    "href": "quarto-notebooks/transport-costs.html#shortest-path-routing",
    "title": "6  Transport costs",
    "section": "6.4 Shortest-path routing",
    "text": "6.4 Shortest-path routing\nHow do I go from A to B?\nFor example, from the first Airbnb in the geo-table…\n\nfirst = abbs.loc[[0], :].to_crs(streets.crs)\n\n…to Puerta del Sol.\n\nimport geopy\ngeopy.geocoders.options.default_user_agent = \"gds4eco\"\nsol = geopandas.tools.geocode(\n    \"Puerta del Sol, Madrid\", geopy.Nominatim\n).to_crs(streets.crs)\nsol\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (440284.049 4474264.421)\nPuerta del Sol, Barrio de los Austrias, Sol, C...\n\n\n\n\n\n\n\nFirst we snap locations to the network:\n\npt_nodes = streets_pdn.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\npt_nodes\n\n0     3071\n1    35729\nName: node_id, dtype: int64\n\n\nThen we can route the shortest path:\n\nroute_nodes = streets_pdn.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\nroute_nodes\n\narray([ 3071,  3476,  8268,  8266,  8267, 18695, 18693,  1432,  1430,\n         353,  8175,  8176, 18121, 17476, 16858, 14322, 16857, 17810,\n       44795, 41220, 41217, 41221, 41652, 18924, 18928, 48943, 18931,\n       21094, 21095, 23219, 15398, 15399, 15400, 47446, 47447, 23276,\n       47448, 23259, 23260, 23261, 27951, 27952, 27953, 48327, 11950,\n       11949, 11944, 19475, 19476, 27333, 30088, 43294, 11940, 11941,\n       11942, 48325, 37484, 48316, 15893, 15890, 15891, 29954, 25453,\n        7341, 34991, 23608, 28217, 21648, 21649, 21651, 39075, 25108,\n       25102, 25101, 25100, 48518, 47287, 34623, 31187, 29615, 48556,\n       22844, 48553, 48555, 40922, 40921, 40923, 48585, 46372, 46371,\n       46370, 45675, 45676, 38778, 38777, 19144, 20498, 20497, 20499,\n       47737, 42303, 42302, 35730, 35727, 35729])\n\n\nWith this information, we can build the route line manually.\nThe code to generate the route involves writing a function and is a bit more advanced than expected for this course. If this looks too complicated, do not despair.\n\nfrom shapely.geometry import LineString\n\ndef route_nodes_to_line(nodes, network):\n    pts = network.nodes_df.loc[nodes, :]\n    s = geopandas.GeoDataFrame(\n        {\"src_node\": [nodes[0]], \"tgt_node\": [nodes[1]]},\n        geometry=[LineString(pts.values)],\n        crs=streets.crs\n    )\n    return s\n\nWe can calculate the route:\n\nroute = route_nodes_to_line(route_nodes, streets_pdn)\n\nAnd we get it back as a geo-table (with one row):\n\nroute\n\n\n\n\n\n\n\n\nsrc_node\ntgt_node\ngeometry\n\n\n\n\n0\n3071\n3476\nLINESTRING (442606.507 4478714.516, 442597.100...\n\n\n\n\n\n\n\nPlease note this builds a simplified line for the route, not one that is based on the original geometries.\n\nfig, ax = plt.subplots()\n\nroute.plot(\n    figsize=(9, 9),\n    color=\"red\",\n    ax=ax\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=route.crs,\n    source=contextily.providers.CartoDB.Voyager,\n    zoom=14\n)\n\nplt.show()\n\n\n\n\nBut distance calculations are based on the original network). If we wanted to obtain the length of the route:\n\nroute_len = streets_pdn.shortest_path_length(\n    pt_nodes[0], pt_nodes[1]\n)\nround(route_len / 1000, 3) # Dist in Km\n\n5.458\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n- What is the network distance between CEMFI and Puerta del Sol?\n- BONUS I: how much longer is it than if you could fly in a straight line?\n- BONUS II: if one walks at a speed of 5 Km/h, how long does the walk take you?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#weighted-routing",
    "href": "quarto-notebooks/transport-costs.html#weighted-routing",
    "title": "6  Transport costs",
    "section": "6.5 Weighted routing",
    "text": "6.5 Weighted routing\nHow do I go from A to B passing by the “best” buildings?\nThis is really an extension of standard routing that takes advantage of the flexibility of pandana.Network objects.\nNote that the route we defined above, does not pass by the “best” buildings.\n\nbb = route.total_bounds\n\nfig, ax = plt.subplots()\n\nstreets.cx[\n    bb[0]: bb[2], bb[1]:bb[3]\n].plot(\n    \"average_quality\", scheme=\"quantiles\", ax=ax\n)\n\nroute.plot(color=\"r\", linewidth=2.5, ax=ax)\n\nax.set_title(\"Mean Building Quality\")\nax.set_axis_off()\n\nplt.show()\n\n\n\n\nThe overall process to achieve this is the very similar; the main difference is, when we build the Network object, to replace distance (mm_len) with a measure that combines distance and building quality. Note that we want to maximise building quality, but the routing algorithms use a minimisation function. Hence, our composite index will need to reflect that.\nThe strategy is divided in the following steps:\n\nRe-scale distance between 0 and 1\nBuild a measure inverse to building quality in the \\([0, 1]\\) range\nGenerate a combined measure (wdist) by picking a weighting parameter\nBuild a new Network object that incorporates wdist instead of distance\nCompute route between the two points of interest\n\nFor 1., we can use the scaler in scikit-learn:\n\nfrom sklearn.preprocessing import minmax_scale\n\nThen generate and attach to edges a scaled version of mm_len:\n\nedges[\"scaled_dist\"] = minmax_scale(edges[\"mm_len\"])\n\nWe can compare distance with scaled distance. The correlation should be perfect, the scaling is only a change of scale or unit.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\"mm_len\", \"scaled_dist\", ax=ax)\nax.set_title(\"Distance Vs Scaled Distance\")\nplt.show()\n\n\n\n\nWe move on to 2., with a similar approach. We will use the negative of the building quality average (average_quality):\n\nedges[\"scaled_inv_bquality\"] = minmax_scale(\n    -edges[\"average_quality\"]\n)\n\nAnd again, we can plot the relation between building quality and the scaled quality.\n\nfig, ax = plt.subplots()\nedges.plot.scatter(\n    \"average_quality\", \"scaled_inv_bquality\", ax=ax\n)\nax.set_title(\"Quality Vs Inv. Scaled Quality\")\nplt.show()\n\n\n\n\nTaking 1. and 2. into 3. we can build wdist. For this example, we will give each dimension the same weight (0.5), but this is at discretion of the researcher.\n\nw = 0.5\nedges[\"wdist\"] = (\n    edges[\"scaled_dist\"] * w +\n    edges[\"scaled_inv_bquality\"] * (1-w)\n)\n\nNow we can recreate the Network object based on our new measure (4.) and provide routing. Since it is the same process as with distance, we will do it all in one go:\n\n# Build new graph object\nw_graph = pandana.Network(\n    nodes.geometry.x,\n    nodes.geometry.y,\n    edges[\"node_start\"],\n    edges[\"node_end\"],\n    edges[[\"wdist\"]]\n)\n# Snap locations to their nearest node\npt_nodes = w_graph.get_node_ids(\n    [first.geometry.x.iloc[0], sol.geometry.x.iloc[0]], \n    [first.geometry.y.iloc[0], sol.geometry.y.iloc[0]]\n)\n# Generate route\nw_route_nodes = w_graph.shortest_path(\n    pt_nodes[0], pt_nodes[1]\n)\n# Build LineString\nw_route = route_nodes_to_line(\n    w_route_nodes, w_graph\n)\n\nGenerating contraction hierarchies with 8 threads.\nSetting CH node vector of size 49985\nSetting CH edge vector of size 66499\nRange graph removed 444 edges of 132998\n. 10% . 20% . 30% . 40% . 50% . 60% . 70% . 80% . 90% . 100%\n\n\nNow we are ready to display it on a map:\n\nfig, ax = plt.subplots()\n# Building quality\nstreets.plot(\n    \"average_quality\", \n    scheme=\"quantiles\", \n    cmap=\"magma\",\n    linewidth=0.5,\n    figsize=(9, 9), \n    ax=ax\n)\n# Shortest route\nroute.plot(\n    color=\"xkcd:orange red\", linewidth=3, ax=ax, label=\"Shortest\"\n)\n# Weighted route\nw_route.plot(\n    color=\"xkcd:easter green\", linewidth=3, ax=ax, label=\"Weighted\"\n)\n# Styling\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge:\n1. Explore the differences in the output of weighted routing if you change the weight between distance and the additional constrain.\n2. Recreate weighted routing using the linearity of street segments. How can you go from A to B avoiding long streets?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#proximity",
    "href": "quarto-notebooks/transport-costs.html#proximity",
    "title": "6  Transport costs",
    "section": "6.6 Proximity",
    "text": "6.6 Proximity\nWhat is the nearest internet cafe for Airbnb’s without WiFi?\nFirst we identify Airbnb’s without WiFi:\n\nno_wifi = abbs.query(\n    \"WiFi == '0'\"\n).to_crs(streets.crs)\n\nThen pull WiFi spots in Madrid from OpenStreetMap:\n\nicafes = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"amenity\": \"internet_cafe\"}\n).to_crs(streets.crs).reset_index()\n\n\nfig, ax = plt.subplots()\n\nno_wifi.plot(\n    color=\"red\", \n    markersize=1,\n    alpha=0.5,\n    label=\"Airbnb no WiFi\",\n    figsize=(9, 9),\n    ax=ax\n)\n\nicafes.plot(\n    ax=ax, color=\"lime\", label=\"Internet cafes\"\n)\n\ncontextily.add_basemap(\n    ax, \n    crs=no_wifi.crs,\n    source=contextily.providers.CartoDB.Voyager\n)\nax.set_axis_off()\nplt.legend()\nplt.show()\n\n\n\n\nThe logic for this operation is the following:\n\nAdd the points of interest (POIs, the internet cafes) to the network object (streets_pdn)\nFind the nearest node to each POI\nFind the nearest node to each Airbnb without WiFi\nConnect each Airbnb to its nearest internet cafe\n\nWe can add the internet cafes to the network object (1.) with the set_pois method. Note we set maxitems=1 because we are only going to query for the nearest cafe. This will make computations much faster.\n\nstreets_pdn.set_pois(\n    category=\"Internet cafes\", # Our name for the layer in the `Network` object\n    maxitems=1,                # Use to count only nearest cafe\n    maxdist=100000,            # 100km so everything is included\n    x_col=icafes.geometry.x,   # X coords of cafes\n    y_col=icafes.geometry.y,   # Y coords of cafes\n)\n\nOnce the cafes are added to the network, we can find the nearest one to each node (2.). Note there are some nodes for which we can’t find a nearest cafe. These are related to disconnected parts of the network.\n\ncafe2nnode = streets_pdn.nearest_pois(\n    100000,              # Max distance to look for\n    \"Internet cafes\",    # POIs to look for\n    num_pois=1,          # No. of POIs to include\n    include_poi_ids=True # Store POI ID\n).join(# Then add the internet cafee IDs and name\n    icafes[['osmid', 'name']],\n    on=\"poi1\"\n).rename(# Rename the distance from node to cafe\n    columns={1: \"dist2icafe\"}\n)\ncafe2nnode.head()\n\n\n\n\n\n\n\n\ndist2icafe\npoi1\nosmid\nname\n\n\nnodeID\n\n\n\n\n\n\n\n\n0\n5101.421875\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n1\n5190.265137\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n2\n5252.475098\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n3\n5095.101074\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n4\n5676.117188\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n\n\n\n\n\nTo make things easier down the line, we can link cafe2nnode to the cafe IDs. And we can also link Airbnb’s to nodes (3.) following a similar approach as we have seen above:\n\nabbs_nnode = streets_pdn.get_node_ids(\n    no_wifi.geometry.x, no_wifi.geometry.y\n)\nabbs_nnode.head()\n\n26      8872\n50     10905\n62     41158\n63     34257\n221    32215\nName: node_id, dtype: int64\n\n\nFinally, we can bring together both to find out what is the nearest internet cafe for each Airbnb (4.).\n\nabb_icafe = no_wifi[\n    [\"geometry\"]     # Keep only geometries of ABBs w/o WiFi\n].assign(\n    nnode=abbs_nnode # Attach to thse ABBs the nearest node in the network\n).join(              # Join to each ABB the nearest cafe using node IDs\n    cafe2nnode, \n    on=\"nnode\"\n)\nabb_icafe.head()\n\n\n\n\n\n\n\n\ngeometry\nnnode\ndist2icafe\npoi1\nosmid\nname\n\n\n\n\n26\nPOINT (443128.256 4483599.841)\n8872\n4926.223145\n9.0\n3.770327e+09\nSilver Envíos 2\n\n\n50\nPOINT (441885.677 4475916.602)\n10905\n1876.392944\n19.0\n6.922981e+09\nLocutorio\n\n\n62\nPOINT (440439.640 4476480.771)\n41158\n1164.812988\n17.0\n5.573414e+09\nNaN\n\n\n63\nPOINT (438485.311 4471714.377)\n34257\n1466.537964\n5.0\n2.304485e+09\nNaN\n\n\n221\nPOINT (439941.104 4473117.914)\n32215\n354.268005\n15.0\n5.412145e+09\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate distances to nearest internet cafe for ABBs with WiFi. On average, which of the two groups (with and without WiFi) are closer to internet cafes?"
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#accessibility",
    "href": "quarto-notebooks/transport-costs.html#accessibility",
    "title": "6  Transport costs",
    "section": "6.7 Accessibility",
    "text": "6.7 Accessibility\nThis flips the previous question on its head and, instead of asking what is the nearest POI to a given point, along the network (irrespective of distance), it asks how many POIs can I access within a network-based distance radius?\n\nparks = ox.features_from_place(\n    \"Madrid, Spain\", tags={\"leisure\": \"park\"}\n).to_crs(streets.crs)\n\n\nFor example, how many parks are within 500m(-euclidean) of an Airbnb?\n\nWe draw a radius of 500m around each AirBnb:\n\nbuffers = geopandas.GeoDataFrame(\n    geometry=abbs.to_crs(\n        streets.crs\n    ).buffer(\n        500\n    )\n)\n\nThen intersect it with the location of parks, and count by buffer (ie. Airbnb):\n\npark_count = geopandas.sjoin(\n    parks, buffers\n).groupby(\n    \"index_right\"\n).size()\n\n\nHow many parks are within 500m(-network) of an Airbnb?\n\nWe need to approach this as a calculation within the network. The logic of steps thus looks like:\n\nUse the aggregation module in pandana to count the number of parks within 500m of each node in the network\nExtract the counts for the nodes nearest to Airbnb properties\nAssign park counts to each Airbnb\n\nWe can set up the aggregate engine (1.). This involves three steps:\n\nObtain nearest node for each park\n\n\nparks_nnode = streets_pdn.get_node_ids(\n    parks.centroid.x, parks.centroid.y\n)\n\n\nInsert the parks’ nearest node through set so it can be “aggregated”\n\n\nstreets_pdn.set(\n    parks_nnode, name=\"Parks\"\n)\n\n\n“Aggregate” for a distance of 500m, effectively counting the number of parks within 500m of each node\n\n\nparks_by_node = streets_pdn.aggregate(\n    distance=500, type=\"count\", name=\"Parks\"\n)\nparks_by_node.head()\n\nnodeID\n0    5.0\n1    5.0\n2    6.0\n3    8.0\n4    1.0\ndtype: float64\n\n\nAt this point, we have the number of parks within 500m of every node in the network. To identify those that correspond to each Airbnb (3.), we first pull out the nearest nodes to each ABB:\n\nabbs_xys = abbs.to_crs(streets.crs).geometry\nabbs_nnode = streets_pdn.get_node_ids(\n    abbs_xys.x, abbs_xys.y\n)\n\nAnd use the list to assign the count of the nearest node to each Airbnb:\n\npark_count_network = abbs_nnode.map(\n    parks_by_node\n)\npark_count_network.head()\n\n0     4.0\n1     9.0\n2     5.0\n3     0.0\n4    12.0\nName: node_id, dtype: float64\n\n\n\nFor which areas do both differ most?\n\nWe can compare the two counts above to explore to what extent the street layout is constraining access to nearby parks.\n\npark_comp = geopandas.GeoDataFrame(\n    {\n        \"Euclidean\": park_count, \n        \"Network\": park_count_network\n    },\n    geometry=abbs.geometry,\n    crs=abbs.crs\n)\n\n\nfig, ax = plt.subplots()\npark_comp.plot.scatter(\"Euclidean\", \"Network\", ax=ax)\nax.axline([0, 0], [1, 1], color='red') #45-degree line\nplt.show()\n\n\n\n\nNote there are a few cases where there are more network counts than Euclidean. These are due to the slight inaccuracies introduced by calculating network distances from nodes rather than the locations themselves.\nGeographically:\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Euclidean count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[0]\n)\ncontextily.add_basemap(\n    axs[0], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[0].set_axis_off()\naxs[0].set_title(\"Euclidean Distances\")\n\n# Count difference\nwith_parks = park_comp.query(\n    \"(Network &gt; 0) & (Euclidean &gt; 0)\"\n)\ncount_diff = 100 * (\n    with_parks[\"Euclidean\"] - \n    with_parks[\"Network\"]\n) / with_parks[\"Euclidean\"]\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=count_diff\n).dropna().plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[1]\n)\ncontextily.add_basemap(\n    axs[1], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[1].set_axis_off()\naxs[1].set_title(\"Count Difference (%)\")\n\n# Network count\nabbs.to_crs(\n    streets.crs\n).assign(\n    n_parks=park_count_network\n).fillna(0).plot(\n    \"n_parks\", \n    scheme=\"fisherjenkssampled\", \n    alpha=0.5,\n    markersize=1,\n    legend=True,\n    ax=axs[2]\n)\ncontextily.add_basemap(\n    axs[2], \n    crs=streets.crs,\n    source=contextily.providers.CartoDB.PositronNoLabels\n)\naxs[2].set_axis_off()\naxs[2].set_title(\"Network Distances\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChallenge: Calculate accessibility to other ABBs from each ABB through the network. How many ABBs can you access within 500m of each ABB?\nNote you will need to use the locations of ABBs both as the source and the target for routing in this case."
  },
  {
    "objectID": "quarto-notebooks/transport-costs.html#next-steps",
    "href": "quarto-notebooks/transport-costs.html#next-steps",
    "title": "6  Transport costs",
    "section": "6.8 Next steps",
    "text": "6.8 Next steps\nIf you found the content in this block useful, the following resources represent some suggestions on where to go next:\n\nThe pandana tutorial and documentation are excellent places to get a more detailed and comprehensive view into the functionality of the library"
  },
  {
    "objectID": "quarto-notebooks/datasets.html#madrid",
    "href": "quarto-notebooks/datasets.html#madrid",
    "title": "Datasets",
    "section": "Madrid",
    "text": "Madrid\n\nAirbnb properties\nThis dataset has been sourced from the course “Spatial Modelling for Data Scientists”. The file imported here corresponds to the v0.1.0 version.\nThis dataset contains a pre-processed set of properties advertised on the AirBnb website within the region of Madrid (Spain), together with house characteristics.\n\nData file madrid_abb.gpkg.\nCode used to generate the file [URL].\nFurhter information [URL].\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nAirbnb neighbourhoods\nThis dataset has been directly sourced from the website Inside Airbnb. The file was imported on February 10th 2021.\nThis dataset contains neighbourhood boundaries for the city of Madrid, as provided by Inside Airbnb.\n\nData file neighbourhoods.geojson.\nFurhter information: [URL].\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nArturo\nThis dataset contains the street layout of Madrid as well as scores of habitability, where available, associated with street segments. The data originate from the Arturo Project, by 300,000Km/s, and the available file here is a slimmed down version of their official street layout distributed by the project.\n\nData file download arturo_streets.gpkg.\nCode used to generate the file [Page], borrowed from here.\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nSentinel 2 - 120m mosaic\nThis dataset contains four scenes for the region of Madrid (Spain) extracted from the Digital Twin Sandbox Sentinel-2 collection, by the SentinelHub. Each scene corresponds to the following dates in 2019:\n\nJanuary 1st\nApril 1st\nJuly 10th\nNovember 17th\n\nEach scene includes red, green, blue and near-infrared bands.\n\nData files (Jan 1st madrid_scene_s2_120_2019-1-1.tif, Apr 1st madrid_scene_s2_120_2019-4-1.tif, Jul 10th madrid_scene_s2_120_2019-7-10.tif, Nov 27th madrid_scene_s2_120_2019-11-27.tif)\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nSentinel 2 - 10m GHS composite\nThis dataset contains a scene for the region of Madrid (Spain) extracted from the GHS Composite S2, by the European Commission.\n\nData file madrid_scene_s2_10_tc.tif\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication."
  },
  {
    "objectID": "quarto-notebooks/datasets.html#cambodia",
    "href": "quarto-notebooks/datasets.html#cambodia",
    "title": "Datasets",
    "section": "Cambodia",
    "text": "Cambodia\n\nPollution\nSurface with \\(NO_2\\) measurements (tropospheric column) information attached from Sentinel 5.\n\nData file cambodia_s5_no2.tif\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\n\n\nFriction surfaces\nThis dataset is an extraction of the following two data products by Weiss et al. (2020) {cite}weiss2020global and distributed through the Malaria Atlas Project:\n\nGlobal friction surface enumerating land-based travel walking-only speed without access to motorized transport for a nominal year 2019 (Minutes required to travel one metre)\nGlobal friction surface enumerating land-based travel speed with access to motorized transport for a nominal year 2019 (Minutes required to travel one metre)\n\nEach is provided on a separate fie.\n\n️ Data files (cambodia_2020_motorized_friction_surface.tif and cambodia_2020_walking_friction_surface.tif)\nCode used to generate the file [Page]\n️ Furhter information [URL]\n\n\n\nRegional aggregates\nThis dataset relies on boundaries from the Humanitarian Data Exchange. The file is provided by the World Food Programme through the Humanitarian Data Exchange and was accessed on February 15th 2021.`\nPollution and friction aggregated at Level 2 (municipality) administrative boundaries for Cambodia.\n\nData file cambodia_regional.gpkg\nCode used to generate the file [Page], borrowed from here\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication.\n\n\nCambodian cities\nExtract from the Urban Centre Database (UCDB), version 1.2, of the centroid for Cambodian cities.\n\n️ Data file cambodian_cities.geojson\nCode used to generate the file [Page], borrowed from here\nFurhter information [URL]\n\nThis dataset is licensed under a license CC0 1.0 Universal Public Domain Dedication."
  },
  {
    "objectID": "quarto-notebooks/references.html",
    "href": "quarto-notebooks/references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, Jennings, Dipto Sarkar, and Leysia Palen. 2019.\n“Corporate Editors in the Evolving Landscape of\nOpenStreetMap.” ISPRS International Journal of\nGeo-Information 8 (5): 232.\n\n\nArribas-Bel, Dani. 2019. “A Course on Geographic Data\nScience.” The Journal of Open Source Education 2 (14).\nhttps://doi.org/https://doi.org/10.21105/jose.00042.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS\nUsers. ESRI press.\n\n\nMcKinney, Wes. 2013. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython. 1st ed.\nPaperback; O’Reilly Media. http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\\&path=ASIN/1449319793.\n\n\nPietrostefani, Elisabetta, and Carmen Cabrera-Arnau. 2024.\n“A Course in Geographic\nData Science.” https://pietrostefani.github.io/gds/.\n\n\nRey, Sergio J., Daniel Arribas-Bel, and Levi J. Wolf. forthcoming.\nGeographic Data Science with PySAL and the PyData Stack. CRC\npress."
  }
]